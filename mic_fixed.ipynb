{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qZba3NYv5W-"
   },
   "outputs": [],
   "source": [
    "#storage of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpbZ7MdttW99"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVEnFN6bwDgI"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhG-3QjC4QF_"
   },
   "outputs": [],
   "source": [
    "#finalising the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iT7j9jmr1AJ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S4HsVGl8wh9a",
    "outputId": "44be94d5-5d0d-420c-d006-7b37be5628dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100   220  100   220    0     0   2196      0 --:--:-- --:--:-- --:--:--  2200\n"
     ]
    }
   ],
   "source": [
    "!curl -L \"https://storage.googleapis.com/kaggle-data-sets/2768513/4784071/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250827%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250827T024359Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=1a1ddbaf1b0e32e9aabd16dd7e2f70fd8fe2ae40dcdf8e65453881aba3d1db0ccc133c315998513fdbb1093f834d23234a8a8ab08924dcc653b3b75d0841cf7e6d013fd68b33d60fee3a355e88b56f26362ff5f6a57d1f50c75d8a8e64bb28db4ba8918a3ffce070c310fd1c12f0648ce4b3768481ca3faceefbe0ae1d9d5db37123030768f70da56cae3988736fa857ce149fc39f0489b001495f73889e5bbb0fa093f3b0fb5e9854fad6fe9f74115979c4747898315a284f42df649583810b62e7c86b49d2ec2c98c3203ff90f7d2c19201b15b9fb57386197c55f28a0c56c6a537e100a85647b53b8307ddfa00810766c7a248fa3b81c2f6edd3ead8d00da\" -o archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "954z-8TuwjCR",
    "outputId": "4803f971-f420-44d0-c189-5cbf02c968cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100   220  100   220    0     0   2789      0 --:--:-- --:--:-- --:--:--  2820\n"
     ]
    }
   ],
   "source": [
    "!curl -L \"https://storage.googleapis.com/kaggle-data-sets/2750746/4753077/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250827%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250827T024259Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=6a1bc74b093049c74c92a8d2d572a9222a8e5f225175c3d71bdcf9ddd1371b56382e176ad942bfab0db6351eaec280cbf9cb6f4b3a99cf96d4c73753f05d54958db32fbf765280cc47b280da3700caf22c84590285ec390dd27e57c4ac6152107cd8148ee15da87e2f2a42a7fc059d1ee65825bd969480898d9d06d8969b16c475c55981300d3ec48520fad01323a62a45c82c8b193f6e11c83bae0c2ca63174f142f1b51ed209243c7dec994ed256ef1ca71868bf08f6bbcb5e71241ff9e694a599ddc3d8ceb1bad9ef99ae0614b0fe01cec125d880870532b54549238fbb69f0f36edaddf7799dbea379965474d31f74306d81d4abaa3499d1fee2c29125c5\" -o dataset_second.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdKFtHBkyB7U"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wVSkQsXxn2B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d87986e",
    "outputId": "bff9b98c-2b38-487a-ba2a-771f6b5abbf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in '/content/dataset_one_extracted':\n",
      "\n",
      "Files in '/content/dataset_second_extracted':\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "extracted_path_one = '/content/dataset_one_extracted'\n",
    "extracted_path_two = '/content/dataset_second_extracted'\n",
    "\n",
    "print(f\"Files in '{extracted_path_one}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_two}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBbhrYoDy-CM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "extracted_path_one = '/content/dataset_one_extracted'\n",
    "extracted_path_two = '/content/dataset_second_extracted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "CfITM5DxzGQ8",
    "outputId": "e9f12784-a72f-4562-a2cc-6abff199558e"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (ipython-input-2125240012.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2125240012.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    import\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e53b68c6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "archive_path_one = '/content/archive.zip'\n",
    "archive_path_two = '/content/dataset_second.zip'\n",
    "\n",
    "# Check the size of the downloaded files\n",
    "print(f\"Size of '{archive_path_one}': {os.path.getsize(archive_path_one)} bytes\")\n",
    "print(f\"Size of '{archive_path_two}': {os.path.getsize(archive_path_two)} bytes\")\n",
    "\n",
    "# Re-attempt extraction and list contents of zip files\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck'\n",
    "\n",
    "os.makedirs(extracted_path_one, exist_ok=True)\n",
    "os.makedirs(extracted_path_two, exist_ok=True)\n",
    "\n",
    "print(f\"\\nAttempting to re-extract '{archive_path_one}' to '{extracted_path_one}'\")\n",
    "try:\n",
    "    with zipfile.ZipFile(archive_path_one, 'r') as zip_ref:\n",
    "        print(\"Contents of archive.zip:\")\n",
    "        zip_ref.printdir()\n",
    "        zip_ref.extractall(extracted_path_one)\n",
    "    print(\"Extraction of archive.zip successful.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: '{archive_path_one}' is a bad zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction of '{archive_path_one}': {e}\")\n",
    "\n",
    "\n",
    "print(f\"\\nAttempting to re-extract '{archive_path_two}' to '{extracted_path_two}'\")\n",
    "try:\n",
    "    with zipfile.ZipFile(archive_path_two, 'r') as zip_ref:\n",
    "        print(\"Contents of dataset_second.zip:\")\n",
    "        zip_ref.printdir()\n",
    "        zip_ref.extractall(extracted_path_two)\n",
    "    print(\"Extraction of dataset_second.zip successful.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: '{archive_path_two}' is a bad zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction of '{archive_path_two}': {e}\")\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_one}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_two}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YDWCdl30d2m"
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing: Loading and preparing audio data\n",
    "\n",
    "# Import necessary libraries for audio processing (we'll add more as needed)\n",
    "import librosa # A common library for audio analysis\n",
    "import soundfile as sf # To read and write sound files\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the paths to the extracted datasets\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck'\n",
    "\n",
    "print(\"Listing all files in the first dataset:\")\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))\n",
    "\n",
    "print(\"\\nListing all files in the second dataset:\")\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))\n",
    "\n",
    "# You might want to add code here later to specifically identify and separate\n",
    "# the bird sounds and music instrument sounds based on their file paths or names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ep1PT6Zf00k1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c27e0eda"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck'\n",
    "\n",
    "bird_sound_files = []\n",
    "music_instrument_files = []\n",
    "\n",
    "# Identify bird sound files from the second dataset (assuming 'Voice of Birds' contains bird sounds)\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        if 'Voice of Birds' in root and name.endswith('.mp3'):\n",
    "            bird_sound_files.append(os.path.join(root, name))\n",
    "\n",
    "# Identify music instrument files from the first dataset (assuming Test_submission and Train_submission contain music instruments)\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        if ('Test_submission' in root or 'Train_submission' in root) and (name.endswith('.wav') or name.endswith('.mp3')):\n",
    "             # Further check if the file name indicates a music instrument\n",
    "            if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords']):\n",
    "                 music_instrument_files.append(os.path.join(root, name))\n",
    "\n",
    "\n",
    "print(f\"Found {len(bird_sound_files)} bird sound files.\")\n",
    "print(f\"Found {len(music_instrument_files)} music instrument files.\")\n",
    "\n",
    "# Display a few file paths from each category as a check\n",
    "print(\"\\nSample bird sound files:\")\n",
    "for f in bird_sound_files[:5]:\n",
    "    print(f)\n",
    "\n",
    "print(\"\\nSample music instrument files:\")\n",
    "for f in music_instrument_files[:5]:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "conb7GO92nkq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ac4993f0"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Assuming bird_sound_files and music_instrument_files lists are populated from the previous step\n",
    "\n",
    "if bird_sound_files:\n",
    "    # Select a random bird sound file\n",
    "    sample_bird_file = random.choice(bird_sound_files)\n",
    "    print(f\"\\nProcessing sample bird sound file: {sample_bird_file}\")\n",
    "\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        y_bird, sr_bird = librosa.load(sample_bird_file)\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfccs_bird = librosa.feature.mfcc(y=y_bird, sr=sr_bird)\n",
    "\n",
    "        print(f\"Original audio shape (bird): {y_bird.shape}\")\n",
    "        print(f\"MFCCs shape (bird): {mfccs_bird.shape}\")\n",
    "        # You can display or visualize the MFCCs here if needed\n",
    "        # display(mfccs_bird)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {sample_bird_file}: {e}\")\n",
    "else:\n",
    "    print(\"\\nNo bird sound files found to process.\")\n",
    "\n",
    "\n",
    "if music_instrument_files:\n",
    "    # Select a random music instrument file\n",
    "    sample_music_file = random.choice(music_instrument_files)\n",
    "    print(f\"\\nProcessing sample music instrument file: {sample_music_file}\")\n",
    "\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        y_music, sr_music = librosa.load(sample_music_file)\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfccs_music = librosa.feature.mfcc(y=y_music, sr=sr_music)\n",
    "\n",
    "        print(f\"Original audio shape (music): {y_music.shape}\")\n",
    "        print(f\"MFCCs shape (music): {mfccs_music.shape}\")\n",
    "        # You can display or visualize the MFCCs here if needed\n",
    "        # display(mfccs_music)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {sample_music_file}: {e}\")\n",
    "else:\n",
    "    print(\"\\nNo music instrument files found to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abafddc8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This will need to be determined based on the desired sequence length or maximum length of audio files\n",
    "\n",
    "def extract_features(file_path, n_mfcc=20):\n",
    "    \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Pad or truncate MFCCs to a fixed length\n",
    "        if mfccs.shape[1] < max_padding:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, max_padding - mfccs.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :max_padding]\n",
    "\n",
    "        return mfccs.T # Transpose to have shape (time steps, n_mfcc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Lists populated from the previous step\n",
    "# bird_sound_files = [...]\n",
    "# music_instrument_files = [...]\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "print(\"Extracting features from bird sound files...\")\n",
    "for i, file_path in enumerate(bird_sound_files):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processing bird file {i + 1}/{len(bird_sound_files)}\")\n",
    "    mfccs = extract_features(file_path, n_mfcc=n_mfcc)\n",
    "    if mfccs is not None:\n",
    "        features.append(mfccs)\n",
    "        labels.append('bird')\n",
    "\n",
    "print(\"\\nExtracting features from music instrument files...\")\n",
    "for i, file_path in enumerate(music_instrument_files):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processing music instrument file {i + 1}/{len(music_instrument_files)}\")\n",
    "    mfccs = extract_features(file_path, n_mfcc=n_mfcc)\n",
    "    if mfccs is not None:\n",
    "        features.append(mfccs)\n",
    "        labels.append('music')\n",
    "\n",
    "# Convert features and labels to numpy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "label_map = {'bird': 0, 'music': 1}\n",
    "numerical_labels = np.array([label_map[label] for label in labels])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, numerical_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nFeature extraction complete.\")\n",
    "print(f\"Total samples: {len(features)}\")\n",
    "print(f\"Shape of features array: {features.shape}\")\n",
    "print(f\"Shape of labels array: {labels.shape}\")\n",
    "print(f\"Shape of training features: {X_train.shape}\")\n",
    "print(f\"Shape of testing features: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ef0ec39"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPooling1D\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    # Add a 1D Convolutional layer to process the sequential nature of MFCCs\n",
    "    Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # Another Conv1D layer\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # Flatten the output for the dense layers\n",
    "    Flatten(),\n",
    "\n",
    "    # Dense layers for classification\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    # Output layer: 1 neuron with sigmoid activation for binary classification\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdacb285"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"\\nTest Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1c8dd73c"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, RepeatVector, TimeDistributed, Input\n",
    "\n",
    "# Define the LSTM model architecture for sequence generation\n",
    "model_gen = Sequential([\n",
    "    # Add an Input layer explicitly\n",
    "    Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    # LSTM layer to process the input sequences\n",
    "    LSTM(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # RepeatVector to repeat the input for the TimeDistributed layer\n",
    "    # This is useful if the output sequence has the same length as the input\n",
    "    RepeatVector(X_train.shape[1]),\n",
    "\n",
    "    # LSTM layer to generate the output sequence\n",
    "    LSTM(128, activation='relu', return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # TimeDistributed Dense layer to output features for each time step\n",
    "    TimeDistributed(Dense(X_train.shape[2], activation='linear'))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_gen.compile(optimizer='adam', loss='mse') # Using Mean Squared Error for regression-like output\n",
    "\n",
    "# Print the model summary\n",
    "model_gen.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e341ee9"
   },
   "outputs": [],
   "source": [
    "# Train the LSTM generation model\n",
    "history_gen = model_gen.fit(X_train, X_train, epochs=50, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6c376282"
   },
   "outputs": [],
   "source": [
    "# Check for NaN values in X_train\n",
    "nan_in_X_train = np.isnan(X_train).sum()\n",
    "print(f\"Number of NaN values in X_train: {nan_in_X_train}\")\n",
    "\n",
    "# Check for infinite values in X_train\n",
    "inf_in_X_train = np.isinf(X_train).sum()\n",
    "print(f\"Number of infinite values in X_train: {inf_in_X_train}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fc9c6b0"
   },
   "outputs": [],
   "source": [
    "# Train the LSTM generation model again with fewer epochs\n",
    "history_gen = model_gen.fit(X_train, X_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYweQXrppQoJ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6hNqXSgueFE"
   },
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4c3dc79a"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "# Define a new GRU model architecture for sequence generation\n",
    "model_gen_gru = Sequential([\n",
    "    # Add an Input layer explicitly\n",
    "    Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    # GRU layer to process the input sequences\n",
    "    GRU(128, activation='relu', return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # Another GRU layer\n",
    "    GRU(128, activation='relu', return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # TimeDistributed Dense layer to output features for each time step\n",
    "    TimeDistributed(Dense(X_train.shape[2], activation='linear'))\n",
    "])\n",
    "\n",
    "# Compile the GRU model with gradient clipping\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=1.0) # Add gradient clipping\n",
    "model_gen_gru.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Print the model summary\n",
    "model_gen_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d10aa848"
   },
   "outputs": [],
   "source": [
    "# Train the GRU generation model\n",
    "history_gen_gru = model_gen_gru.fit(X_train, X_train, epochs=50, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4d355580"
   },
   "outputs": [],
   "source": [
    "def generate_mfccs(model, start_sequence, num_steps):\n",
    "    \"\"\"Generates new MFCC sequences using the trained GRU model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras model (GRU in this case).\n",
    "        start_sequence: A numpy array representing the initial sequence\n",
    "                        (shape: (1, sequence_length, n_mfcc)).\n",
    "        num_steps: The number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated MFCC sequence.\n",
    "    \"\"\"\n",
    "    generated_sequence = np.copy(start_sequence)\n",
    "    current_sequence = np.copy(start_sequence)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        # Predict the next time step\n",
    "        predicted_mfcc = model.predict(current_sequence)\n",
    "\n",
    "        # Append the predicted MFCC to the generated sequence\n",
    "        generated_sequence = np.append(generated_sequence, predicted_mfcc, axis=1)\n",
    "\n",
    "        # Update the current sequence by removing the oldest time step and adding the new one\n",
    "        current_sequence = np.append(current_sequence[:, 1:, :], predicted_mfcc, axis=1)\n",
    "\n",
    "    return generated_sequence.squeeze() # Remove the batch dimension\n",
    "\n",
    "# Generate a starting sequence (e.g., take a random sample from the training data)\n",
    "# Ensureing the starting sequence has the correct shape (1, sequence_length, n_mfcc)\n",
    "start_index = np.random.randint(0, X_train.shape[0])\n",
    "start_sequence = X_train[start_index:start_index+1, :, :]\n",
    "\n",
    "# Define the number of steps to generate\n",
    "generation_steps = 200 # Generate 200 additional time steps\n",
    "\n",
    "# Generate new MFCC features using the GRU model\n",
    "generated_mfccs = generate_mfccs(model_gen_gru, start_sequence, generation_steps)\n",
    "#printing the step to move forword in the process\n",
    "print(f\"Shape of the generated MFCCs: {generated_mfccs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTmvIaze7sZr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78249a57"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# Define the sampling rate (assuming it was 22050 Hz, a common rate, adjust if a different rate was used during extraction)\n",
    "sr = 22050\n",
    "\n",
    "# Convert the generated MFCCs back to an audio time-series\n",
    "# We need to transpose the generated_mfccs back to the shape (n_mfcc, time_steps)\n",
    "audio_time_series = librosa.feature.inverse.mfcc_to_audio(generated_mfccs.T, sr=sr)\n",
    "\n",
    "# Define the output filename\n",
    "output_filename = 'generated_music.wav'\n",
    "\n",
    "# Save the generated audio to a WAV file\n",
    "sf.write(output_filename, audio_time_series, sr)\n",
    "\n",
    "print(f\"Generated audio saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYs60Iaic1S1"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This will need to be determined based on the desired sequence length or maximum length of audio files\n",
    "\n",
    "def extract_features(file_path, n_mfcc=20):\n",
    "    \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Pad or truncate MFCCs to a fixed length\n",
    "        if mfccs.shape[1] < max_padding:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, max_padding - mfccs.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :max_padding]\n",
    "\n",
    "        return mfccs.T # Transpose to have shape (time steps, n_mfcc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Lists populated from the previous step\n",
    "# bird_sound_files = [...]\n",
    "# music_instrument_files = [...]\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "print(\"Extracting features from bird sound files...\")\n",
    "for i, file_path in enumerate(bird_sound_files):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processing bird file {i + 1}/{len(bird_sound_files)}\")\n",
    "    mfccs = extract_features(file_path, n_mfcc=n_mfcc)\n",
    "    if mfccs is not None:\n",
    "        features.append(mfccs)\n",
    "        labels.append('bird')\n",
    "\n",
    "print(\"\\nExtracting features from music instrument files...\")\n",
    "for i, file_path in enumerate(music_instrument_files):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processing music instrument file {i + 1}/{len(music_instrument_files)}\")\n",
    "    mfccs = extract_features(file_path, n_mfcc=n_mfcc)\n",
    "    if mfccs is not None:\n",
    "        features.append(mfccs)\n",
    "        labels.append('music')\n",
    "\n",
    "# Convert features and labels to numpy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "label_map = {'bird': 0, 'music': 1}\n",
    "numerical_labels = np.array([label_map[label] for label in labels])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, numerical_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nFeature extraction complete.\")\n",
    "print(f\"Total samples: {len(features)}\")\n",
    "print(f\"Shape of features array: {features.shape}\")\n",
    "print(f\"Shape of labels array: {labels.shape}\")\n",
    "print(f\"Shape of training features: {X_train.shape}\")\n",
    "print(f\"Shape of testing features: {X_test.shape}\")\n",
    "\n",
    "def generate_mfccs(model, start_sequence, num_steps):\n",
    "    \"\"\"Generates new MFCC sequences using the trained GRU model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras model (GRU in this case).\n",
    "        start_sequence: A numpy array representing the initial sequence\n",
    "                        (shape: (1, sequence_length, n_mfcc)).\n",
    "        num_steps: The number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated MFCC sequence.\n",
    "    \"\"\"\n",
    "    generated_sequence = np.copy(start_sequence)\n",
    "    current_sequence = np.copy(start_sequence)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        # Predict the next time step\n",
    "        predicted_mfcc = model.predict(current_sequence)\n",
    "\n",
    "        # Append the predicted MFCC to the generated sequence\n",
    "        generated_sequence = np.append(generated_sequence, predicted_mfcc, axis=1)\n",
    "\n",
    "        # Update the current sequence by removing the oldest time step and adding the new one\n",
    "        current_sequence = np.append(current_sequence[:, 1:, :], predicted_mfcc, axis=1)\n",
    "\n",
    "    return generated_sequence.squeeze() # Remove the batch dimension\n",
    "\n",
    "# Generate a starting sequence (e.g., take a random sample from the training data)\n",
    "# Ensure the starting sequence has the correct shape (1, sequence_length, n_mfcc)\n",
    "start_index = np.random.randint(0, X_train.shape[0])\n",
    "start_sequence = X_train[start_index:start_index+1, :, :]\n",
    "\n",
    "# Define the number of steps to generate\n",
    "generation_steps = 200 # Generate 200 additional time steps\n",
    "\n",
    "# Generate new MFCC features using the GRU model\n",
    "generated_mfccs = generate_mfccs(model_gen_gru, start_sequence, generation_steps)\n",
    "\n",
    "print(f\"Shape of the generated MFCCs: {generated_mfccs.shape}\")\n",
    "\n",
    "# Define the sampling rate (assuming it was 22050 Hz, a common rate, adjust if a different rate was used during extraction)\n",
    "sr = 22050\n",
    "\n",
    "# Convert the generated MFCCs back to an audio time-series\n",
    "# We need to transpose the generated_mfccs back to the shape (n_mfcc, time_steps)\n",
    "audio_time_series = librosa.feature.inverse.mfcc_to_audio(generated_mfccs.T, sr=sr)\n",
    "\n",
    "# Define the output filename\n",
    "output_filename = 'generated_music.wav'\n",
    "\n",
    "# Save the generated audio to a WAV file\n",
    "sf.write(output_filename, audio_time_series, sr)\n",
    "\n",
    "print(f\"Generated audio saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCQExesBc7Zj"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This will need to be determined based on the desired sequence length or maximum length of audio files\n",
    "\n",
    "def extract_features(file_path, n_mfcc=20):\n",
    "    \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Pad or truncate MFCCs to a fixed length\n",
    "        if mfccs.shape[1] < max_padding:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, max_padding - mfccs.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :max_padding]\n",
    "\n",
    "        return mfccs.T # Transpose to have shape (time steps, n_mfcc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define the paths to the extracted datasets\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck'\n",
    "\n",
    "bird_sound_files = []\n",
    "music_instrument_files = []\n",
    "\n",
    "# Identify bird sound files from the second dataset (assuming 'Voice of Birds' contains bird sounds)\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        if 'Voice of Birds' in root and name.endswith('.mp3'):\n",
    "            bird_sound_files.append(os.path.join(root, name))\n",
    "\n",
    "# Identify music instrument files from the first dataset (assuming Test_submission and Train_submission contain music instruments)\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        if ('Test_submission' in root or 'Train_submission' in root) and (name.endswith('.wav') or name.endswith('.mp3')):\n",
    "             # Further check if the file name indicates a music instrument\n",
    "            if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords']):\n",
    "                 music_instrument_files.append(os.path.join(root, name))\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "print(\"Extracting features from bird sound files...\")\n",
    "for i, file_path in enumerate(bird_sound_files):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processing bird file {i + 1}/{len(bird_sound_files)}\")\n",
    "    mfccs = extract_features(file_path, n_mfcc=n_mfcc)\n",
    "    if mfccs is not None:\n",
    "        features.append(mfccs)\n",
    "        labels.append('bird')\n",
    "\n",
    "print(\"\\nExtracting features from music instrument files...\")\n",
    "for i, file_path in enumerate(music_instrument_files):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processing music instrument file {i + 1}/{len(music_instrument_files)}\")\n",
    "    mfccs = extract_features(file_path, n_mfcc=n_mfcc)\n",
    "    if mfccs is not None:\n",
    "        features.append(mfccs)\n",
    "        labels.append('music')\n",
    "\n",
    "# Convert features and labels to numpy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "label_map = {'bird': 0, 'music': 1}\n",
    "numerical_labels = np.array([label_map[label] for label in labels])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, numerical_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nFeature extraction complete.\")\n",
    "print(f\"Total samples: {len(features)}\")\n",
    "print(f\"Shape of features array: {features.shape}\")\n",
    "print(f\"Shape of labels array: {labels.shape}\")\n",
    "print(f\"Shape of training features: {X_train.shape}\")\n",
    "print(f\"Shape of testing features: {X_test.shape}\")\n",
    "\n",
    "def generate_mfccs(model, start_sequence, num_steps):\n",
    "    \"\"\"Generates new MFCC sequences using the trained GRU model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras model (GRU in this case).\n",
    "        start_sequence: A numpy array representing the initial sequence\n",
    "                        (shape: (1, sequence_length, n_mfcc)).\n",
    "        num_steps: The number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated MFCC sequence.\n",
    "    \"\"\"\n",
    "    generated_sequence = np.copy(start_sequence)\n",
    "    current_sequence = np.copy(start_sequence)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        # Predict the next time step\n",
    "        predicted_mfcc = model.predict(current_sequence)\n",
    "\n",
    "        # Append the predicted MFCC to the generated sequence\n",
    "        generated_sequence = np.append(generated_sequence, predicted_mfcc, axis=1)\n",
    "\n",
    "        # Update the current sequence by removing the oldest time step and adding the new one\n",
    "        current_sequence = np.append(current_sequence[:, 1:, :], predicted_mfcc, axis=1)\n",
    "\n",
    "    return generated_sequence.squeeze() # Remove the batch dimension\n",
    "\n",
    "# Generate a starting sequence (e.g., take a random sample from the training data)\n",
    "# Ensure the starting sequence has the correct shape (1, sequence_length, n_mfcc)\n",
    "start_index = np.random.randint(0, X_train.shape[0])\n",
    "start_sequence = X_train[start_index:start_index+1, :, :]\n",
    "\n",
    "# Define the number of steps to generate\n",
    "generation_steps = 200 # Generate 200 additional time steps\n",
    "\n",
    "# Generate new MFCC features using the GRU model\n",
    "generated_mfccs = generate_mfccs(model_gen_gru, start_sequence, generation_steps)\n",
    "\n",
    "print(f\"Shape of the generated MFCCs: {generated_mfccs.shape}\")\n",
    "\n",
    "# Define the sampling rate (assuming it was 22050 Hz, a common rate, adjust if a different rate was used during extraction)\n",
    "sr = 22050\n",
    "\n",
    "# Convert the generated MFCCs back to an audio time-series\n",
    "# We need to transpose the generated_mfccs back to the shape (n_mfcc, time_steps)\n",
    "audio_time_series = librosa.feature.inverse.mfcc_to_audio(generated_mfccs.T, sr=sr)\n",
    "\n",
    "# Define the output filename\n",
    "output_filename = 'generated_music.wav'\n",
    "\n",
    "# Save the generated audio to a WAV file\n",
    "sf.write(output_filename, audio_time_series, sr)\n",
    "\n",
    "print(f\"Generated audio saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPAoEv-Cdjrj"
   },
   "outputs": [],
   "source": [
    "# Define the sampling rate (assuming it was 22050 Hz, a common rate, adjust if a different rate was used during extraction)\n",
    "sr = 22050\n",
    "\n",
    "# Convert the generated MFCCs back to an audio time-series\n",
    "audio_time_series = librosa.feature.inverse.mfcc_to_audio(generated_mfccs.T, sr=sr)\n",
    "\n",
    "# Define the output filename\n",
    "output_filename = 'generated_music.wav'\n",
    "\n",
    "# Save the generated audio to a WAV file\n",
    "sf.write(output_filename, audio_time_series, sr)\n",
    "\n",
    "print(f\"Generated audio saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRgPCl_dfzBY"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, TimeDistributed, Input\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This will need to be determined based on the desired sequence length or maximum length of audio files\n",
    "\n",
    "def extract_features(file_path, n_mfcc=20):\n",
    "    \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Pad or truncate MFCCs to a fixed length\n",
    "        if mfccs.shape[1] < max_padding:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, max_padding - mfccs.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :max_padding]\n",
    "\n",
    "        return mfccs.T # Transpose to have shape (time steps, n_mfcc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define the paths to the extracted datasets\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck'\n",
    "\n",
    "bird_sound_files = []\n",
    "music_instrument_files = []\n",
    "\n",
    "# Identify bird sound files from the second dataset (assuming 'Voice of Birds' contains bird sounds)\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        if 'Voice of Birds' in root and name.endswith('.mp3'):\n",
    "            bird_sound_files.append(os.path.join(root, name))\n",
    "\n",
    "# Identify music instrument files from the first dataset (assuming Test_submission and Train_submission contain music instruments)\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        if ('Test_submission' in root or 'Train_submission' in root) and (name.endswith('.wav') or name.endswith('.mp3')):\n",
    "             # Further check if the file name indicates a music instrument\n",
    "            if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords']):\n",
    "                 music_instrument_files.append(os.path.join(root, name))\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "print(\"Extracting features from bird sound files...\")\n",
    "for i, file_path in enumerate(bird_sound_files):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processing bird file {i + 1}/{len(bird_sound_files)}\")\n",
    "    mfccs = extract_features(file_path, n_mfcc=n_mfcc)\n",
    "    if mfccs is not None:\n",
    "        features.append(mfccs)\n",
    "        labels.append('bird')\n",
    "\n",
    "print(\"\\nExtracting features from music instrument files...\")\n",
    "for i, file_path in enumerate(music_instrument_files):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processing music instrument file {i + 1}/{len(music_instrument_files)}\")\n",
    "    mfccs = extract_features(file_path, n_mfcc=n_mfcc)\n",
    "    if mfccs is not None:\n",
    "        features.append(mfccs)\n",
    "        labels.append('music')\n",
    "\n",
    "# Convert features and labels to numpy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "label_map = {'bird': 0, 'music': 1}\n",
    "numerical_labels = np.array([label_map[label] for label in labels])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, numerical_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nFeature extraction complete.\")\n",
    "print(f\"Total samples: {len(features)}\")\n",
    "print(f\"Shape of features array: {features.shape}\")\n",
    "print(f\"Shape of labels array: {labels.shape}\")\n",
    "print(f\"Shape of training features: {X_train.shape}\")\n",
    "print(f\"Shape of testing features: {X_test.shape}\")\n",
    "\n",
    "# Define a new GRU model architecture for sequence generation\n",
    "model_gen_gru = Sequential([\n",
    "    # Add an Input layer explicitly\n",
    "    Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    # GRU layer to process the input sequences\n",
    "    GRU(128, activation='relu', return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # Another GRU layer\n",
    "    GRU(128, activation='relu', return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # TimeDistributed Dense layer to output features for each time step\n",
    "    TimeDistributed(Dense(X_train.shape[2], activation='linear'))\n",
    "])\n",
    "\n",
    "# Compile the GRU model with gradient clipping\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=1.0) # Add gradient clipping\n",
    "model_gen_gru.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Train the GRU generation model\n",
    "print(\"\\nTraining the GRU generation model...\")\n",
    "history_gen_gru = model_gen_gru.fit(X_train, X_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "\n",
    "def generate_mfccs(model, start_sequence, num_steps):\n",
    "    \"\"\"Generates new MFCC sequences using the trained GRU model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras model (GRU in this case).\n",
    "        start_sequence: A numpy array representing the initial sequence\n",
    "                        (shape: (1, sequence_length, n_mfcc)).\n",
    "        num_steps: The number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated MFCC sequence.\n",
    "    \"\"\"\n",
    "    generated_sequence = np.copy(start_sequence)\n",
    "    current_sequence = np.copy(start_sequence)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        # Predict the next time step\n",
    "        predicted_mfcc = model.predict(current_sequence)\n",
    "\n",
    "        # Append the predicted MFCC to the generated sequence\n",
    "        generated_sequence = np.append(generated_sequence, predicted_mfcc, axis=1)\n",
    "\n",
    "        # Update the current sequence by removing the oldest time step and adding the new one\n",
    "        current_sequence = np.append(current_sequence[:, 1:, :], predicted_mfcc, axis=1)\n",
    "\n",
    "    return generated_sequence.squeeze() # Remove the batch dimension\n",
    "\n",
    "# Generate a starting sequence (e.g., take a random sample from the training data)\n",
    "# Ensure the starting sequence has the correct shape (1, sequence_length, n_mfcc)\n",
    "start_index = np.random.randint(0, X_train.shape[0])\n",
    "start_sequence = X_train[start_index:start_index+1, :, :]\n",
    "\n",
    "# Define the number of steps to generate\n",
    "generation_steps = 200 # Generate 200 additional time steps\n",
    "\n",
    "# Generate new MFCC features using the GRU model\n",
    "print(\"\\nGenerating new MFCC features...\")\n",
    "generated_mfccs = generate_mfccs(model_gen_gru, start_sequence, generation_steps)\n",
    "\n",
    "print(f\"Shape of the generated MFCCs: {generated_mfccs.shape}\")\n",
    "\n",
    "# Define the sampling rate (assuming it was 22050 Hz, a common rate, adjust if a different rate was used during extraction)\n",
    "sr = 22050\n",
    "\n",
    "# Convert the generated MFCCs back to an audio time-series\n",
    "# We need to transpose the generated_mfccs back to the shape (n_mfcc, time_steps)\n",
    "print(\"\\nConverting MFCCs to audio...\")\n",
    "audio_time_series = librosa.feature.inverse.mfcc_to_audio(generated_mfccs.T, sr=sr)\n",
    "\n",
    "# Define the output filename\n",
    "output_filename = 'generated_music.wav'\n",
    "\n",
    "# Save the generated audio to a WAV file\n",
    "print(f\"Saving generated audio to {output_filename}...\")\n",
    "sf.write(output_filename, audio_time_series, sr)\n",
    "\n",
    "print(f\"Generated audio saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c2c5188"
   },
   "source": [
    "**Reasoning**:\n",
    "Load the data from the CSV file into a pandas DataFrame and display the first few rows to inspect the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAJHO4em1AY5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/tmp/data.csv')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKh9t4a71OxV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_paths = ['/data/data.csv', 'data.csv']\n",
    "data_file_path = None\n",
    "\n",
    "for path in file_paths:\n",
    "    if os.path.exists(path):\n",
    "        data_file_path = path\n",
    "        break\n",
    "\n",
    "if data_file_path:\n",
    "    df = pd.read_csv(data_file_path)\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"Error: data.csv not found in any of the expected locations.\")\n",
    "    df = None # Set df to None to indicate failure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fa776df"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, TimeDistributed, Input\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This will need to be determined based on the desired sequence length or maximum length of audio files\n",
    "\n",
    "def extract_features(file_path, n_mfcc=20):\n",
    "    \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Pad or truncate MFCCs to a fixed length\n",
    "        if mfccs.shape[1] < max_padding:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, max_padding - mfccs.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :max_padding]\n",
    "\n",
    "        return mfccs.T # Transpose to have shape (time steps, n_mfcc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define the paths to the extracted datasets\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck'\n",
    "\n",
    "bird_sound_files = []\n",
    "music_instrument_files = []\n",
    "\n",
    "# Identify bird sound files from the second dataset (assuming 'Voice of Birds' contains bird sounds)\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        if 'Voice of Birds' in root and name.endswith('.mp3'):\n",
    "            bird_sound_files.append(os.path.join(root, name))\n",
    "\n",
    "# Identify music instrument files from the first dataset (assuming Test_submission and Train_submission contain music instruments)\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        if ('Test_submission' in root or 'Train_submission' in root) and (name.endswith('.wav') or name.endswith('.mp3')):\n",
    "             # Further check if the file name indicates a music instrument\n",
    "            if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords']):\n",
    "                 music_instrument_files.append(os.path.join(root, name))\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "print(\"Extracting features from bird sound files...\")\n",
    "for i, file_path in enumerate(bird_sound_files):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processing bird file {i + 1}/{len(bird_sound_files)}\")\n",
    "    mfccs = extract_features(file_path, n_mfcc=n_mfcc)\n",
    "    if mfccs is not None:\n",
    "        features.append(mfccs)\n",
    "        labels.append('bird')\n",
    "\n",
    "print(\"\\nExtracting features from music instrument files...\")\n",
    "for i, file_path in enumerate(music_instrument_files):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processing music instrument file {i + 1}/{len(music_instrument_files)}\")\n",
    "    mfccs = extract_features(file_path, n_mfcc=n_mfcc)\n",
    "    if mfccs is not None:\n",
    "        features.append(mfccs)\n",
    "        labels.append('music')\n",
    "\n",
    "# Convert features and labels to numpy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "label_map = {'bird': 0, 'music': 1}\n",
    "numerical_labels = np.array([label_map[label] for label in labels])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, numerical_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nFeature extraction complete.\")\n",
    "print(f\"Total samples: {len(features)}\")\n",
    "print(f\"Shape of features array: {features.shape}\")\n",
    "print(f\"Shape of labels array: {labels.shape}\")\n",
    "print(f\"Shape of training features: {X_train.shape}\")\n",
    "print(f\"Shape of testing features: {X_test.shape}\")\n",
    "\n",
    "# Define a new GRU model architecture for sequence generation\n",
    "model_gen_gru = Sequential([\n",
    "    # Add an Input layer explicitly\n",
    "    Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    # GRU layer to process the input sequences\n",
    "    GRU(128, activation='relu', return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # Another GRU layer\n",
    "    GRU(128, activation='relu', return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    # TimeDistributed Dense layer to output features for each time step\n",
    "    TimeDistributed(Dense(X_train.shape[2], activation='linear'))\n",
    "])\n",
    "\n",
    "# Compile the GRU model with gradient clipping\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=1.0) # Add gradient clipping\n",
    "model_gen_gru.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Train the GRU generation model\n",
    "print(\"\\nTraining the GRU generation model...\")\n",
    "history_gen_gru = model_gen_gru.fit(X_train, X_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "\n",
    "def generate_mfccs(model, start_sequence, num_steps):\n",
    "    \"\"\"Generates new MFCC sequences using the trained GRU model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras model (GRU in this case).\n",
    "        start_sequence: A numpy array representing the initial sequence\n",
    "                        (shape: (1, sequence_length, n_mfcc)).\n",
    "        num_steps: The number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated MFCC sequence.\n",
    "    \"\"\"\n",
    "    generated_sequence = np.copy(start_sequence)\n",
    "    current_sequence = np.copy(start_sequence)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        # Predict the next time step\n",
    "        predicted_mfcc = model.predict(current_sequence)\n",
    "\n",
    "        # Append the predicted MFCC to the generated sequence\n",
    "        generated_sequence = np.append(generated_sequence, predicted_mfcc, axis=1)\n",
    "\n",
    "        # Update the current sequence by removing the oldest time step and adding the new one\n",
    "        current_sequence = np.append(current_sequence[:, 1:, :], predicted_mfcc, axis=1)\n",
    "\n",
    "    return generated_sequence.squeeze() # Remove the batch dimension\n",
    "\n",
    "# Generate a starting sequence (e.g., take a random sample from the training data)\n",
    "# Ensure the starting sequence has the correct shape (1, sequence_length, n_mfcc)\n",
    "start_index = np.random.randint(0, X_train.shape[0])\n",
    "start_sequence = X_train[start_index:start_index+1, :, :]\n",
    "\n",
    "# Define the number of steps to generate\n",
    "generation_steps = 200 # Generate 200 additional time steps\n",
    "\n",
    "# Generate new MFCC features using the GRU model\n",
    "print(\"\\nGenerating new MFCC features...\")\n",
    "generated_mfccs = generate_mfccs(model_gen_gru, start_sequence, generation_steps)\n",
    "\n",
    "print(f\"Shape of the generated MFCCs: {generated_mfccs.shape}\")\n",
    "\n",
    "# Define the sampling rate (assuming it was 22050 Hz, a common rate, adjust if a different rate was used during extraction)\n",
    "sr = 22050\n",
    "\n",
    "# Convert the generated MFCCs back to an audio time-series\n",
    "# We need to transpose the generated_mfccs back to the shape (n_mfcc, time_steps)\n",
    "print(\"\\nConverting MFCCs to audio...\")\n",
    "audio_time_series = librosa.feature.inverse.mfcc_to_audio(generated_mfccs.T, sr=sr)\n",
    "\n",
    "# Define the output filename\n",
    "output_filename = 'generated_music.wav'\n",
    "\n",
    "# Save the generated audio to a WAV file\n",
    "print(f\"Saving generated audio to {output_filename}...\")\n",
    "sf.write(output_filename, audio_time_series, sr)\n",
    "\n",
    "print(f\"Generated audio saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3a0470ee"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck'\n",
    "\n",
    "print(f\"Files in '{extracted_path_one}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_two}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50f5c803"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "archive_path_one = '/content/archive.zip'\n",
    "archive_path_two = '/content/dataset_second.zip'\n",
    "\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck'\n",
    "\n",
    "os.makedirs(extracted_path_one, exist_ok=True)\n",
    "os.makedirs(extracted_path_two, exist_ok=True)\n",
    "\n",
    "print(f\"Attempting to re-extract '{archive_path_one}' to '{extracted_path_one}'\")\n",
    "try:\n",
    "    with zipfile.ZipFile(archive_path_one, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_path_one)\n",
    "    print(\"Extraction of archive.zip successful.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: '{archive_path_one}' is a bad zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction of '{archive_path_one}': {e}\")\n",
    "\n",
    "\n",
    "print(f\"\\nAttempting to re-extract '{archive_path_two}' to '{extracted_path_two}'\")\n",
    "try:\n",
    "    with zipfile.ZipFile(archive_path_two, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_path_two)\n",
    "    print(\"Extraction of dataset_second.zip successful.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: '{archive_path_two}' is a bad zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction of '{archive_path_two}': {e}\")\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_one}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_two}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b83adc02"
   },
   "outputs": [],
   "source": [
    "!curl -L \"https://storage.googleapis.com/kaggle-data-sets/2768513/4784071/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250827%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250827T024359Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=1a1ddbaf1b0e32e9aabd16dd7e2f70fd8fe2ae40dcdf8e65453881aba3d1db0ccc133c315998513fdbb1093f834d23234a8a8ab08924dcc653b3b75d0841cf7e6d013fd68b33d60fee3a355e88b56f26362ff5f6a57d1f50c75d8a8e64bb28db4ba8918a3ffce070c310fd1c12f0648ce4b3768481ca3faceefbe0ae1d9d5db37123030768f70da56cae3988736fa857ce149fc39f0489b001495f73889e5bbb0fa093f3b0fb5e9854fad6f9f74115979c4747898315a284f42df649583810b62e7c86b49d2ec2c98c3203ff90f7d2c19201b15b9fb57386197c55f28a0c56c6a537e100a85647b53b8307ddfa00810766c7a248fa3b81c2f6edd3ead8d00da\" -o archive.zip\n",
    "!curl -L \"https://storage.googleapis.com/kaggle-data-sets/2750746/4753077/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250827%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250827T024259Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=6a1bc74b093049c74c92a8d2d572a9222a8e5f225175c3d71bdcf9ddd1371b56382e176ad942bfab0db6351eaec280cbf9cb6f4b3a99cf96d4c73753f05d54958db32fbf765280cc47b280da3700caf22c84590285ec390dd27e57c4ac6152107cd8148ee15da87e2f2a42a7fc059d1ee65825bd969480898d9d06d8969b16c475c55981300d3ec48520fad01323a62a45c82c8b193f6e11c83bae0c2ca63174f142f1b51ed209243c7dec994ed256ef1ca71868bf08f6bbcb5e71241ff9e694a599ddc3d8ceb1cad9ef99ae0614b0fe01cec125d880870532b54549238fbb69f0f36edaddf7799bdea379965474d31f74306d81d4abaa3499d1fee2c29125c5\" -o dataset_second.zip\n",
    "import zipfile\n",
    "\n",
    "archive_path_one = '/content/archive.zip'\n",
    "archive_path_two = '/content/dataset_second.zip'\n",
    "\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck'\n",
    "\n",
    "os.makedirs(extracted_path_one, exist_ok=True)\n",
    "os.makedirs(extracted_path_two, exist_ok=True)\n",
    "\n",
    "print(f\"Attempting to re-extract '{archive_path_one}' to '{extracted_path_one}'\")\n",
    "try:\n",
    "    with zipfile.ZipFile(archive_path_one, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_path_one)\n",
    "    print(\"Extraction of archive.zip successful.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: '{archive_path_one}' is a bad zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction of '{archive_path_one}': {e}\")\n",
    "\n",
    "\n",
    "print(f\"\\nAttempting to re-extract '{archive_path_two}' to '{extracted_path_two}'\")\n",
    "try:\n",
    "    with zipfile.ZipFile(archive_path_two, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_path_two)\n",
    "    print(\"Extraction of dataset_second.zip successful.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: '{archive_path_two}' is a bad zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction of '{archive_path_two}': {e}\")\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_one}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_two}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "962d8464"
   },
   "outputs": [],
   "source": [
    "# !curl -L \"https://storage.googleapis.com/kaggle-data-sets/2768513/4784071/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250827%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250827T024359Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=1a1ddbaf1b0e32e9aabd16dd7e2f70fd8fe2ae40dcdf8e65453881aba3d1db0ccc133c315998513fdbb1093f834d23234a8a8ab08924dcc653b3b75d0841cf7e6d013fd68b33d60fee3a355e88b56f26362ff5f6a57d1f50c75d8a8e64bb28db4ba8918a3ffce070c310fd1c12f0648ce4b3768481ca3faceefbe0ae1d9d5db37123030768f70da56cae3988736fa857ce149fc39f0489b001495f73889e5bbb0fa093f3b0fb5e9854fad6f9f74115979c4747898315a284f42df649583810b62e7c86b49d2ec2c98c3203ff90f7d2c19201b15b9fb57386197c55f28a0c56c6a537e100a85647b53b8307ddfa00810766c7a248fa3b81c2f6edd3ead8d00da\" -o archive.zip\n",
    "# !curl -L \"https://storage.googleapis.com/kaggle-data-sets/2750746/4753077/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250827%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250827T024259Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=6a1bc74b093049c74c92a8d2d572a9222a8e5f225175c3d71bdcf9ddd1371b56382e176ad942bfab0db6351eaec280cbf9cb6f4b3a99cf96d4c73753f05d54958db32fbf765280cc47b280da3700caf22c84590285ec390dd27e57c4ac6152107cd8148ee15da87e2f2a42a7fc059d1ee65825bd969480898d9d06d8969b16c475c55981300d3ec48520fad01323a62a45c82c8b193f6e11c83bae0c2ca63174f142f1b51ed209243c7dec994ed256ef1ca71868bf08f6bbcb5e71241ff9e694a599ddc3d8ceb1cad9ef99ae0614b0fe01cec125d880870532b54549238fbb69f0f36edaddf7799bdea379965474d31f74306d81d4abaa3499d1fee2c29125c5\" -o dataset_second.zip\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "archive_path_one = '/content/archive.zip'\n",
    "archive_path_two = '/content/dataset_second.zip'\n",
    "\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck'\n",
    "\n",
    "os.makedirs(extracted_path_one, exist_ok=True)\n",
    "os.makedirs(extracted_path_two, exist_ok=True)\n",
    "\n",
    "print(f\"Attempting to re-extract '{archive_path_one}' to '{extracted_path_one}'\")\n",
    "try:\n",
    "    with zipfile.ZipFile(archive_path_one, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_path_one)\n",
    "    print(\"Extraction of archive.zip successful.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: '{archive_path_one}' is a bad zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction of '{archive_path_one}': {e}\")\n",
    "\n",
    "\n",
    "print(f\"\\nAttempting to re-extract '{archive_path_two}' to '{extracted_path_two}'\")\n",
    "try:\n",
    "    with zipfile.ZipFile(archive_path_two, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_path_two)\n",
    "    print(\"Extraction of dataset_second.zip successful.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: '{archive_path_two}' is a bad zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction of '{archive_path_two}': {e}\")\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_one}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_two}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-0MzP8PU7SK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFGMnRwWVfq9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5cd8ebdf"
   },
   "outputs": [],
   "source": [
    "!curl -L \"https://storage.googleapis.com/kaggle-data-sets/2768513/4784071/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250827%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250827T024359Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=1a1ddbaf1b0e32e9aabd16dd7e2f70fd8fe2ae40dcdf8e65453881aba3d1db0ccc133c315998513fdbb1093f834d23234a8a8ab08924dcc653b3b75d0841cf7e6d013fd68b33d60fee3a355e88b56f26362ff5f6a57d1f50c75d8a8e64bb28db4ba8918a3ffce070c310fd1c12f0648ce4b3768481ca3faceefbe0ae1d9d5db37123030768f70da56cae3988736fa857ce149fc39f0489b001495f73889e5bbb0fa093f3b0fb5e9854fad6fe9f74115979c4747898315a284f42df649583810b62e7c86b49d2ec2c98c3203ff90f7d2c19201b15b9fb57386197c55f28a0c56c6a537e100a85647b53b8307ddfa00810766c7a248fa3b81c2f6edd3ead8d00da\" -o archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c2ffd9d"
   },
   "outputs": [],
   "source": [
    "# Analyzing user input types and proposing conditioning methods\n",
    "\n",
    "# User Input Types:\n",
    "# 1. Lyrics (Text): User provides existing lyrics.\n",
    "# 2. Lyric Generation Request (Text Prompt): User provides a text prompt to generate lyrics.\n",
    "# 3. User-Recorded Audio (Audio): User provides an audio recording.\n",
    "\n",
    "# Methods for Transforming User Input for Conditioning:\n",
    "\n",
    "# 1. Text Inputs (Lyrics and Lyric Generation Prompts):\n",
    "#    - Text Embeddings: Use pre-trained word embeddings (e.g., GloVe, Word2Vec) or transformer-based embeddings (e.g., Sentence-BERT, a smaller version of BERT) to convert text into dense vectors that capture semantic meaning. These embeddings can represent individual words, phrases, or the entire input text.\n",
    "#    - Symbolic Representations: For simpler conditioning based on keywords or themes, one-hot encoding or a custom vocabulary mapping could be used, although embeddings are generally preferred for capturing richer meaning.\n",
    "\n",
    "# 2. User-Recorded Audio Input:\n",
    "#    - Audio Feature Extraction: Extract relevant audio features that represent the characteristics of the user's audio. MFCCs (already used for the music data) are a good candidate. Other possibilities include chroma features (representing the pitch content) or spectrograms (visual representation of frequency over time). The choice of features might depend on what aspects of the user's audio we want to influence the generated music (e.g., melody, rhythm, timbre).\n",
    "\n",
    "# Strategies for Incorporating Transformed User Inputs into the Model:\n",
    "\n",
    "# Assuming a sequence generation model (like the GRU model we've been working with) that generates a sequence of musical features (e.g., MFCCs):\n",
    "\n",
    "# 1. Concatenation:\n",
    "#    - Concatenate the transformed user input features with the musical features at the input layer or at intermediate layers of the generator model. For sequence inputs (like MFCCs from user audio or sequential text embeddings), this would involve aligning the time steps. For a single embedding representing the entire input text, this embedding could be repeated across all time steps or used in a more sophisticated way.\n",
    "\n",
    "# 2. Initial State Conditioning:\n",
    "#    - Use the transformed user input features as the initial state (hidden state and/or cell state for LSTMs) of the recurrent layers (GRU or LSTM) in the generator model. This allows the input to influence the initial \"memory\" of the sequence generation process.\n",
    "\n",
    "# 3. Attention Mechanisms:\n",
    "#    - Implement attention mechanisms that allow the generator model to selectively focus on different parts of the user input sequence (if it's a sequence like audio features or sequential text embeddings) while generating each time step of the music. This can help in aligning musical elements with specific parts of the user input.\n",
    "\n",
    "# 4. Conditional Generation Architectures:\n",
    "#    - Explore conditional variants of generative models. For example, in a Conditional GAN, the generator and discriminator would take the user input as an additional input, guiding the generation of music that is conditioned on the input. In a Conditional VAE, the encoder and decoder would be conditioned on the user input.\n",
    "\n",
    "# Proposed Approach (combining strategies):\n",
    "\n",
    "# For Text Inputs (Lyrics/Prompts):\n",
    "# - Use a pre-trained transformer-based text embedding model to get a fixed-size vector representation of the input text.\n",
    "# - This embedding vector can be used to condition the GRU model.\n",
    "\n",
    "# For User-Recorded Audio Input:\n",
    "# - Extract MFCC features from the user's audio, similar to how the music data was processed.\n",
    "# - This sequence of MFCCs can be used to condition the GRU model.\n",
    "\n",
    "# Conditioning the GRU Model:\n",
    "# A flexible approach would be to use the user input (either the text embedding or the audio MFCCs) to influence the initial state of the GRU layers and potentially concatenate it at the input layer, possibly with an attention mechanism for audio input.\n",
    "\n",
    "# Example Sketch of a Conditioned GRU Model (Conceptual):\n",
    "# Input: User Input (e.g., text embedding or audio MFCCs)\n",
    "# Input: Musical Features (MFCCs)\n",
    "\n",
    "# Conditioned GRU Layer:\n",
    "# - The initial state of the GRU layer is computed based on the user input.\n",
    "# - The GRU layer processes the musical features sequence, influenced by the initial state.\n",
    "\n",
    "# Output: Generated Musical Features (MFCCs)\n",
    "\n",
    "# The specific implementation would involve defining a custom Keras layer or model that handles the conditioning input and integrates it with the GRU layers.\n",
    "\n",
    "print(\"Analysis of user input types and proposed conditioning methods complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6446467"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, GRU, Input, TimeDistributed\n",
    "\n",
    "# Define the dimensions of the musical features (MFCCs) and the conditioning input\n",
    "musical_feature_dim = X_train.shape[2] # Number of MFCCs\n",
    "musical_sequence_length = X_train.shape[1] # Length of the MFCC sequences\n",
    "\n",
    "# Assuming a fixed size for the text embedding if text input is used.\n",
    "# If using audio conditioning, the shape would be (sequence_length, n_mfcc)\n",
    "# Let's assume for now a single conditioning vector (e.g., a text embedding)\n",
    "# We'll need to adjust this if we use sequential audio conditioning.\n",
    "conditioning_feature_dim = 128 # Example dimension for a text embedding\n",
    "\n",
    "# Define the input layers\n",
    "musical_input = Input(shape=(musical_sequence_length, musical_feature_dim), name='musical_input')\n",
    "conditioning_input = Input(shape=(conditioning_feature_dim,), name='conditioning_input')\n",
    "\n",
    "# Repeat the conditioning input across the musical sequence length\n",
    "# This is one way to integrate a single conditioning vector\n",
    "repeated_conditioning = tf.keras.layers.RepeatVector(musical_sequence_length)(conditioning_input)\n",
    "\n",
    "# Concatenate the musical input with the repeated conditioning input\n",
    "# The conditioning input is added as additional features at each time step\n",
    "combined_input = Concatenate(axis=-1)([musical_input, repeated_conditioning])\n",
    "\n",
    "# GRU layers\n",
    "gru_1 = GRU(128, activation='relu', return_sequences=True)(combined_input)\n",
    "dropout_1 = Dropout(0.2)(gru_1)\n",
    "\n",
    "gru_2 = GRU(128, activation='relu', return_sequences=True)(dropout_1)\n",
    "dropout_2 = Dropout(0.2)(gru_2)\n",
    "\n",
    "# TimeDistributed Dense layer to output the generated musical features\n",
    "output_layer = TimeDistributed(Dense(musical_feature_dim, activation='linear'))(dropout_2)\n",
    "\n",
    "# Define the conditioned generation model\n",
    "model_gen_conditioned = Model(inputs=[musical_input, conditioning_input], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=1.0) # Add gradient clipping\n",
    "model_gen_conditioned.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Print the model summary\n",
    "model_gen_conditioned.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ok8HVm5ctCz3"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, GRU, Input, TimeDistributed\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This will need to be determined based on the desired sequence length or maximum length of audio files\n",
    "\n",
    "def extract_features(file_path, n_mfcc=20):\n",
    "    \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Pad or truncate MFCCs to a fixed length\n",
    "        if mfccs.shape[1] < max_padding:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, max_padding - mfccs.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :max_padding]\n",
    "\n",
    "        return mfccs.T # Transpose to have shape (time steps, n_mfcc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define the paths to the extracted datasets\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck'\n",
    "\n",
    "bird_sound_files = []\n",
    "music_instrument_files = []\n",
    "\n",
    "# Identify bird sound files from the second dataset (assuming 'Voice of Birds' contains bird sounds)\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        if 'Voice of Birds' in root and name.endswith('.mp3'):\n",
    "            bird_sound_files.append(os.path.join(root, name))\n",
    "\n",
    "# Identify music instrument files from the first dataset (assuming Test_submission and Train_submission contain music instruments)\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        if ('Test_submission' in root or 'Train_submission' in root) and (name.endswith('.wav') or name.endswith('.mp3')):\n",
    "             # Further check if the file name indicates a music instrument\n",
    "            if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords']):\n",
    "                 music_instrument_files.append(os.path.join(root, name))\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "print(\"Extracting features from bird sound files...\")\n",
    "for i, file_path in enumerate(bird_sound_files):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processing bird file {i + 1}/{len(bird_sound_files)}\")\n",
    "    mfccs = extract_features(file_path, n_mfcc=n_mfcc)\n",
    "    if mfccs is not None:\n",
    "        features.append(mfccs)\n",
    "        labels.append('bird')\n",
    "\n",
    "print(\"\\nExtracting features from music instrument files...\")\n",
    "for i, file_path in enumerate(music_instrument_files):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processing music instrument file {i + 1}/{len(music_instrument_files)}\")\n",
    "    mfccs = extract_features(file_path, n_mfcc=n_mfcc)\n",
    "    if mfccs is not None:\n",
    "        features.append(mfccs)\n",
    "        labels.append('music')\n",
    "\n",
    "# Convert features and labels to numpy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Convert labels to numerical format\n",
    "label_map = {'bird': 0, 'music': 1}\n",
    "numerical_labels = np.array([label_map[label] for label in labels])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, numerical_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nFeature extraction complete.\")\n",
    "print(f\"Total samples: {len(features)}\")\n",
    "print(f\"Shape of features array: {features.shape}\")\n",
    "print(f\"Shape of labels array: {labels.shape}\")\n",
    "print(f\"Shape of training features: {X_train.shape}\")\n",
    "print(f\"Shape of testing features: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# Define the dimensions of the musical features (MFCCs) and the conditioning input\n",
    "musical_feature_dim = X_train.shape[2] # Number of MFCCs\n",
    "musical_sequence_length = X_train.shape[1] # Length of the MFCC sequences\n",
    "\n",
    "# Assuming a fixed size for the text embedding if text input is used.\n",
    "# If using audio conditioning, the shape would be (sequence_length, n_mfcc)\n",
    "# Let's assume for now a single conditioning vector (e.g., a text embedding)\n",
    "# We'll need to adjust this if we use sequential audio conditioning.\n",
    "conditioning_feature_dim = 128 # Example dimension for a text embedding\n",
    "\n",
    "# Define the input layers\n",
    "musical_input = Input(shape=(musical_sequence_length, musical_feature_dim), name='musical_input')\n",
    "conditioning_input = Input(shape=(conditioning_feature_dim,), name='conditioning_input')\n",
    "\n",
    "# Repeat the conditioning input across the musical sequence length\n",
    "# This is one way to integrate a single conditioning vector\n",
    "repeated_conditioning = tf.keras.layers.RepeatVector(musical_sequence_length)(conditioning_input)\n",
    "\n",
    "# Concatenate the musical input with the repeated conditioning input\n",
    "# The conditioning input is added as additional features at each time step\n",
    "combined_input = Concatenate(axis=-1)([musical_input, repeated_conditioning])\n",
    "\n",
    "# GRU layers\n",
    "gru_1 = GRU(128, activation='relu', return_sequences=True)(combined_input)\n",
    "dropout_1 = Dropout(0.2)(gru_1)\n",
    "\n",
    "gru_2 = GRU(128, activation='relu', return_sequences=True)(dropout_1)\n",
    "dropout_2 = Dropout(0.2)(gru_2)\n",
    "\n",
    "# TimeDistributed Dense layer to output the generated musical features\n",
    "output_layer = TimeDistributed(Dense(musical_feature_dim, activation='linear'))(dropout_2)\n",
    "\n",
    "# Define the conditioned generation model\n",
    "model_gen_conditioned = Model(inputs=[musical_input, conditioning_input], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=1.0) # Add gradient clipping\n",
    "model_gen_conditioned.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Print the model summary\n",
    "model_gen_conditioned.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2e6dcd53"
   },
   "outputs": [],
   "source": [
    "# Create dummy conditioning data for training\n",
    "# Assuming conditioning_feature_dim is the size of our conditioning vector\n",
    "# For training, we'll just use random vectors with the same number of samples as X_train\n",
    "dummy_conditioning_train = np.random.rand(X_train.shape[0], conditioning_feature_dim)\n",
    "dummy_conditioning_val = np.random.rand(X_val.shape[0], conditioning_feature_dim)\n",
    "\n",
    "\n",
    "# Train the conditioned GRU generation model\n",
    "print(\"\\nTraining the conditioned GRU generation model with dummy input...\")\n",
    "history_gen_conditioned = model_gen_conditioned.fit(\n",
    "    [X_train, dummy_conditioning_train],\n",
    "    X_train,\n",
    "    epochs=50, # Increased epochs as training with dummy data is quick\n",
    "    batch_size=32,\n",
    "    validation_data=([X_val, dummy_conditioning_val], X_val) # Use validation data\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57760808"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q91DtWmFv6Df"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14lBy2LYwFCM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50f1e5cb"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, GRU, Input, TimeDistributed\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This will need to be determined based on the desired sequence length or maximum length of audio files\n",
    "\n",
    "def extract_features(file_path, n_mfcc=20):\n",
    "    \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Pad or truncate MFCCs to a fixed length\n",
    "        if mfccs.shape[1] < max_padding:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, max_padding - mfccs.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :max_padding]\n",
    "\n",
    "        return mfccs.T # Transpose to have shape (time steps, n_mfcc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define the paths to the extracted datasets (using the recheck directories)\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck' # This directory might be empty\n",
    "\n",
    "bird_sound_files = []\n",
    "music_instrument_files = []\n",
    "\n",
    "# Identify bird sound files from the second dataset (assuming 'Voice of Birds' contains bird sounds)\n",
    "# We will still attempt to list, but expect it to be empty based on previous output\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        if 'Voice of Birds' in root and name.endswith('.mp3'):\n",
    "            bird_sound_files.append(os.path.join(root, name))\n",
    "\n",
    "# Identify music instrument files from the first dataset (assuming Test_submission and Train_submission contain music instruments)\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        if ('Test_submission' in root or 'Train_submission' in root) and (name.endswith('.wav') or name.endswith('.mp3')):\n",
    "             # Further check if the file name indicates a music instrument\n",
    "            if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords']):\n",
    "                 music_instrument_files.append(os.path.join(root, name))\n",
    "\n",
    "# Use only music instrument files for feature extraction as bird sounds are not available\n",
    "files_to_process = music_instrument_files\n",
    "\n",
    "features = []\n",
    "labels = [] # Labels will not be used for this generation task, but we keep the structure\n",
    "\n",
    "print(\"Extracting features from music instrument files...\")\n",
    "for i, file_path in enumerate(files_to_process):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processing music instrument file {i + 1}/{len(files_to_process)}\")\n",
    "    mfccs = extract_features(file_path, n_mfcc=n_mfcc)\n",
    "    if mfccs is not None:\n",
    "        features.append(mfccs)\n",
    "        # We don't need labels for the generation model training\n",
    "\n",
    "# Convert features to numpy array\n",
    "features = np.array(features)\n",
    "\n",
    "# Split data into training and a small validation set if needed for monitoring\n",
    "# For an unsupervised generation task, we primarily train on the features themselves\n",
    "# We can split features for training and validation to monitor overfitting\n",
    "X_train, X_val = train_test_split(features, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(\"\\nFeature extraction complete.\")\n",
    "print(f\"Total samples: {len(features)}\")\n",
    "print(f\"Shape of features array: {features.shape}\")\n",
    "print(f\"Shape of training features: {X_train.shape}\")\n",
    "print(f\"Shape of validation features: {X_val.shape}\")\n",
    "\n",
    "\n",
    "# Define the dimensions of the musical features (MFCCs) and the conditioning input\n",
    "musical_feature_dim = X_train.shape[2] # Number of MFCCs\n",
    "musical_sequence_length = X_train.shape[1] # Length of the MFCC sequences\n",
    "\n",
    "# Assuming a fixed size for the text embedding if text input is used.\n",
    "# For now, we'll use a placeholder conditioning dimension as we are training with dummy input\n",
    "conditioning_feature_dim = 128 # Example dimension for a text embedding\n",
    "\n",
    "# Define the input layers\n",
    "musical_input = Input(shape=(musical_sequence_length, musical_feature_dim), name='musical_input')\n",
    "conditioning_input = Input(shape=(conditioning_feature_dim,), name='conditioning_input')\n",
    "\n",
    "# Repeat the conditioning input across the musical sequence length\n",
    "# This is one way to integrate a single conditioning vector\n",
    "repeated_conditioning = tf.keras.layers.RepeatVector(musical_sequence_length)(conditioning_input)\n",
    "\n",
    "# Concatenate the musical input with the repeated conditioning input\n",
    "# The conditioning input is added as additional features at each time step\n",
    "combined_input = Concatenate(axis=-1)([musical_input, repeated_conditioning])\n",
    "\n",
    "# GRU layers\n",
    "gru_1 = GRU(128, activation='relu', return_sequences=True)(combined_input)\n",
    "dropout_1 = Dropout(0.2)(gru_1)\n",
    "\n",
    "gru_2 = GRU(128, activation='relu', return_sequences=True)(dropout_1)\n",
    "dropout_2 = Dropout(0.2)(gru_2)\n",
    "\n",
    "# TimeDistributed Dense layer to output the generated musical features\n",
    "output_layer = TimeDistributed(Dense(musical_feature_dim, activation='linear'))(dropout_2)\n",
    "\n",
    "# Define the conditioned generation model\n",
    "model_gen_conditioned = Model(inputs=[musical_input, conditioning_input], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=1.0) # Add gradient clipping\n",
    "model_gen_conditioned.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Print the model summary\n",
    "model_gen_conditioned.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cc7fea0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import tensorflow as tf # Ensure tensorflow is imported for tf.keras.layers.RepeatVector\n",
    "from tensorflow.keras.layers import Dense # Import Dense for the projection layer\n",
    "\n",
    "# Define a simple projection model/layer to map averaged audio features to conditioning_feature_dim\n",
    "# This needs to be defined outside the function if we want to reuse it.\n",
    "# For simplicity here, let's define a small sequential model\n",
    "# Assuming n_mfcc and conditioning_feature_dim are defined in the notebook.\n",
    "audio_conditioning_projection_model = tf.keras.Sequential([\n",
    "    Input(shape=(n_mfcc,)), # Input shape is the number of MFCCs after averaging over time\n",
    "    Dense(conditioning_feature_dim, activation='relu') # Project to the conditioning feature dimension\n",
    "])\n",
    "\n",
    "\n",
    "def generate_music_from_input(model, user_input, input_type, generation_steps=200, sr=22050, max_padding=174, n_mfcc=20):\n",
    "    \"\"\"Generates music based on user input (text or audio).\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras conditional music generation model.\n",
    "        user_input: The user's input (string for text, file path for audio).\n",
    "        input_type: Type of input ('text_lyrics', 'text_prompt', 'audio').\n",
    "        generation_steps: Number of additional time steps to generate.\n",
    "        sr: Sampling rate for audio output.\n",
    "        max_padding: Target sequence length for padding/truncating features.\n",
    "        n_mfcc: Number of MFCCs extracted.\n",
    "\n",
    "    Returns:\n",
    "        The path to the generated audio file, or None if an error occurred.\n",
    "    \"\"\"\n",
    "    conditioning_input_vector = None # We'll prepare a single vector for conditioning\n",
    "    processed_lyrics = None\n",
    "\n",
    "    if input_type == 'text_lyrics' or input_type == 'text_prompt':\n",
    "        is_prompt = (input_type == 'text_prompt')\n",
    "        # process_text_input already returns a single vector embedding (1, conditioning_feature_dim)\n",
    "        conditioning_input_vector, processed_lyrics = process_text_input(user_input, is_prompt=is_prompt)\n",
    "        if conditioning_input_vector is None:\n",
    "            print(\"Error processing text input.\")\n",
    "            return None\n",
    "    elif input_type == 'audio':\n",
    "        print(\"Processing audio input for conditioning.\")\n",
    "        processed_audio_features_sequence = process_audio_input(user_input, max_padding, n_mfcc)\n",
    "        if processed_audio_features_sequence is None:\n",
    "            print(\"Error processing audio input.\")\n",
    "            return None\n",
    "\n",
    "        # Average the audio features over the time axis to get a single vector per audio sample\n",
    "        averaged_audio_features = np.mean(processed_audio_features_sequence, axis=1) # Shape (1, n_mfcc)\n",
    "\n",
    "        # Project the averaged audio features to the conditioning feature dimension\n",
    "        conditioning_input_vector = audio_conditioning_projection_model.predict(averaged_audio_features) # Shape (1, conditioning_feature_dim)\n",
    "        print(f\"Projected audio conditioning vector shape: {conditioning_input_vector.shape}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid input type. Please use 'text_lyrics', 'text_prompt', or 'audio'.\")\n",
    "        return None\n",
    "\n",
    "    # Ensure conditioning_input_vector is not None before proceeding\n",
    "    if conditioning_input_vector is None:\n",
    "         print(\"Conditioning input vector is None. Cannot proceed.\")\n",
    "         return None\n",
    "\n",
    "\n",
    "    # Generate a starting musical sequence (e.g., take a random sample from the training data)\n",
    "    # Ensure the starting sequence has the correct shape (1, sequence_length, n_mfcc)\n",
    "    if 'X_train' not in globals() or X_train.shape[0] == 0:\n",
    "         print(\"Training data (X_train) is not available or empty. Cannot generate a starting sequence.\")\n",
    "         return None\n",
    "\n",
    "    start_index = np.random.randint(0, X_train.shape[0])\n",
    "    start_sequence = X_train[start_index:start_index+1, :, :]\n",
    "\n",
    "    # Generate new MFCC features using the trained conditional GRU model\n",
    "    print(\"\\nGenerating new conditional MFCC features...\")\n",
    "    generated_mfccs_conditional = generate_conditional_mfccs(model, start_sequence, conditioning_input_vector, generation_steps)\n",
    "\n",
    "    print(f\"Shape of the generated conditional MFCCs: {generated_mfccs_conditional.shape}\")\n",
    "\n",
    "    # Define the sampling rate (assuming it was 22050 Hz, adjust if a different rate was used)\n",
    "    sr = 22050\n",
    "\n",
    "    # Convert the generated MFCCs back to an audio time-series\n",
    "    print(\"\\nConverting conditional MFCCs to audio...\")\n",
    "    audio_time_series_conditional = librosa.feature.inverse.mfcc_to_audio(generated_mfccs_conditional.T, sr=sr)\n",
    "\n",
    "    # Define the output filename\n",
    "    output_filename_conditional = 'generated_music_conditional.wav'\n",
    "\n",
    "    # Save the generated audio to a WAV file\n",
    "    print(f\"Saving generated conditional audio to {output_filename_conditional}...\")\n",
    "    sf.write(output_filename_conditional, audio_time_series_conditional, sr)\n",
    "\n",
    "    print(f\"Generated conditional audio saved successfully to {output_filename_conditional}\")\n",
    "\n",
    "    return output_filename_conditional\n",
    "\n",
    "# --- Example Usage (requires trained model and data) ---\n",
    "# Assuming model_gen_conditioned, X_train, conditioning_feature_dim, max_padding, n_mfcc, sr are defined\n",
    "\n",
    "# Example 1: Generate music from provided lyrics\n",
    "# user_lyrics_input = \"A gentle rain falls on the roof, a soft and steady sound.\"\n",
    "# generated_file_lyrics = generate_music_from_input(model_gen_conditioned, user_lyrics_input, 'text_lyrics',\n",
    "#                                                   generation_steps=200, sr=sr, max_padding=max_padding, n_mfcc=n_mfcc)\n",
    "# if generated_file_lyrics:\n",
    "#     print(f\"\\nMusic generated from lyrics: {generated_file_lyrics}\")\n",
    "\n",
    "# Example 2: Generate music from a lyric generation prompt\n",
    "# user_prompt_input = \"Create a calm and peaceful melody.\"\n",
    "# generated_file_prompt = generate_music_from_input(model_gen_conditioned, user_prompt_input, 'text_prompt',\n",
    "#                                                  generation_steps=200, sr=sr, max_padding=max_padding, n_mfcc=n_mfcc)\n",
    "# if generated_file_prompt:\n",
    "#     print(f\"\\nMusic generated from prompt: {generated_file_prompt}\")\n",
    "\n",
    "# Example 3: Generate music from user audio (using dummy audio file)\n",
    "# Note: Replace 'path/to/your/audio.wav' with the actual path to a user-recorded audio file\n",
    "# We need to provide a dummy audio file path for demonstration if a real one isn't available.\n",
    "# For demonstration, let's create a dummy audio file if it doesn't exist.\n",
    "# This is a placeholder and won't sound like real audio conditioning.\n",
    "dummy_audio_path = '/content/dummy_user_audio.wav'\n",
    "if not os.path.exists(dummy_audio_path):\n",
    "    print(f\"Creating a dummy audio file at {dummy_audio_path} for demonstration.\")\n",
    "    # Create a short silent WAV file as a placeholder\n",
    "    dummy_audio_ts = np.zeros(sr * 5) # 5 seconds of silence\n",
    "    sf.write(dummy_audio_path, dummy_audio_ts, sr)\n",
    "\n",
    "\n",
    "user_audio_input_path = dummy_audio_path # Use the dummy audio file for demonstration\n",
    "print(f\"\\nAttempting to generate music from audio file: {user_audio_input_path}\")\n",
    "\n",
    "if os.path.exists(user_audio_input_path):\n",
    "     generated_file_audio = generate_music_from_input(model_gen_conditioned, user_audio_input_path, 'audio',\n",
    "                                                      generation_steps=200, sr=sr, max_padding=max_padding, n_mfcc=n_mfcc)\n",
    "     if generated_file_audio:\n",
    "         print(f\"\\nMusic generated from audio saved to: {generated_file_audio}\")\n",
    "     else:\n",
    "         print(\"\\nFailed to generate music from audio.\")\n",
    "else:\n",
    "    print(f\"\\nUser audio file not found at {user_audio_input_path}. Skipping audio-conditioned generation example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2104fefe"
   },
   "outputs": [],
   "source": [
    "# Placeholder for a language model (replace with actual model if available)\n",
    "# For demonstration, this function will just return a dummy lyric or the prompt itself\n",
    "def generate_lyrics_with_language_model(prompt):\n",
    "    \"\"\"Placeholder function to generate lyrics based on a prompt using a language model.\"\"\"\n",
    "    print(f\"Generating lyrics for prompt: '{prompt}' using a placeholder model.\")\n",
    "    # In a real scenario, you would integrate a language model here (e.g., fine-tuned GPT-2, T5, or an API call)\n",
    "    # For now, return a simple response or the prompt itself.\n",
    "    return f\"Generated lyrics based on '{prompt}':\\nVerse 1: The rhythm flows, the melody calls\\nChorus: Music fills the air, breaking down walls\"\n",
    "\n",
    "# Function to get text embeddings\n",
    "# We'll use a placeholder for a text embedding model (e.g., Sentence-BERT, or just a simple hashing/vectorization for now)\n",
    "# For a real application, integrate a pre-trained model from libraries like `transformers` or `sentence-transformers`\n",
    "# For demonstration, we'll return a random dummy embedding\n",
    "def get_text_embedding(text):\n",
    "    \"\"\"Placeholder function to get a fixed-size embedding for text.\"\"\"\n",
    "    print(f\"Getting embedding for text: '{text[:50]}...' using a placeholder model.\")\n",
    "    # In a real scenario, use a pre-trained text embedding model\n",
    "    # For now, return a random vector of the size expected by the conditioning input layer\n",
    "    # Assuming conditioning_feature_dim is defined from the model architecture\n",
    "    return np.random.rand(1, conditioning_feature_dim) # Shape (1, conditioning_feature_dim)\n",
    "\n",
    "def process_text_input(user_input, is_prompt=False):\n",
    "    \"\"\"Processes user text input (lyrics or prompt) and returns a conditioning embedding.\"\"\"\n",
    "    if is_prompt:\n",
    "        lyrics = generate_lyrics_with_language_model(user_input)\n",
    "    else:\n",
    "        lyrics = user_input # User provided lyrics directly\n",
    "\n",
    "    # Get the embedding for the lyrics\n",
    "    conditioning_embedding = get_text_embedding(lyrics)\n",
    "\n",
    "    return conditioning_embedding, lyrics\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Assume conditioning_feature_dim is defined from the previously defined model\n",
    "# conditioning_feature_dim = 128 # Make sure this is defined in your notebook\n",
    "\n",
    "# Example 1: User provides lyrics\n",
    "user_lyrics = \"In the quiet of the night, a melody softly plays.\"\n",
    "conditioning_for_lyrics, processed_lyrics = process_text_input(user_lyrics, is_prompt=False)\n",
    "print(f\"\\nProcessed lyrics: {processed_lyrics}\")\n",
    "print(f\"Shape of conditioning embedding for lyrics: {conditioning_for_lyrics.shape}\")\n",
    "\n",
    "# Example 2: User provides a prompt for lyric generation\n",
    "user_prompt = \"Write a short poem about a rainy day.\"\n",
    "conditioning_for_prompt, generated_lyrics = process_text_input(user_prompt, is_prompt=True)\n",
    "print(f\"\\nProcessed (generated) lyrics: {generated_lyrics}\")\n",
    "print(f\"Shape of conditioning embedding for prompt: {conditioning_for_prompt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e477d5de"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def process_audio_input(audio_file_path, target_sequence_length, n_mfcc):\n",
    "    \"\"\"Processes user audio input and returns features suitable for conditioning.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path: Path to the user's audio file.\n",
    "        target_sequence_length: The expected sequence length for model input (e.g., max_padding).\n",
    "        n_mfcc: The number of MFCCs to extract.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the processed audio features (shape: (1, target_sequence_length, n_mfcc)),\n",
    "        or None if processing fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_file_path, sr=None) # Load with original sampling rate\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Pad or truncate MFCCs to the target sequence length\n",
    "        if mfccs.shape[1] < target_sequence_length:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, target_sequence_length - mfccs.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :target_sequence_length]\n",
    "\n",
    "        # Transpose and add a batch dimension\n",
    "        processed_features = mfccs.T[np.newaxis, :, :] # Shape (1, target_sequence_length, n_mfcc)\n",
    "\n",
    "        print(f\"Processed audio features shape: {processed_features.shape}\")\n",
    "        return processed_features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio file {audio_file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Assuming max_padding and n_mfcc are defined from previous steps\n",
    "# max_padding = 174\n",
    "# n_mfcc = 20\n",
    "\n",
    "# To test this, you would need an actual audio file path.\n",
    "# For now, we'll just show the function definition.\n",
    "# Example:\n",
    "# dummy_audio_path = '/content/sample_audio.wav' # Replace with a real path\n",
    "# processed_audio_features = process_audio_input(dummy_audio_path, max_padding, n_mfcc)\n",
    "# if processed_audio_features is not None:\n",
    "#     print(\"Audio processing successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0d48216"
   },
   "outputs": [],
   "source": [
    "# --- Demonstrate Lyric Generation and Music Generation from a Text Prompt ---\n",
    "# Assuming model_gen_conditioned, generate_music_from_input,\n",
    "# conditioning_feature_dim, max_padding, n_mfcc, sr are defined.\n",
    "\n",
    "user_lyric_idea_prompt = \"Write a song about a journey through a forest.\"\n",
    "print(f\"Attempting to generate lyrics and music from prompt: '{user_lyric_idea_prompt}'\")\n",
    "\n",
    "# The generate_music_from_input function with input_type='text_prompt'\n",
    "# will call the placeholder lyric generation and then use the resulting lyrics for conditioning.\n",
    "generated_file_from_lyric_idea = generate_music_from_input(model_gen_conditioned, user_lyric_idea_prompt, 'text_prompt',\n",
    "                                                         generation_steps=200, sr=sr, max_padding=max_padding, n_mfcc=n_mfcc)\n",
    "\n",
    "if generated_file_from_lyric_idea:\n",
    "    print(f\"\\nMusic generated from lyric idea prompt saved to: {generated_file_from_lyric_idea}\")\n",
    "else:\n",
    "    print(\"\\nFailed to generate music from lyric idea prompt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "118ec621"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, GRU, Input, TimeDistributed\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This will need to be determined based on the desired sequence length or maximum length of audio files\n",
    "n_chroma = 12 # Number of chroma bins\n",
    "\n",
    "def extract_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "    \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract Chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "        # Pad or truncate combined features to a fixed length\n",
    "        if combined_features.shape[1] < target_sequence_length:\n",
    "            combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "        else:\n",
    "            combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "        return combined_features.T # Transpose to have shape (time steps, total_features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_user_audio_for_conditioning(audio_file_path, target_sequence_length, n_mfcc, n_chroma):\n",
    "    \"\"\"Processes user audio input and returns features suitable for conditioning.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path: Path to the user's audio file.\n",
    "        target_sequence_length: The expected sequence length for model input (e.g., max_padding).\n",
    "        n_mfcc: The number of MFCCs to extract.\n",
    "        n_chroma: The number of chroma bins to extract.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the processed audio features (shape: (1, target_sequence_length, total_feature_dimension)),\n",
    "        or None if processing fails.\n",
    "    \"\"\"\n",
    "    processed_features = extract_features(audio_file_path, target_sequence_length, n_mfcc, n_chroma)\n",
    "\n",
    "    if processed_features is not None:\n",
    "        # Add a batch dimension\n",
    "        processed_features = processed_features[np.newaxis, :, :] # Shape (1, target_sequence_length, total_feature_dimension)\n",
    "        print(f\"Processed user audio features shape for conditioning: {processed_features.shape}\")\n",
    "        return processed_features\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Placeholder for a language model (replace with actual model if available)\n",
    "def generate_lyrics_with_language_model(prompt):\n",
    "    \"\"\"Placeholder function to generate lyrics based on a prompt using a language model.\"\"\"\n",
    "    print(f\"Generating lyrics for prompt: '{prompt}' using a placeholder model.\")\n",
    "    return f\"Generated lyrics based on '{prompt}':\\nVerse 1: The rhythm flows, the melody calls\\nChorus: Music fills the air, breaking down walls\"\n",
    "\n",
    "# Function to get text embeddings\n",
    "# We'll use a placeholder for a text embedding model\n",
    "def get_text_embedding(text, embedding_dim):\n",
    "    \"\"\"Placeholder function to get a fixed-size embedding for text.\"\"\"\n",
    "    print(f\"Getting embedding for text: '{text[:50]}...' using a placeholder model.\")\n",
    "    return np.random.rand(1, embedding_dim) # Shape (1, embedding_dim)\n",
    "\n",
    "def process_text_input(user_input, embedding_dim, is_prompt=False):\n",
    "    \"\"\"Processes user text input (lyrics or prompt) and returns a conditioning embedding.\"\"\"\n",
    "    if is_prompt:\n",
    "        lyrics = generate_lyrics_with_language_model(user_input)\n",
    "    else:\n",
    "        lyrics = user_input # User provided lyrics directly\n",
    "\n",
    "    conditioning_embedding = get_text_embedding(lyrics, embedding_dim)\n",
    "\n",
    "    return conditioning_embedding, lyrics\n",
    "\n",
    "\n",
    "def generate_conditional_mfccs(model, start_sequence, conditioning_input, generation_steps):\n",
    "    \"\"\"Generates new MFCC sequences using the trained conditional model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras conditional generation model.\n",
    "        start_sequence: A numpy array representing the initial musical sequence\n",
    "                        (shape: (1, sequence_length, musical_feature_dim)).\n",
    "        conditioning_input: The conditioning input as a single vector (shape: (1, conditioning_feature_dim)).\n",
    "        generation_steps: The number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated musical MFCC sequence.\n",
    "    \"\"\"\n",
    "    generated_sequence = np.copy(start_sequence)\n",
    "    current_musical_sequence = np.copy(start_sequence)\n",
    "    sequence_length = start_sequence.shape[1] # Get the expected sequence length\n",
    "\n",
    "\n",
    "    # Assuming the model takes a list of inputs: [musical_input, conditioning_input]\n",
    "    for step in range(generation_steps):\n",
    "        # Predict the next time step of musical features\n",
    "        # The model predicts a sequence of length `sequence_length`. We only need the last predicted step.\n",
    "        predicted_mfcc_sequence = model.predict([current_musical_sequence, conditioning_input]) # Shape (1, sequence_length, musical_feature_dim)\n",
    "        predicted_next_step_mfcc = predicted_mfcc_sequence[:, -1:, :] # Take the last predicted time step, shape (1, 1, musical_feature_dim)\n",
    "\n",
    "\n",
    "        # Append the predicted MFCC to the generated sequence\n",
    "        generated_sequence = np.append(generated_sequence, predicted_next_step_mfcc, axis=1)\n",
    "\n",
    "        # Update the current musical sequence by taking the last `sequence_length - 1` elements and appending the new prediction\n",
    "        current_musical_sequence = np.append(current_musical_sequence[:, 1:, :], predicted_next_step_mfcc, axis=1)\n",
    "\n",
    "\n",
    "    return generated_sequence.squeeze() # Remove the batch dimension\n",
    "\n",
    "\n",
    "def generate_music_from_input(model, user_input, input_type, generation_steps=200, sr=22050, max_padding=174, n_mfcc=20, n_chroma=12, conditioning_feature_dim=128):\n",
    "    \"\"\"Generates music based on user input (text or audio).\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras conditional music generation model.\n",
    "        user_input: The user's input (string for text, file path for audio).\n",
    "        input_type: Type of input ('text_lyrics', 'text_prompt', 'audio').\n",
    "        generation_steps: Number of additional time steps to generate.\n",
    "        sr: Sampling rate for audio output.\n",
    "        max_padding: Target sequence length for padding/truncating features.\n",
    "        n_mfcc: Number of MFCCs extracted.\n",
    "        n_chroma: Number of chroma bins extracted.\n",
    "        conditioning_feature_dim: Dimension of the conditioning feature vector (for text).\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        The path to the generated audio file, or None if an error occurred.\n",
    "    \"\"\"\n",
    "    conditioning_input_for_model = None\n",
    "    processed_lyrics = None # To store generated lyrics if applicable\n",
    "\n",
    "    if input_type == 'text_lyrics' or input_type == 'text_prompt':\n",
    "        is_prompt = (input_type == 'text_prompt')\n",
    "        conditioning_input_for_model, processed_lyrics = process_text_input(user_input, conditioning_feature_dim, is_prompt=is_prompt)\n",
    "        if conditioning_input_for_model is None:\n",
    "            print(\"Error processing text input for conditioning.\")\n",
    "            return None\n",
    "    elif input_type == 'audio':\n",
    "        print(\"Processing user audio input for conditioning.\")\n",
    "        # Process user audio to get features\n",
    "        user_audio_features = process_user_audio_for_conditioning(user_input, max_padding, n_mfcc, n_chroma)\n",
    "        if user_audio_features is None:\n",
    "            print(\"Error processing user audio input.\")\n",
    "            return None\n",
    "\n",
    "        # Now, how to use user_audio_features (shape: (1, seq_len, total_audio_features))\n",
    "        # to condition a model that expects (1, conditioning_feature_dim)?\n",
    "        # This requires modifying the model architecture or adding a layer to\n",
    "        # process the audio features into the expected conditioning shape.\n",
    "        # Given the current model expects a single vector, let's average the audio features\n",
    "        # over the time dimension and project it to the conditioning_feature_dim.\n",
    "        # This is a simplification; a better approach might be a dedicated audio encoder.\n",
    "\n",
    "        averaged_audio_features = np.mean(user_audio_features, axis=1) # Shape (1, total_audio_features)\n",
    "\n",
    "        # Need a projection layer or model to map averaged_audio_features to conditioning_feature_dim\n",
    "        # Let's define a simple Dense layer for this purpose\n",
    "        total_audio_features = n_mfcc + n_chroma # Update total features dimension\n",
    "        audio_projection_layer = Dense(conditioning_feature_dim, activation='relu')\n",
    "        # Need to build this layer if it's the first time it's called\n",
    "        if not audio_projection_layer.built:\n",
    "             audio_projection_layer.build(input_shape=(None, total_audio_features))\n",
    "\n",
    "\n",
    "        conditioning_input_for_model = audio_projection_layer(averaged_audio_features)\n",
    "        print(f\"Projected audio conditioning vector shape: {conditioning_input_for_model.shape}\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid input type. Please use 'text_lyrics', 'text_prompt', or 'audio'.\")\n",
    "        return None\n",
    "\n",
    "    # Ensure conditioning_input_for_model is not None before proceeding\n",
    "    if conditioning_input_for_model is None:\n",
    "         print(\"Conditioning input for model is None. Cannot proceed.\")\n",
    "         return None\n",
    "\n",
    "    # Generate a starting musical sequence (e.g., take a random sample from the training data)\n",
    "    if 'X_train' not in globals() or X_train.shape[0] == 0:\n",
    "         print(\"Training data (X_train) is not available or empty. Cannot generate a starting sequence.\")\n",
    "         return None\n",
    "\n",
    "\n",
    "    start_index = np.random.randint(0, X_train.shape[0])\n",
    "    # The starting sequence should have the same feature dimension as the model's musical input\n",
    "    # which is just MFCCs based on the model definition\n",
    "    start_sequence = X_train[start_index:start_index+1, :, :]\n",
    "\n",
    "\n",
    "    # Generate new musical features using the trained conditional GRU model\n",
    "    print(\"\\nGenerating new conditional musical features...\")\n",
    "    # The model expects musical_input (MFCCs) and conditioning_input\n",
    "    # The conditioning_input is now a single vector (1, conditioning_feature_dim)\n",
    "    generated_musical_features = generate_conditional_mfccs(model, start_sequence, conditioning_input_for_model, generation_steps)\n",
    "\n",
    "\n",
    "    print(f\"Shape of the generated musical features: {generated_musical_features.shape}\")\n",
    "\n",
    "    # Convert the generated musical features (MFCCs) back to an audio time-series\n",
    "    print(\"\\nConverting generated musical features to audio...\")\n",
    "    # Ensure the generated features are only MFCCs if the inverse transform expects only MFCCs\n",
    "    # If the generation model outputs combined features, we would need to separate or use a different inverse transform\n",
    "    # Assuming the model is trained to output only MFCCs for the musical output layer\n",
    "    audio_time_series_conditional = librosa.feature.inverse.mfcc_to_audio(generated_musical_features.T, sr=sr)\n",
    "\n",
    "\n",
    "    # Define the output filename\n",
    "    output_filename_conditional = 'generated_music_conditional.wav'\n",
    "\n",
    "    # Save the generated audio to a WAV file\n",
    "    print(f\"Saving generated conditional audio to {output_filename_conditional}...\")\n",
    "    sf.write(output_filename_conditional, audio_time_series_conditional, sr)\n",
    "\n",
    "    print(f\"Generated conditional audio saved successfully to {output_filename_conditional}\")\n",
    "\n",
    "    # Optionally, save the generated lyrics to a text file if lyrics were generated\n",
    "    if processed_lyrics and input_type == 'text_prompt':\n",
    "        lyrics_output_filename = 'generated_lyrics.txt'\n",
    "        with open(lyrics_output_filename, 'w') as f:\n",
    "            f.write(processed_lyrics)\n",
    "        print(f\"Generated lyrics saved to {lyrics_output_filename}\")\n",
    "\n",
    "\n",
    "    return output_filename_conditional\n",
    "\n",
    "# --- Re-run feature extraction and model definition to ensure variables are defined ---\n",
    "# Define the paths to the extracted datasets (using the recheck directories)\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck' # This directory might be empty\n",
    "\n",
    "bird_sound_files = []\n",
    "music_instrument_files = []\n",
    "\n",
    "# Identify bird sound files from the second dataset (assuming 'Voice of Birds' contains bird sounds)\n",
    "# We will still attempt to list, but expect it to be empty based on previous output\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        if 'Voice of Birds' in root and name.endswith('.mp3'):\n",
    "            bird_sound_files.append(os.path.join(root, name))\n",
    "\n",
    "# Identify music instrument files from the first dataset (assuming Test_submission and Train_submission contain music instruments)\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        if ('Test_submission' in root or 'Train_submission' in root) and (name.endswith('.wav') or name.endswith('.mp3')):\n",
    "             # Further check if the file name indicates a music instrument\n",
    "            if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords']):\n",
    "                 music_instrument_files.append(os.path.join(root, name))\n",
    "\n",
    "# Use only music instrument files for feature extraction as bird sounds are not available\n",
    "files_to_process = music_instrument_files\n",
    "\n",
    "features = []\n",
    "\n",
    "print(\"Extracting features from music instrument files for training...\")\n",
    "# Use the updated extract_features function to get combined features for training if needed for a different model\n",
    "# However, the current generation model is trained on MFCCs only for its musical input/output.\n",
    "# Let's stick to extracting only MFCCs for X_train for consistency with the model architecture defined below.\n",
    "# If we want to train a model that uses combined features, we would need to adjust here and the model definition.\n",
    "\n",
    "# Reverting extract_features usage for X_train to only MFCCs for consistency with model_gen_conditioned\n",
    "def extract_mfcc_features_only(file_path, target_sequence_length, n_mfcc=20):\n",
    "    \"\"\"Extract only MFCC features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Pad or truncate MFCCs to a fixed length\n",
    "        if mfccs.shape[1] < target_sequence_length:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, target_sequence_length - mfccs.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :target_sequence_length]\n",
    "\n",
    "        return mfccs.T # Transpose to have shape (time steps, n_mfcc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "for i, file_path in enumerate(files_to_process):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processing music instrument file {i + 1}/{len(files_to_process)}\")\n",
    "    mfccs = extract_mfcc_features_only(file_path, max_padding, n_mfcc=n_mfcc)\n",
    "    if mfccs is not None:\n",
    "        features.append(mfccs)\n",
    "\n",
    "\n",
    "# Convert features to numpy array\n",
    "features = np.array(features)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val = train_test_split(features, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(\"\\nFeature extraction complete.\")\n",
    "print(f\"Total samples: {len(features)}\")\n",
    "print(f\"Shape of features array: {features.shape}\")\n",
    "print(f\"Shape of training features: {X_train.shape}\")\n",
    "print(f\"Shape of validation features: {X_val.shape}\")\n",
    "\n",
    "\n",
    "# Define the dimensions of the musical features (MFCCs) and the conditioning input\n",
    "musical_feature_dim = X_train.shape[2] # Number of MFCCs\n",
    "musical_sequence_length = X_train.shape[1] # Length of the MFCC sequences\n",
    "\n",
    "# Assuming a fixed size for the text embedding if text input is used.\n",
    "# For now, we'll use a placeholder conditioning dimension as we are training with dummy input\n",
    "conditioning_feature_dim = 128 # Example dimension for a text embedding\n",
    "\n",
    "# Define the input layers\n",
    "musical_input = Input(shape=(musical_sequence_length, musical_feature_dim), name='musical_input')\n",
    "conditioning_input = Input(shape=(conditioning_feature_dim,), name='conditioning_input') # Model still expects a single vector conditioning input\n",
    "\n",
    "# Repeat the conditioning input across the musical sequence length\n",
    "repeated_conditioning = tf.keras.layers.RepeatVector(musical_sequence_length)(conditioning_input)\n",
    "\n",
    "# Concatenate the musical input with the repeated conditioning input\n",
    "combined_input = Concatenate(axis=-1)([musical_input, repeated_conditioning])\n",
    "\n",
    "# GRU layers\n",
    "gru_1 = GRU(128, activation='relu', return_sequences=True)(combined_input)\n",
    "dropout_1 = Dropout(0.2)(gru_1)\n",
    "\n",
    "gru_2 = GRU(128, activation='relu', return_sequences=True)(dropout_1)\n",
    "dropout_2 = Dropout(0.2)(gru_2)\n",
    "\n",
    "# TimeDistributed Dense layer to output the generated musical features (MFCCs)\n",
    "output_layer = TimeDistributed(Dense(musical_feature_dim, activation='linear'))(dropout_2)\n",
    "\n",
    "# Define the conditioned generation model\n",
    "model_gen_conditioned = Model(inputs=[musical_input, conditioning_input], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=1.0) # Add gradient clipping\n",
    "model_gen_conditioned.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Print the model summary\n",
    "model_gen_conditioned.summary()\n",
    "\n",
    "# Create dummy conditioning data for training\n",
    "dummy_conditioning_train = np.random.rand(X_train.shape[0], conditioning_feature_dim)\n",
    "dummy_conditioning_val = np.random.rand(X_val.shape[0], conditioning_feature_dim)\n",
    "\n",
    "# Train the conditioned GRU generation model\n",
    "print(\"\\nTraining the conditioned GRU generation model with dummy input...\")\n",
    "history_gen_conditioned = model_gen_conditioned.fit(\n",
    "    [X_train, dummy_conditioning_train],\n",
    "    X_train,\n",
    "    epochs=50, # Increased epochs as training with dummy data is quick\n",
    "    batch_size=32,\n",
    "    validation_data=([X_val, dummy_conditioning_val], X_val) # Use validation data\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "\n",
    "\n",
    "# --- Redefine generate_conditional_mfccs to work with the single vector conditioning input ---\n",
    "def generate_conditional_mfccs(model, start_sequence, conditioning_input_vector, generation_steps):\n",
    "    \"\"\"Generates new MFCC sequences using the trained conditional model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras conditional generation model.\n",
    "        start_sequence: A numpy array representing the initial musical sequence\n",
    "                        (shape: (1, sequence_length, musical_feature_dim)).\n",
    "        conditioning_input_vector: The conditioning input as a single vector (shape: (1, conditioning_feature_dim)).\n",
    "        generation_steps: The number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated musical MFCC sequence.\n",
    "    \"\"\"\n",
    "    generated_sequence = np.copy(start_sequence)\n",
    "    current_musical_sequence = np.copy(start_sequence)\n",
    "    sequence_length = start_sequence.shape[1] # Get the expected sequence length\n",
    "\n",
    "\n",
    "    # Assuming the model takes a list of inputs: [musical_input, conditioning_input]\n",
    "    for step in range(generation_steps):\n",
    "        # Predict the next time step of musical features\n",
    "        # The model predicts a sequence of length `sequence_length`. We only need the last predicted step.\n",
    "        predicted_mfcc_sequence = model.predict([current_musical_sequence, conditioning_input_vector]) # Shape (1, sequence_length, musical_feature_dim)\n",
    "        predicted_next_step_mfcc = predicted_mfcc_sequence[:, -1:, :] # Take the last predicted time step, shape (1, 1, musical_feature_dim)\n",
    "\n",
    "\n",
    "        # Append the predicted MFCC to the generated sequence\n",
    "        generated_sequence = np.append(generated_sequence, predicted_next_step_mfcc, axis=1)\n",
    "\n",
    "        # Update the current musical sequence by taking the last `sequence_length - 1` elements and appending the new prediction\n",
    "        # This explicitly maintains the sequence length.\n",
    "        current_musical_sequence = np.append(current_musical_sequence[:, 1:, :], predicted_next_step_mfcc, axis=1)\n",
    "\n",
    "\n",
    "    return generated_sequence.squeeze() # Remove the batch dimension\n",
    "\n",
    "\n",
    "# --- Demonstrate Music Generation from User Audio (using dummy audio file) ---\n",
    "# Note: Replace 'path/to/your/audio.wav' with the actual path to a user-recorded audio file\n",
    "# We need to provide a dummy audio file path for demonstration if a real one isn't available.\n",
    "# For demonstration, let's create a dummy audio file if it doesn't exist.\n",
    "# This is a placeholder and won't sound like real audio conditioning.\n",
    "dummy_audio_path = '/content/dummy_user_audio.wav'\n",
    "if not os.path.exists(dummy_audio_path):\n",
    "    print(f\"Creating a dummy audio file at {dummy_audio_path} for demonstration.\")\n",
    "    # Create a short silent WAV file as a placeholder\n",
    "    dummy_audio_ts = np.zeros(sr * 5) # 5 seconds of silence\n",
    "    sf.write(dummy_audio_path, dummy_audio_ts, sr)\n",
    "\n",
    "\n",
    "user_audio_input_path = dummy_audio_path # Use the dummy audio file for demonstration\n",
    "print(f\"\\nAttempting to generate music from audio file: {user_audio_input_path}\")\n",
    "\n",
    "if os.path.exists(user_audio_input_path):\n",
    "     generated_file_audio = generate_music_from_input(model_gen_conditioned, user_audio_input_path, 'audio',\n",
    "                                                      generation_steps=200, sr=sr, max_padding=max_padding, n_mfcc=n_mfcc, n_chroma=n_chroma, conditioning_feature_dim=conditioning_feature_dim)\n",
    "     if generated_file_audio:\n",
    "         print(f\"\\nMusic generated from audio saved to: {generated_file_audio}\")\n",
    "     else:\n",
    "         print(\"\\nFailed to generate music from audio.\")\n",
    "else:\n",
    "    print(f\"\\nUser audio file not found at {user_audio_input_path}. Skipping audio-conditioned generation example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sQ2m69nUj0g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIFUH8D2Up0H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qShicIrrU8an"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkSsoINKVD1x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "id": "02ca40b2",
    "outputId": "1c738fd3-7475-4280-894f-4331f5522b2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to generate lyrics and music from prompt: 'Create an upbeat song about a sunny day.'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_music_from_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3706947798.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# This will trigger the placeholder lyric generation within the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# and then use the embedding of the generated lyrics for conditioning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m generated_file_from_lyric_idea = generate_music_from_input(model_gen_conditioned, user_prompt_input, 'text_prompt',\n\u001b[0m\u001b[1;32m     16\u001b[0m                                                          generation_steps=200, sr=sr, max_padding=max_padding, n_mfcc=n_mfcc)\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_music_from_input' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Demonstrate Music Generation from a Text Prompt (using placeholder lyric generation) ---\n",
    "# This cell demonstrates the workflow for generating music based on a text prompt,\n",
    "# which involves the placeholder lyric generation and subsequent text conditioning.\n",
    "\n",
    "# Assuming model_gen_conditioned, generate_music_from_input,\n",
    "# conditioning_feature_dim, max_padding, n_mfcc, sr are defined.\n",
    "# Also assuming process_text_input and generate_conditional_mfccs are defined.\n",
    "\n",
    "user_prompt_input = \"Create an upbeat song about a sunny day.\"\n",
    "print(f\"Attempting to generate lyrics and music from prompt: '{user_prompt_input}'\")\n",
    "\n",
    "# Call the generate_music_from_input function with input_type='text_prompt'\n",
    "# This will trigger the placeholder lyric generation within the function\n",
    "# and then use the embedding of the generated lyrics for conditioning.\n",
    "generated_file_from_lyric_idea = generate_music_from_input(model_gen_conditioned, user_prompt_input, 'text_prompt',\n",
    "                                                         generation_steps=200, sr=sr, max_padding=max_padding, n_mfcc=n_mfcc)\n",
    "\n",
    "if generated_file_from_lyric_idea:\n",
    "    print(f\"\\nMusic generated from lyric idea prompt saved to: {generated_file_from_lyric_idea}\")\n",
    "else:\n",
    "    print(\"\\nFailed to generate music from lyric idea prompt.\")\n",
    "\n",
    "# Optionally, generate music directly from provided lyrics\n",
    "# user_provided_lyrics = \"Birds are singing, flowers bloom, chasing away the gloom.\"\n",
    "# print(f\"\\nAttempting to generate music from provided lyrics: '{user_provided_lyrics}'\")\n",
    "# generated_file_from_lyrics = generate_music_from_input(model_gen_conditioned, user_provided_lyrics, 'text_lyrics',\n",
    "#                                                      generation_steps=200, sr=sr, max_padding=max_padding, n_mfcc=n_mfcc)\n",
    "\n",
    "# if generated_file_from_lyrics:\n",
    "#     print(f\"\\nMusic generated from provided lyrics saved to: {generated_file_from_lyrics}\")\n",
    "# else:\n",
    "#     print(\"\\nFailed to generate music from provided lyrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1b9eb01d"
   },
   "outputs": [],
   "source": [
    "!pip install pydub ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "87936d41",
    "outputId": "ea7209f8-2ed1-4c92-d659-9b283df7d3a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from music instrument files for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
      "  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
      "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
      "  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
      "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
      "  elif re.match('(flt)p?( \\(default\\))?$', token):\n",
      "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
      "  elif re.match('(dbl)p?( \\(default\\))?$', token):\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-297202425.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;31m# Split data into training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, GRU, Input, TimeDistributed\n",
    "from pydub import AudioSegment # Import AudioSegment for MP3 conversion\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This will need to be determined based on the desired sequence length or maximum length of audio files\n",
    "n_chroma = 12 # Number of chroma bins\n",
    "\n",
    "def extract_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "    \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract Chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "        # Pad or truncate combined features to a fixed length\n",
    "        if combined_features.shape[1] < target_sequence_length:\n",
    "            combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "        else:\n",
    "            combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "        return combined_features.T # Transpose to have shape (time steps, total_features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_user_audio_for_conditioning(audio_file_path, target_sequence_length, n_mfcc, n_chroma):\n",
    "    \"\"\"Processes user audio input and returns features suitable for conditioning.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path: Path to the user's audio file.\n",
    "        target_sequence_length: The expected sequence length for model input (e.g., max_padding).\n",
    "        n_mfcc: The number of MFCCs to extract.\n",
    "        n_chroma: The number of chroma bins to extract.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the processed audio features (shape: (1, target_sequence_length, total_feature_dimension)),\n",
    "        or None if processing fails.\n",
    "    \"\"\"\n",
    "    processed_features = extract_features(audio_file_path, target_sequence_length, n_mfcc, n_chroma)\n",
    "\n",
    "    if processed_features is not None:\n",
    "        # Add a batch dimension\n",
    "        processed_features = processed_features[np.newaxis, :, :] # Shape (1, target_sequence_length, total_feature_dimension)\n",
    "        print(f\"Processed user audio features shape for conditioning: {processed_features.shape}\")\n",
    "        return processed_features\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Placeholder for a language model (replace with actual model if available)\n",
    "def generate_lyrics_with_language_model(prompt):\n",
    "    \"\"\"Placeholder function to generate lyrics based on a prompt using a language model.\"\"\"\n",
    "    print(f\"Generating lyrics for prompt: '{prompt}' using a placeholder model.\")\n",
    "    return f\"Generated lyrics based on '{prompt}':\\nVerse 1: The rhythm flows, the melody calls\\nChorus: Music fills the air, breaking down walls\"\n",
    "\n",
    "# Function to get text embeddings\n",
    "# We'll use a placeholder for a text embedding model\n",
    "def get_text_embedding(text, embedding_dim):\n",
    "    \"\"\"Placeholder function to get a fixed-size embedding for text.\"\"\"\n",
    "    print(f\"Getting embedding for text: '{text[:50]}...' using a placeholder model.\")\n",
    "    return np.random.rand(1, embedding_dim) # Shape (1, embedding_dim)\n",
    "\n",
    "def process_text_input(user_input, embedding_dim, is_prompt=False):\n",
    "    \"\"\"Processes user text input (lyrics or prompt) and returns a conditioning embedding.\"\"\"\n",
    "    if is_prompt:\n",
    "        lyrics = generate_lyrics_with_language_model(user_input)\n",
    "    else:\n",
    "        lyrics = user_input # User provided lyrics directly\n",
    "\n",
    "    conditioning_embedding = get_text_embedding(lyrics, embedding_dim)\n",
    "\n",
    "    return conditioning_embedding, lyrics\n",
    "\n",
    "\n",
    "def generate_conditional_mfccs(model, start_sequence, conditioning_input, generation_steps):\n",
    "    \"\"\"Generates new MFCC sequences using the trained conditional model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras conditional generation model.\n",
    "        start_sequence: A numpy array representing the initial musical sequence\n",
    "                        (shape: (1, sequence_length, musical_feature_dim)).\n",
    "        conditioning_input: The conditioning input as a single vector (shape: (1, conditioning_feature_dim)).\n",
    "        generation_steps: The number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated musical MFCC sequence.\n",
    "    \"\"\"\n",
    "    generated_sequence = np.copy(start_sequence)\n",
    "    current_musical_sequence = np.copy(start_sequence)\n",
    "    sequence_length = start_sequence.shape[1] # Get the expected sequence length\n",
    "\n",
    "\n",
    "    # Assuming the model takes a list of inputs: [musical_input, conditioning_input]\n",
    "    for _ in range(generation_steps):\n",
    "        # Predict the next time step of musical features\n",
    "        predicted_mfcc = model.predict([current_musical_sequence, conditioning_input]) # Model expects [musical_input, conditioning_input]\n",
    "\n",
    "\n",
    "        # Append the predicted MFCC to the generated sequence (only the new time step)\n",
    "        generated_sequence = np.append(generated_sequence, predicted_mfcc[:, -1:, :], axis=1)\n",
    "\n",
    "        # Update the current musical sequence by removing the oldest time step and adding the new one\n",
    "        # Ensure the sequence length remains constant\n",
    "        current_musical_sequence = np.append(current_musical_sequence[:, 1:, :], predicted_mfcc[:, -1:, :], axis=1)\n",
    "\n",
    "\n",
    "    return generated_sequence.squeeze() # Remove the batch dimension\n",
    "\n",
    "\n",
    "def generate_music_from_input(model, user_input, input_type, generation_steps=200, sr=22050, max_padding=174, n_mfcc=20, n_chroma=12, conditioning_feature_dim=128):\n",
    "    \"\"\"Generates music based on user input (text or audio).\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras conditional music generation model.\n",
    "        user_input: The user's input (string for text, file path for audio).\n",
    "        input_type: Type of input ('text_lyrics', 'text_prompt', 'audio').\n",
    "        generation_steps: Number of additional time steps to generate.\n",
    "        sr: Sampling rate for audio output.\n",
    "        max_padding: Target sequence length for padding/truncating features.\n",
    "        n_mfcc: Number of MFCCs extracted.\n",
    "        n_chroma: Number of chroma bins extracted.\n",
    "        conditioning_feature_dim: Dimension of the conditioning feature vector (for text).\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        The path to the generated audio file (MP3), or None if an error occurred.\n",
    "    \"\"\"\n",
    "    conditioning_input_for_model = None\n",
    "    processed_lyrics = None # To store generated lyrics if applicable\n",
    "\n",
    "    if input_type == 'text_lyrics' or input_type == 'text_prompt':\n",
    "        is_prompt = (input_type == 'text_prompt')\n",
    "        conditioning_input_for_model, processed_lyrics = process_text_input(user_input, conditioning_feature_dim, is_prompt=is_prompt)\n",
    "        if conditioning_input_for_model is None:\n",
    "            print(\"Error processing text input for conditioning.\")\n",
    "            return None\n",
    "    elif input_type == 'audio':\n",
    "        print(\"Processing user audio input for conditioning.\")\n",
    "        # Process user audio to get features\n",
    "        user_audio_features = process_user_audio_for_conditioning(user_input, max_padding, n_mfcc, n_chroma)\n",
    "        if user_audio_features is None:\n",
    "            print(\"Error processing user audio input.\")\n",
    "            return None\n",
    "\n",
    "        # Now, how to use user_audio_features (shape: (1, seq_len, total_audio_features))\n",
    "        # to condition a model that expects (1, conditioning_feature_dim)?\n",
    "        # This requires modifying the model architecture or adding a layer to\n",
    "        # process the audio features into the expected conditioning shape.\n",
    "        # Given the current model expects a single vector, let's average the audio features\n",
    "        # over the time dimension and project it to the conditioning_feature_dim.\n",
    "        # This is a simplification; a better approach might be a dedicated audio encoder.\n",
    "\n",
    "        averaged_audio_features = np.mean(user_audio_features, axis=1) # Shape (1, total_audio_features)\n",
    "\n",
    "        # Need a projection layer or model to map averaged_audio_features to conditioning_feature_dim\n",
    "        # Let's define a simple Dense layer for this purpose\n",
    "        total_audio_features = n_mfcc + n_chroma # Update total features dimension\n",
    "        audio_projection_layer = Dense(conditioning_feature_dim, activation='relu')\n",
    "        # Need to build this layer if it's the first time it's called\n",
    "        if not audio_projection_layer.built:\n",
    "             audio_projection_layer.build(input_shape=(None, total_audio_features))\n",
    "\n",
    "\n",
    "        conditioning_input_for_model = audio_projection_layer(averaged_audio_features)\n",
    "        print(f\"Projected audio conditioning vector shape: {conditioning_input_for_model.shape}\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid input type. Please use 'text_lyrics', 'text_prompt', or 'audio'.\")\n",
    "        return None\n",
    "\n",
    "    # Ensure conditioning_input_for_model is not None before proceeding\n",
    "    if conditioning_input_for_model is None:\n",
    "         print(\"Conditioning input for model is None. Cannot proceed.\")\n",
    "         return None\n",
    "\n",
    "    # Generate a starting musical sequence (e.g., take a random sample from the training data)\n",
    "    if 'X_train' not in globals() or X_train.shape[0] == 0:\n",
    "         print(\"Training data (X_train) is not available or empty. Cannot generate a starting sequence.\")\n",
    "         return None\n",
    "\n",
    "\n",
    "    start_index = np.random.randint(0, X_train.shape[0])\n",
    "    # The starting sequence should have the same feature dimension as the model's musical input\n",
    "    # which is just MFCCs based on the model definition\n",
    "    start_sequence = X_train[start_index:start_index+1, :, :]\n",
    "\n",
    "\n",
    "    # Generate new musical features using the trained conditional GRU model\n",
    "    print(\"\\nGenerating new conditional musical features...\")\n",
    "    # The model expects musical_input (MFCCs) and conditioning_input\n",
    "    # The conditioning_input is now a single vector (1, conditioning_feature_dim)\n",
    "    generated_musical_features = generate_conditional_mfccs(model, start_sequence, conditioning_input_for_model, generation_steps)\n",
    "\n",
    "\n",
    "    print(f\"Shape of the generated musical features: {generated_musical_features.shape}\")\n",
    "\n",
    "    # Convert the generated musical features (MFCCs) back to an audio time-series\n",
    "    print(\"\\nConverting generated musical features to audio...\")\n",
    "    # Ensure the generated features are only MFCCs if the inverse transform expects only MFCCs\n",
    "    # If the generation model outputs combined features, we would need to separate or use a different inverse transform\n",
    "    # Assuming the model is trained to output only MFCCs for the musical output layer\n",
    "    audio_time_series_conditional = librosa.feature.inverse.mfcc_to_audio(generated_musical_features.T, sr=sr)\n",
    "\n",
    "\n",
    "    # Define the output filename (WAV and MP3)\n",
    "    output_filename_wav = 'generated_music_conditional.wav'\n",
    "    output_filename_mp3 = 'generated_music_conditional.mp3'\n",
    "\n",
    "\n",
    "    # Save the generated audio to a WAV file first\n",
    "    print(f\"Saving generated conditional audio to {output_filename_wav}...\")\n",
    "    sf.write(output_filename_wav, audio_time_series_conditional, sr)\n",
    "    print(f\"Generated conditional audio saved successfully to {output_filename_wav}\")\n",
    "\n",
    "\n",
    "    # Convert the WAV file to MP3\n",
    "    print(f\"Converting {output_filename_wav} to {output_filename_mp3}...\")\n",
    "    try:\n",
    "        audio = AudioSegment.from_wav(output_filename_wav)\n",
    "        audio.export(output_filename_mp3, format=\"mp3\")\n",
    "        print(f\"Conversion to MP3 successful. Output saved to {output_filename_mp3}\")\n",
    "        # Optionally, remove the intermediate WAV file\n",
    "        # os.remove(output_filename_wav)\n",
    "        # print(f\"Removed intermediate WAV file: {output_filename_wav}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MP3 conversion: {e}\")\n",
    "        return None # Return None if MP3 conversion fails\n",
    "\n",
    "\n",
    "    # Optionally, save the generated lyrics to a text file if lyrics were generated\n",
    "    if processed_lyrics and input_type == 'text_prompt':\n",
    "        lyrics_output_filename = 'generated_lyrics.txt'\n",
    "        with open(lyrics_output_filename, 'w') as f:\n",
    "            f.write(processed_lyrics)\n",
    "        print(f\"Generated lyrics saved to {lyrics_output_filename}\")\n",
    "\n",
    "\n",
    "    return output_filename_mp3 # Return the path to the MP3 file\n",
    "\n",
    "# --- Re-run feature extraction, model definition, and training to ensure variables are defined ---\n",
    "# Define the paths to the extracted datasets (using the recheck directories)\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck' # This directory might be empty\n",
    "\n",
    "bird_sound_files = []\n",
    "music_instrument_files = []\n",
    "\n",
    "# Identify bird sound files from the second dataset (assuming 'Voice of Birds' contains bird sounds)\n",
    "# We will still attempt to list, but expect it to be empty based on previous output\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        if 'Voice of Birds' in root and name.endswith('.mp3'):\n",
    "            bird_sound_files.append(os.path.join(root, name))\n",
    "\n",
    "# Identify music instrument files from the first dataset (assuming Test_submission and Train_submission contain music instruments)\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        if ('Test_submission' in root or 'Train_submission' in root) and (name.endswith('.wav') or name.endswith('.mp3')):\n",
    "             # Further check if the file name indicates a music instrument\n",
    "            if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords']):\n",
    "                 music_instrument_files.append(os.path.join(root, name))\n",
    "\n",
    "# Use only music instrument files for feature extraction as bird sounds are not available\n",
    "files_to_process = music_instrument_files\n",
    "\n",
    "features = []\n",
    "\n",
    "print(\"Extracting features from music instrument files for training...\")\n",
    "# Use the updated extract_features function to get combined features for training if needed for a different model\n",
    "# However, the current generation model is trained on MFCCs only for its musical input/output.\n",
    "# Let's stick to extracting only MFCCs for X_train for consistency with the model architecture defined below.\n",
    "# If we want to train a model that uses combined features, we would need to adjust here and the model definition.\n",
    "\n",
    "# Reverting extract_features usage for X_train to only MFCCs for consistency with model_gen_conditioned\n",
    "def extract_mfcc_features_only(file_path, target_sequence_length, n_mfcc=20):\n",
    "    \"\"\"Extract only MFCC features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Pad or truncate MFCCs to a fixed length\n",
    "        if mfccs.shape[1] < target_sequence_length:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, target_sequence_length - mfccs.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :target_sequence_length]\n",
    "\n",
    "        return mfccs.T # Transpose to have shape (time steps, n_mfcc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "for i, file_path in enumerate(files_to_process):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processing music instrument file {i + 1}/{len(files_to_process)}\")\n",
    "    mfccs = extract_mfcc_features_only(file_path, max_padding, n_mfcc=n_mfcc)\n",
    "    if mfccs is not None:\n",
    "        features.append(mfccs)\n",
    "\n",
    "\n",
    "# Convert features to numpy array\n",
    "features = np.array(features)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val = train_test_split(features, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(\"\\nFeature extraction complete.\")\n",
    "print(f\"Total samples: {len(features)}\")\n",
    "print(f\"Shape of features array: {features.shape}\")\n",
    "print(f\"Shape of training features: {X_train.shape}\")\n",
    "print(f\"Shape of validation features: {X_val.shape}\")\n",
    "\n",
    "\n",
    "# Define the dimensions of the musical features (MFCCs) and the conditioning input\n",
    "musical_feature_dim = X_train.shape[2] # Number of MFCCs\n",
    "musical_sequence_length = X_train.shape[1] # Length of the MFCC sequences\n",
    "\n",
    "# Assuming a fixed size for the text embedding if text input is used.\n",
    "# For now, we'll use a placeholder conditioning dimension as we are training with dummy input\n",
    "conditioning_feature_dim = 128 # Example dimension for a text embedding\n",
    "\n",
    "# Define the input layers\n",
    "musical_input = Input(shape=(musical_sequence_length, musical_feature_dim), name='musical_input')\n",
    "conditioning_input = Input(shape=(conditioning_feature_dim,), name='conditioning_input') # Model still expects a single vector conditioning input\n",
    "\n",
    "# Repeat the conditioning input across the musical sequence length\n",
    "repeated_conditioning = tf.keras.layers.RepeatVector(musical_sequence_length)(conditioning_input)\n",
    "\n",
    "# Concatenate the musical input with the repeated conditioning input\n",
    "combined_input = Concatenate(axis=-1)([musical_input, repeated_conditioning])\n",
    "\n",
    "# GRU layers\n",
    "gru_1 = GRU(128, activation='relu', return_sequences=True)(combined_input)\n",
    "dropout_1 = Dropout(0.2)(gru_1)\n",
    "\n",
    "gru_2 = GRU(128, activation='relu', return_sequences=True)(dropout_1)\n",
    "dropout_2 = Dropout(0.2)(gru_2)\n",
    "\n",
    "# TimeDistributed Dense layer to output the generated musical features (MFCCs)\n",
    "output_layer = TimeDistributed(Dense(musical_feature_dim, activation='linear'))(dropout_2)\n",
    "\n",
    "# Define the conditioned generation model\n",
    "model_gen_conditioned = Model(inputs=[musical_input, conditioning_input], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=1.0) # Add gradient clipping\n",
    "model_gen_conditioned.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Print the model summary\n",
    "model_gen_conditioned.summary()\n",
    "\n",
    "# Create dummy conditioning data for training\n",
    "dummy_conditioning_train = np.random.rand(X_train.shape[0], conditioning_feature_dim)\n",
    "dummy_conditioning_val = np.random.rand(X_val.shape[0], conditioning_feature_dim)\n",
    "\n",
    "# Train the conditioned GRU generation model\n",
    "print(\"\\nTraining the conditioned GRU generation model with dummy input...\")\n",
    "history_gen_conditioned = model_gen_conditioned.fit(\n",
    "    [X_train, dummy_conditioning_train],\n",
    "    X_train,\n",
    "    epochs=50, # Increased epochs as training with dummy data is quick\n",
    "    batch_size=32,\n",
    "    validation_data=([X_val, dummy_conditioning_val], X_val) # Use validation data\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "\n",
    "\n",
    "# --- Redefine generate_conditional_mfccs to work with the single vector conditioning input ---\n",
    "def generate_conditional_mfccs(model, start_sequence, conditioning_input_vector, generation_steps):\n",
    "    \"\"\"Generates new MFCC sequences using the trained conditional model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras conditional generation model.\n",
    "        start_sequence: A numpy array representing the initial musical sequence\n",
    "                        (shape: (1, sequence_length, musical_feature_dim)).\n",
    "        conditioning_input_vector: The conditioning input as a single vector (shape: (1, conditioning_feature_dim)).\n",
    "        generation_steps: The number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated musical MFCC sequence.\n",
    "    \"\"\"\n",
    "    generated_sequence = np.copy(start_sequence)\n",
    "    current_musical_sequence = np.copy(start_sequence)\n",
    "    sequence_length = start_sequence.shape[1] # Get the expected sequence length\n",
    "\n",
    "\n",
    "    # Assuming the model takes a list of inputs: [musical_input, conditioning_input]\n",
    "    for _ in range(generation_steps):\n",
    "        # Predict the next time step of musical features\n",
    "        predicted_mfcc = model.predict([current_musical_sequence, conditioning_input_vector]) # Model expects [musical_input, conditioning_input]\n",
    "\n",
    "\n",
    "        # Append the predicted MFCC to the generated sequence (only the new time step)\n",
    "        generated_sequence = np.append(generated_sequence, predicted_mfcc[:, -1:, :], axis=1)\n",
    "\n",
    "        # Update the current musical sequence by removing the oldest time step and adding the new one\n",
    "        # Ensure the sequence length remains constant\n",
    "        current_musical_sequence = np.append(current_musical_sequence[:, 1:, :], predicted_mfcc[:, -1:, :], axis=1)\n",
    "\n",
    "\n",
    "    return generated_sequence.squeeze() # Remove the batch dimension\n",
    "\n",
    "\n",
    "# --- Demonstrate Music Generation from User Audio (using dummy audio file) ---\n",
    "# Note: Replace 'path/to/your/audio.wav' with the actual path to a user-recorded audio file\n",
    "# We need to provide a dummy audio file path for demonstration if a real one isn't available.\n",
    "# For demonstration, let's create a dummy audio file if it doesn't exist.\n",
    "# This is a placeholder and won't sound like real audio conditioning.\n",
    "dummy_audio_path = '/content/dummy_user_audio.wav'\n",
    "if not os.path.exists(dummy_audio_path):\n",
    "    print(f\"Creating a dummy audio file at {dummy_audio_path} for demonstration.\")\n",
    "    # Create a short silent WAV file as a placeholder\n",
    "    dummy_audio_ts = np.zeros(sr * 5) # 5 seconds of silence\n",
    "    sf.write(dummy_audio_path, dummy_audio_ts, sr)\n",
    "\n",
    "\n",
    "user_audio_input_path = dummy_audio_path # Use the dummy audio file for demonstration\n",
    "print(f\"\\nAttempting to generate music from audio file: {user_audio_input_path}\")\n",
    "\n",
    "if os.path.exists(user_audio_input_path):\n",
    "     generated_file_audio = generate_music_from_input(model_gen_conditioned, user_audio_input_path, 'audio',\n",
    "                                                      generation_steps=200, sr=sr, max_padding=max_padding, n_mfcc=n_mfcc, n_chroma=n_chroma, conditioning_feature_dim=conditioning_feature_dim)\n",
    "     if generated_file_audio:\n",
    "         print(f\"\\nMusic generated from audio saved to: {generated_file_audio}\")\n",
    "     else:\n",
    "         print(\"\\nFailed to generate music from audio.\")\n",
    "else:\n",
    "    print(f\"\\nUser audio file not found at {user_audio_input_path}. Skipping audio-conditioned generation example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfdd629e"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, GRU, Input, TimeDistributed\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This will need to be determined based on the desired sequence length or maximum length of audio files\n",
    "n_chroma = 12 # Number of chroma bins\n",
    "total_audio_features_dim = n_mfcc + n_chroma # Total features for audio conditioning\n",
    "\n",
    "def extract_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "    \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract Chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "        # Pad or truncate combined features to a fixed length\n",
    "        if combined_features.shape[1] < target_sequence_length:\n",
    "            combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "        else:\n",
    "            combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "        return combined_features.T # Transpose to have shape (time steps, total_features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_user_audio_for_conditioning(audio_file_path, target_sequence_length, n_mfcc, n_chroma):\n",
    "    \"\"\"Processes user audio input and returns features suitable for conditioning.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path: Path to the user's audio file.\n",
    "        target_sequence_length: The expected sequence length for model input (e.g., max_padding).\n",
    "        n_mfcc: The number of MFCCs to extract.\n",
    "        n_chroma: The number of chroma bins to extract.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the processed audio features (shape: (1, target_sequence_length, total_feature_dimension)),\n",
    "        or None if processing fails.\n",
    "    \"\"\"\n",
    "    processed_features = extract_features(audio_file_path, target_sequence_length, n_mfcc, n_chroma)\n",
    "\n",
    "    if processed_features is not None:\n",
    "        # Add a batch dimension\n",
    "        processed_features = processed_features[np.newaxis, :, :] # Shape (1, target_sequence_length, total_feature_dimension)\n",
    "        print(f\"Processed user audio features shape for conditioning: {processed_features.shape}\")\n",
    "        return processed_features\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Placeholder for a language model (replace with actual model if available)\n",
    "def generate_lyrics_with_language_model(prompt):\n",
    "    \"\"\"Placeholder function to generate lyrics based on a prompt using a language model.\"\"\"\n",
    "    print(f\"Generating lyrics for prompt: '{prompt}' using a placeholder model.\")\n",
    "    return f\"Generated lyrics based on '{prompt}':\\nVerse 1: The rhythm flows, the melody calls\\nChorus: Music fills the air, breaking down walls\"\n",
    "\n",
    "# Function to get text embeddings\n",
    "# We'll use a placeholder for a text embedding model\n",
    "def get_text_embedding(text, embedding_dim):\n",
    "    \"\"\"Placeholder function to get a fixed-size embedding for text.\"\"\"\n",
    "    print(f\"Getting embedding for text: '{text[:50]}...' using a placeholder model.\")\n",
    "    return np.random.rand(1, embedding_dim) # Shape (1, embedding_dim)\n",
    "\n",
    "def process_text_input(user_input, embedding_dim, is_prompt=False):\n",
    "    \"\"\"Processes user text input (lyrics or prompt) and returns a conditioning embedding.\"\"\"\n",
    "    if is_prompt:\n",
    "        lyrics = generate_lyrics_with_language_model(user_input)\n",
    "    else:\n",
    "        lyrics = user_input # User provided lyrics directly\n",
    "\n",
    "    conditioning_embedding = get_text_embedding(lyrics, embedding_dim)\n",
    "\n",
    "    return conditioning_embedding, lyrics\n",
    "\n",
    "\n",
    "def generate_conditional_mfccs(model, start_sequence, conditioning_input, generation_steps):\n",
    "    \"\"\"Generates new MFCC sequences using the trained conditional model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras conditional generation model.\n",
    "        start_sequence: A numpy array representing the initial musical sequence\n",
    "                        (shape: (1, sequence_length, musical_feature_dim)).\n",
    "        conditioning_input: The conditioning input (text embedding or audio features).\n",
    "                            Shape varies based on input_type.\n",
    "        generation_steps: The number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated musical MFCC sequence.\n",
    "    \"\"\"\n",
    "    generated_sequence = np.copy(start_sequence)\n",
    "    current_musical_sequence = np.copy(start_sequence)\n",
    "    sequence_length = start_sequence.shape[1] # Get the expected sequence length\n",
    "\n",
    "    # Assuming the model takes a list of inputs: [musical_input, conditioning_input]\n",
    "    # The conditioning_input's shape depends on whether it's text or audio\n",
    "    # If text, it's (1, conditioning_feature_dim)\n",
    "    # If audio, it's (1, sequence_length, total_audio_features_dim)\n",
    "\n",
    "    # Determine the input structure for the model based on the shape of conditioning_input\n",
    "    if conditioning_input.shape[-1] == conditioning_feature_dim and conditioning_input.ndim == 2:\n",
    "        # Text conditioning (single vector)\n",
    "        model_inputs = [current_musical_sequence, conditioning_input]\n",
    "        print(\"Using text conditioning input shape:\", conditioning_input.shape)\n",
    "    elif conditioning_input.shape[-1] == total_audio_features_dim and conditioning_input.ndim == 3:\n",
    "         # Audio conditioning (sequence)\n",
    "         # For sequential conditioning, we might need a different model architecture\n",
    "         # or a way to combine the audio features sequence with the musical sequence over time.\n",
    "         # Given the current model expects a single conditioning vector repeated,\n",
    "         # this approach with sequential audio conditioning will require model modification.\n",
    "         # For now, let's adapt the generation to work with the current model by\n",
    "         # averaging the audio features if sequential audio conditioning is provided,\n",
    "         # or modifying the model to handle sequential conditioning properly.\n",
    "\n",
    "         # Since the model architecture is defined to take a single vector conditioning input,\n",
    "         # we need to convert the sequential audio features to a single vector.\n",
    "         # Averaging over time and projecting is a possible approach, but might lose temporal info.\n",
    "         print(\"Warning: Model architecture is designed for single vector conditioning. Averaging audio features.\")\n",
    "         averaged_audio_features = np.mean(conditioning_input, axis=1) # Shape (1, total_audio_features_dim)\n",
    "\n",
    "         # Need a projection layer if total_audio_features_dim != conditioning_feature_dim\n",
    "         if total_audio_features_dim != conditioning_feature_dim:\n",
    "             print(\"Warning: Projecting averaged audio features to match conditioning_feature_dim.\")\n",
    "             # Define a simple Dense layer for this purpose\n",
    "             audio_projection_layer = Dense(conditioning_feature_dim, activation='relu')\n",
    "             # Need to build this layer if it's the first time it's called\n",
    "             if not audio_projection_layer.built:\n",
    "                  audio_projection_layer.build(input_shape=(None, total_audio_features_dim))\n",
    "             conditioning_input_vector = audio_projection_layer(averaged_audio_features)\n",
    "         else:\n",
    "             conditioning_input_vector = averaged_audio_features # Use directly if dimensions match\n",
    "\n",
    "         model_inputs = [current_musical_sequence, conditioning_input_vector]\n",
    "         print(\"Using averaged and projected audio conditioning input shape:\", conditioning_input_vector.shape)\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Unexpected conditioning input shape.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    for _ in range(generation_steps):\n",
    "        # Predict the next time step of musical features\n",
    "        predicted_mfcc = model.predict(model_inputs) # Model expects [musical_input, conditioning_input]\n",
    "\n",
    "\n",
    "        # Append the predicted MFCC to the generated sequence (only the new time step)\n",
    "        generated_sequence = np.append(generated_sequence, predicted_mfcc[:, -1:, :], axis=1)\n",
    "\n",
    "        # Update the current musical sequence by removing the oldest time step and adding the new one\n",
    "        # Ensure the sequence length remains constant\n",
    "        current_musical_sequence = np.append(current_musical_sequence[:, 1:, :], predicted_mfcc[:, -1:, :], axis=1)\n",
    "\n",
    "        # If using sequential audio conditioning, need to update the conditioning input for the next step as well\n",
    "        # This would require a different model architecture that processes both sequences.\n",
    "        # With the current model expecting a single conditioning vector, this update is not applicable\n",
    "        # and the single conditioning vector remains constant throughout generation.\n",
    "\n",
    "\n",
    "    return generated_sequence.squeeze() # Remove the batch dimension\n",
    "\n",
    "\n",
    "def generate_music_from_input(model, user_input, input_type, generation_steps=200, sr=22050, max_padding=174, n_mfcc=20, n_chroma=12, conditioning_feature_dim=128):\n",
    "    \"\"\"Generates music based on user input (text or audio).\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras conditional music generation model.\n",
    "        user_input: The user's input (string for text, file path for audio).\n",
    "        input_type: Type of input ('text_lyrics', 'text_prompt', 'audio').\n",
    "        generation_steps: Number of additional time steps to generate.\n",
    "        sr: Sampling rate for audio output.\n",
    "        max_padding: Target sequence length for padding/truncating features.\n",
    "        n_mfcc: Number of MFCCs extracted.\n",
    "        n_chroma: Number of chroma bins extracted.\n",
    "        conditioning_feature_dim: Dimension of the conditioning feature vector (for text).\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        The path to the generated audio file (MP3), or None if an error occurred.\n",
    "    \"\"\"\n",
    "    conditioning_input_for_model = None\n",
    "    processed_lyrics = None # To store generated lyrics if applicable\n",
    "\n",
    "    if input_type == 'text_lyrics' or input_type == 'text_prompt':\n",
    "        is_prompt = (input_type == 'text_prompt')\n",
    "        conditioning_input_for_model, processed_lyrics = process_text_input(user_input, conditioning_feature_dim, is_prompt=is_prompt)\n",
    "        if conditioning_input_for_model is None:\n",
    "            print(\"Error processing text input for conditioning.\")\n",
    "            return None\n",
    "        # For text conditioning, repeat the single vector for each step if the model architecture requires it\n",
    "        # Our current model concatenates the repeated vector in the model definition, so we pass the single vector.\n",
    "\n",
    "    elif input_type == 'audio':\n",
    "        print(\"Processing user audio input for conditioning.\")\n",
    "        # Process user audio to get features\n",
    "        user_audio_features_sequence = process_user_audio_for_conditioning(user_input, max_padding, n_mfcc, n_chroma)\n",
    "        if user_audio_features_sequence is None:\n",
    "            print(\"Error processing user audio input.\")\n",
    "            return None\n",
    "\n",
    "        # For the current model architecture that expects a single conditioning vector,\n",
    "        # we need to convert the sequential audio features to a single vector.\n",
    "        # A simple approach is to average over time and project.\n",
    "        averaged_audio_features = np.mean(user_audio_features_sequence, axis=1) # Shape (1, total_audio_features_dim)\n",
    "\n",
    "        # Need a projection layer if total_audio_features_dim != conditioning_feature_dim\n",
    "        # Define a simple Dense layer for this purpose\n",
    "        total_audio_features = n_mfcc + n_chroma # Update total features dimension\n",
    "        audio_projection_layer = Dense(conditioning_feature_dim, activation='relu')\n",
    "        # Need to build this layer if it's the first time it's called\n",
    "        if not audio_projection_layer.built:\n",
    "             audio_projection_layer.build(input_shape=(None, total_audio_features))\n",
    "\n",
    "        conditioning_input_for_model = audio_projection_layer(averaged_audio_features)\n",
    "        print(f\"Projected audio conditioning vector shape: {conditioning_input_for_model.shape}\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid input type. Please use 'text_lyrics', 'text_prompt', or 'audio'.\")\n",
    "        return None\n",
    "\n",
    "    # Ensure conditioning_input_for_model is not None before proceeding\n",
    "    if conditioning_input_for_model is None:\n",
    "         print(\"Conditioning input for model is None. Cannot proceed.\")\n",
    "         return None\n",
    "\n",
    "    # Generate a starting musical sequence (e.g., take a random sample from the training data)\n",
    "    if 'X_train' not in globals() or X_train.shape[0] == 0:\n",
    "         print(\"Training data (X_train) is not available or empty. Cannot generate a starting sequence.\")\n",
    "         return None\n",
    "\n",
    "\n",
    "    start_index = np.random.randint(0, X_train.shape[0])\n",
    "    # The starting sequence should have the same feature dimension as the model's musical input\n",
    "    # which is just MFCCs based on the model definition\n",
    "    start_sequence = X_train[start_index:start_index+1, :, :]\n",
    "\n",
    "\n",
    "    # Generate new musical features using the trained conditional GRU model\n",
    "    print(\"\\nGenerating new conditional musical features...\")\n",
    "    # The model expects musical_input (MFCCs) and conditioning_input\n",
    "    # The conditioning_input is now a single vector (1, conditioning_feature_dim)\n",
    "    generated_musical_features = generate_conditional_mfccs(model, start_sequence, conditioning_input_for_model, generation_steps)\n",
    "\n",
    "\n",
    "    print(f\"Shape of the generated musical features: {generated_musical_features.shape}\")\n",
    "\n",
    "    # Convert the generated musical features (MFCCs) back to an audio time-series\n",
    "    print(\"\\nConverting generated musical features to audio...\")\n",
    "    # Ensure the generated features are only MFCCs if the inverse transform expects only MFCCs\n",
    "    # If the generation model outputs combined features, we would need to separate or use a different inverse transform\n",
    "    # Assuming the model is trained to output only MFCCs for the musical output layer\n",
    "    audio_time_series_conditional = librosa.feature.inverse.mfcc_to_audio(generated_musical_features.T, sr=sr)\n",
    "\n",
    "\n",
    "    # Define the output filename (WAV and MP3)\n",
    "    output_filename_wav = 'generated_music_conditional.wav'\n",
    "    output_filename_mp3 = 'generated_music_conditional.mp3'\n",
    "\n",
    "\n",
    "    # Save the generated audio to a WAV file first\n",
    "    print(f\"Saving generated conditional audio to {output_filename_wav}...\")\n",
    "    sf.write(output_filename_wav, audio_time_series_conditional, sr)\n",
    "    print(f\"Generated conditional audio saved successfully to {output_filename_wav}\")\n",
    "\n",
    "\n",
    "    # Convert the WAV file to MP3\n",
    "    print(f\"Converting {output_filename_wav} to {output_filename_mp3}...\")\n",
    "    try:\n",
    "        audio = AudioSegment.from_wav(output_filename_wav)\n",
    "        audio.export(output_filename_mp3, format=\"mp3\")\n",
    "        print(f\"Conversion to MP3 successful. Output saved to {output_filename_mp3}\")\n",
    "        # Optionally, remove the intermediate WAV file\n",
    "        # os.remove(output_filename_wav)\n",
    "        # print(f\"Removed intermediate WAV file: {output_filename_wav}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MP3 conversion: {e}\")\n",
    "        return None # Return None if MP3 conversion fails\n",
    "\n",
    "\n",
    "    # Optionally, save the generated lyrics to a text file if lyrics were generated\n",
    "    if processed_lyrics and input_type == 'text_prompt':\n",
    "        lyrics_output_filename = 'generated_lyrics.txt'\n",
    "        with open(lyrics_output_filename, 'w') as f:\n",
    "            f.write(processed_lyrics)\n",
    "        print(f\"Generated lyrics saved to {lyrics_output_filename}\")\n",
    "\n",
    "\n",
    "    return output_filename_mp3 # Return the path to the MP3 file\n",
    "\n",
    "# --- Re-run feature extraction, model definition, and training to ensure variables are defined ---\n",
    "# Define the paths to the extracted datasets (using the recheck directories)\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck'\n",
    "\n",
    "bird_sound_files = []\n",
    "music_instrument_files = []\n",
    "\n",
    "# Identify bird sound files from the second dataset (assuming 'Voice of Birds' contains bird sounds)\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        if 'Voice of Birds' in root and name.endswith('.mp3'):\n",
    "            bird_sound_files.append(os.path.join(root, name))\n",
    "\n",
    "# Identify music instrument files from the first dataset (assuming Test_submission and Train_submission contain music instruments)\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        if ('Test_submission' in root or 'Train_submission' in root) and (name.endswith('.wav') or name.endswith('.mp3')):\n",
    "             # Further check if the file name indicates a music instrument\n",
    "            if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords']):\n",
    "                 music_instrument_files.append(os.path.join(root, name))\n",
    "\n",
    "# Use only music instrument files for feature extraction for the music generation model\n",
    "files_to_process_music = music_instrument_files\n",
    "\n",
    "features_music = []\n",
    "\n",
    "print(\"Extracting MFCC features from music instrument files for training music generation model...\")\n",
    "def extract_mfcc_features_only(file_path, target_sequence_length, n_mfcc=20):\n",
    "    \"\"\"Extract only MFCC features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Pad or truncate MFCCs to a fixed length\n",
    "        if mfccs.shape[1] < target_sequence_length:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, target_sequence_length - mfccs.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :target_sequence_length]\n",
    "\n",
    "        return mfccs.T # Transpose to have shape (time steps, n_mfcc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "for i, file_path in enumerate(files_to_process_music):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processing music instrument file {i + 1}/{len(files_to_process_music)}\")\n",
    "    mfccs = extract_mfcc_features_only(file_path, max_padding, n_mfcc=n_mfcc)\n",
    "    if mfccs is not None:\n",
    "        features_music.append(mfccs)\n",
    "\n",
    "\n",
    "# Convert music features to numpy array\n",
    "features_music = np.array(features_music)\n",
    "\n",
    "# Split music data into training and validation sets\n",
    "X_train_music, X_val_music = train_test_split(features_music, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(\"\\nMusic feature extraction complete.\")\n",
    "print(f\"Total music samples: {len(features_music)}\")\n",
    "print(f\"Shape of music features array: {features_music.shape}\")\n",
    "print(f\"Shape of training music features: {X_train_music.shape}\")\n",
    "print(f\"Shape of validation music features: {X_val_music.shape}\")\n",
    "\n",
    "# Define the dimensions for the music generation model\n",
    "musical_feature_dim = X_train_music.shape[2] # Number of MFCCs for musical output\n",
    "musical_sequence_length = X_train_music.shape[1] # Length of the MFCC sequences\n",
    "\n",
    "# Define the conditioning input dimensions\n",
    "# For this model architecture, the conditioning input is a single vector.\n",
    "# It will be either a text embedding or an averaged+projected audio feature vector.\n",
    "conditioning_feature_dim = 128 # Dimension of the conditioning feature vector\n",
    "\n",
    "# Define the input layers for the conditional music generation model\n",
    "# Input 1: Musical features (MFCCs) - the sequence we are building upon\n",
    "musical_input = Input(shape=(musical_sequence_length, musical_feature_dim), name='musical_input')\n",
    "# Input 2: Conditioning input (single vector) - text embedding or processed audio feature vector\n",
    "conditioning_input = Input(shape=(conditioning_feature_dim,), name='conditioning_input')\n",
    "\n",
    "\n",
    "# Repeat the conditioning input across the musical sequence length\n",
    "repeated_conditioning = tf.keras.layers.RepeatVector(musical_sequence_length)(conditioning_input)\n",
    "\n",
    "# Concatenate the musical input with the repeated conditioning input\n",
    "combined_input = Concatenate(axis=-1)([musical_input, repeated_conditioning])\n",
    "\n",
    "# GRU layers for the sequence generation\n",
    "gru_1 = GRU(128, activation='relu', return_sequences=True)(combined_input)\n",
    "dropout_1 = Dropout(0.2)(gru_1)\n",
    "\n",
    "gru_2 = GRU(128, activation='relu', return_sequences=True)(dropout_1)\n",
    "dropout_2 = Dropout(0.2)(gru_2)\n",
    "\n",
    "# TimeDistributed Dense layer to output the generated musical features (MFCCs) for each time step\n",
    "output_layer = TimeDistributed(Dense(musical_feature_dim, activation='linear'))(dropout_2)\n",
    "\n",
    "# Define the conditioned music generation model\n",
    "model_gen_conditioned = Model(inputs=[musical_input, conditioning_input], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=1.0) # Add gradient clipping\n",
    "model_gen_conditioned.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Print the model summary\n",
    "model_gen_conditioned.summary()\n",
    "\n",
    "\n",
    "# Create dummy conditioning data for training the conditioned music generation model\n",
    "# The dummy conditioning data needs to match the expected input shape for conditioning_input\n",
    "dummy_conditioning_train = np.random.rand(X_train_music.shape[0], conditioning_feature_dim)\n",
    "dummy_conditioning_val = np.random.rand(X_val_music.shape[0], conditioning_feature_dim)\n",
    "\n",
    "# Train the conditioned GRU generation model\n",
    "print(\"\\nTraining the conditioned GRU music generation model with dummy input...\")\n",
    "history_gen_conditioned = model_gen_conditioned.fit(\n",
    "    [X_train_music, dummy_conditioning_train],\n",
    "    X_train_music, # The target output is the same musical sequence\n",
    "    epochs=50, # Increased epochs as training with dummy data is quick\n",
    "    batch_size=32,\n",
    "    validation_data=([X_val_music, dummy_conditioning_val], X_val_music) # Use validation data\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "\n",
    "# --- Demonstrate Music Generation from User Audio (using dummy audio file) ---\n",
    "# Note: Replace 'path/to/your/audio.wav' with the actual path to a user-recorded audio file\n",
    "# We need to provide a dummy audio file path for demonstration if a real one isn't available.\n",
    "# For demonstration, let's create a dummy audio file if it doesn't exist.\n",
    "dummy_audio_path = '/content/dummy_user_audio.wav'\n",
    "if not os.path.exists(dummy_audio_path):\n",
    "    print(f\"Creating a dummy audio file at {dummy_audio_path} for demonstration.\")\n",
    "    # Create a short silent WAV file as a placeholder\n",
    "    dummy_audio_ts = np.zeros(sr * 5) # 5 seconds of silence\n",
    "    sf.write(dummy_audio_path, dummy_audio_ts, sr)\n",
    "\n",
    "\n",
    "user_audio_input_path = dummy_audio_path # Use the dummy audio file for demonstration\n",
    "print(f\"\\nAttempting to generate music from audio file: {user_audio_input_path}\")\n",
    "\n",
    "if os.path.exists(user_audio_input_path):\n",
    "     # Use the updated generate_music_from_input function\n",
    "     generated_file_audio_mp3 = generate_music_from_input(model_gen_conditioned, user_audio_input_path, 'audio',\n",
    "                                                      generation_steps=200, sr=sr, max_padding=max_padding, n_mfcc=n_mfcc, n_chroma=n_chroma, conditioning_feature_dim=conditioning_feature_dim)\n",
    "     if generated_file_audio_mp3:\n",
    "         print(f\"\\nMusic generated from audio saved to: {generated_file_audio_mp3}\")\n",
    "     else:\n",
    "         print(\"\\nFailed to generate music from audio.\")\n",
    "else:\n",
    "    print(f\"\\nUser audio file not found at {user_audio_input_path}. Skipping audio-conditioned generation example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyrm-jbx4xYu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9832e1d1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck'\n",
    "\n",
    "print(f\"Files in '{extracted_path_one}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_two}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "437c4e51"
   },
   "outputs": [],
   "source": [
    "!curl -L \"https://storage.googleapis.com/kaggle-data-sets/2768513/4784071/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250827%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250827T024359Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=1a1ddbaf1b0e32e9aabd16dd7e2f70fd8fe2ae40dcdf8e65453881aba3d1db0ccc133c315998513fdbb1093f834d23234a8a8ab08924dcc653b3b75d0841cf7e6d013fd68b33d60fee3a355e88b56f26362ff5f6a57d1f50c75d8a8e64bb28db4ba8918a3ffce070c310fd1c12f0648ce4b3768481ca3faceefbe0ae1d9d5db37123030768f70da56cae3988736fa857ce149fc39f0489b001495f73889e5bbb0fa093f3b0fb5e9854fad6f9f74115979c4747898315a284f42df649583810b62e7c86b49d2ec2c98c3203ff90f7d2c19201b15b9fb57386197c55f28a0c56c6a537e100a85647b53b8307ddfa00810766c7a248fa3b81c2f6edd3ead8d00da\" -o archive.zip\n",
    "!curl -L \"https://storage.googleapis.com/kaggle-data-sets/2750746/4753077/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250827%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250827T024259Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=6a1bc74b093049c74c92a8d2d572a9222a8e5f225175c3d71bdcf9ddd1371b56382e176ad942bfab0db6351eaec280cbf9cb6f4b3a99cf96d4c73753f05d54958db32fbf765280cc47b280da3700caf22c84590285ec390dd27e57c4ac6152107cd8148ee15da87e2f2a42a7fc059d1ee65825bd969480898d9d06d8969b16c475c55981300d3ec48520fad01323a62a45c82c8b193f6e11c83bae0c2ca63174f142f1b51ed209243c7dec994ed256ef1ca71868bf08f6bbcb5e71241ff9e694a599ddc3d8ceb1cad9ef99ae0614b0fe01cec125d880870532b54549238fbb69f0f36edaddf7799bdea379965474d31f74306d81d4abaa3499d1fee2c29125c5\" -o dataset_second.zip\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "archive_path_one = '/content/archive.zip'\n",
    "archive_path_two = '/content/dataset_second.zip'\n",
    "\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck'\n",
    "\n",
    "os.makedirs(extracted_path_one, exist_ok=True)\n",
    "os.makedirs(extracted_path_two, exist_ok=True)\n",
    "\n",
    "print(f\"Attempting to re-extract '{archive_path_one}' to '{extracted_path_one}'\")\n",
    "try:\n",
    "    with zipfile.ZipFile(archive_path_one, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_path_one)\n",
    "    print(\"Extraction of archive.zip successful.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: '{archive_path_one}' is a bad zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction of '{archive_path_one}': {e}\")\n",
    "\n",
    "\n",
    "print(f\"\\nAttempting to re-extract '{archive_path_two}' to '{extracted_path_two}'\")\n",
    "try:\n",
    "    with zipfile.ZipFile(archive_path_two, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_path_two)\n",
    "    print(\"Extraction of dataset_second.zip successful.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: '{archive_path_two}' is a bad zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction of '{archive_path_two}': {e}\")\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_one}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_two}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3895598"
   },
   "outputs": [],
   "source": [
    "!curl -L \"https://storage.googleapis.com/kaggle-data-sets/2768513/4784071/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250827%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250827T024359Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=1a1ddbaf1b0e32e9aabd16dd7e2f70fd8fe2ae40dcdf8e65453881aba3d1db0ccc133c315998513fdbb1093f834d23234a8a8ab08924dcc653b3b75d0841cf7e6d013fd68b33d60fee3a355e88b56f26362ff5f6a57d1f50c75d8a8e64bb28db4ba8918a3ffce070c310fd1c12f0648ce4b3768481ca3faceefbe0ae1d9d5db37123030768f70da56cae3988736fa857ce149fc39f0489b001495f73889e5bbb0fa093f3b0fb5e9854fad6f9f74115979c4747898315a284f42df649583810b62e7c86b49d2ec2c98c3203ff90f7d2c19201b15b9fb57386197c55f28a0c56c6a537e100a85647b53b8307ddfa00810766c7a248fa3b81c2f6edd3ead8d00da\" -o archive.zip\n",
    "!curl -L \"https://storage.googleapis.com/kaggle-data-sets/2750746/4753077/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250827%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250827T024259Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=6a1bc74b093049c74c92a8d2d572a9222a8e5f225175c3d71bdcf9ddd1371b56382e176ad942bfab0db6351eaec280cbf9cb6f4b3a99cf96d4c73753f05d54958db32fbf765280cc47b280da3700caf22c84590285ec390dd27e57c4ac6152107cd8148ee15da87e2f2a42a7fc059d1ee65825bd969480898d9d06d8969b16c475c55981300d3ec48520fad01323a62a45c82c8b193f6e11c83bae0c2ca63174f142f1b51ed209243c7dec994ed256ef1ca71868bf08f6bbcb5e71241ff9e694a599ddc3d8ceb1cad9ef99ae0614b0fe01cec125d880870532b54549238fbb69f0f36edaddf7799bdea379965474d31f74306d81d4abaa3499d1fee2c29125c5\" -o dataset_second.zip\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "archive_path_one = '/content/archive.zip'\n",
    "archive_path_two = '/content/dataset_second.zip'\n",
    "\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck'\n",
    "\n",
    "os.makedirs(extracted_path_one, exist_ok=True)\n",
    "os.makedirs(extracted_path_two, exist_ok=True)\n",
    "\n",
    "print(f\"Attempting to re-extract '{archive_path_one}' to '{extracted_path_one}'\")\n",
    "try:\n",
    "    with zipfile.ZipFile(archive_path_one, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_path_one)\n",
    "    print(\"Extraction of archive.zip successful.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: '{archive_path_one}' is a bad zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction of '{archive_path_one}': {e}\")\n",
    "\n",
    "\n",
    "print(f\"\\nAttempting to re-extract '{archive_path_two}' to '{extracted_path_two}'\")\n",
    "try:\n",
    "    with zipfile.ZipFile(archive_path_two, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_path_two)\n",
    "    print(\"Extraction of dataset_second.zip successful.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: '{archive_path_two}' is a bad zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction of '{archive_path_two}': {e}\")\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_one}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_two}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "de349403"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, GRU, Input, TimeDistributed\n",
    "from pydub import AudioSegment # Import AudioSegment for MP3 conversion\n",
    "from transformers import pipeline # Import pipeline for using pre-trained models\n",
    "\n",
    "# Define sampling rate (needed for audio processing and saving)\n",
    "sr = 22050 # Common sampling rate, adjust if your audio files use a different rate\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This will need to be determined based on the desired sequence length or maximum length of audio files\n",
    "n_chroma = 12 # Number of chroma bins\n",
    "total_audio_features_dim = n_mfcc + n_chroma # Total features for audio conditioning\n",
    "\n",
    "def extract_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "    \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract Chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "        # Pad or truncate combined features to a fixed length\n",
    "        if combined_features.shape[1] < target_sequence_length:\n",
    "            combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "        else:\n",
    "            combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "        return combined_features.T # Transpose to have shape (time steps, total_features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_user_audio_for_conditioning(audio_file_path, target_sequence_length, n_mfcc, n_chroma):\n",
    "    \"\"\"Processes user audio input and returns features suitable for conditioning.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path: Path to the user's audio file.\n",
    "        target_sequence_length: The expected sequence length for model input (e.g., max_padding).\n",
    "        n_mfcc: The number of MFCCs to extract.\n",
    "        n_chroma: The number of chroma bins to extract.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the processed audio features (shape: (1, target_sequence_length, total_feature_dimension)),\n",
    "        or None if processing fails.\n",
    "    \"\"\"\n",
    "    processed_features = extract_features(audio_file_path, target_sequence_length, n_mfcc, n_chroma)\n",
    "\n",
    "    if processed_features is not None:\n",
    "        # Add a batch dimension\n",
    "        processed_features = processed_features[np.newaxis, :, :] # Shape (1, target_sequence_length, total_feature_dimension)\n",
    "        print(f\"Processed user audio features shape for conditioning: {processed_features.shape}\")\n",
    "        return processed_features\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Updated function to generate lyrics using a language model ---\n",
    "# Initialize the text generation pipeline\n",
    "# Using a smaller model for faster execution, e.g., 'gpt2' or 'distilgpt2'\n",
    "try:\n",
    "    lyric_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "    print(\"Text generation pipeline (distilgpt2) initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing text generation pipeline: {e}\")\n",
    "    lyric_generator = None # Set to None if initialization fails\n",
    "\n",
    "\n",
    "def generate_lyrics_with_language_model(prompt):\n",
    "    \"\"\"Generates lyrics based on a prompt using a language model.\"\"\"\n",
    "    if lyric_generator:\n",
    "        print(f\"Generating lyrics for prompt: '{prompt}' using DistilGPT-2.\")\n",
    "        try:\n",
    "            # Generate text with a reasonable length and without generating too many sequences\n",
    "            generated_text = lyric_generator(prompt, max_length=100, num_return_sequences=1, truncation=True)\n",
    "            lyrics = generated_text[0]['generated_text']\n",
    "            # Basic post-processing: remove the prompt from the generated text if it's repeated\n",
    "            if lyrics.startswith(prompt):\n",
    "                lyrics = lyrics[len(prompt):].strip()\n",
    "            print(\"Lyric generation successful.\")\n",
    "            return lyrics\n",
    "        except Exception as e:\n",
    "            print(f\"Error during lyric generation: {e}\")\n",
    "            return f\"Error generating lyrics for prompt: '{prompt}'\" # Return an error message\n",
    "    else:\n",
    "        print(\"Lyric generator not initialized. Cannot generate lyrics.\")\n",
    "        return f\"Cannot generate lyrics for prompt: '{prompt}' (generator not available)\"\n",
    "\n",
    "\n",
    "# Function to get text embeddings\n",
    "# We'll use a placeholder for a text embedding model\n",
    "def get_text_embedding(text, embedding_dim):\n",
    "    \"\"\"Placeholder function to get a fixed-size embedding for text.\"\"\"\n",
    "    print(f\"Getting embedding for text: '{text[:50]}...' using a placeholder model.\")\n",
    "    return np.random.rand(1, embedding_dim) # Shape (1, embedding_dim)\n",
    "\n",
    "def process_text_input(user_input, embedding_dim, is_prompt=False):\n",
    "    \"\"\"Processes user text input (lyrics or prompt) and returns a conditioning embedding.\"\"\"\n",
    "    if is_prompt:\n",
    "        lyrics = generate_lyrics_with_language_model(user_input)\n",
    "    else:\n",
    "        lyrics = user_input # User provided lyrics directly\n",
    "\n",
    "    conditioning_embedding = get_text_embedding(lyrics, embedding_dim)\n",
    "\n",
    "    return conditioning_embedding, lyrics\n",
    "\n",
    "\n",
    "def generate_conditional_mfccs(model, start_sequence, conditioning_input, generation_steps):\n",
    "    \"\"\"Generates new MFCC sequences using the trained conditional model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras conditional generation model.\n",
    "        start_sequence: A numpy array representing the initial musical sequence\n",
    "                        (shape: (1, sequence_length, musical_feature_dim)).\n",
    "        conditioning_input: The conditioning input (text embedding or audio features).\n",
    "                            Shape varies based on input_type.\n",
    "        generation_steps: The number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated musical MFCC sequence.\n",
    "    \"\"\"\n",
    "    generated_sequence = np.copy(start_sequence)\n",
    "    current_musical_sequence = np.copy(start_sequence)\n",
    "    sequence_length = start_sequence.shape[1] # Get the expected sequence length\n",
    "\n",
    "    # Assuming the model takes a list of inputs: [musical_input, conditioning_input]\n",
    "    # The conditioning_input's shape depends on whether it's text or audio\n",
    "    # If text, it's (1, conditioning_feature_dim)\n",
    "    # If audio, it's (1, sequence_length, total_audio_features_dim)\n",
    "\n",
    "    # Determine the input structure for the model based on the shape of conditioning_input\n",
    "    if conditioning_input.shape[-1] == conditioning_feature_dim and conditioning_input.ndim == 2:\n",
    "        # Text conditioning (single vector)\n",
    "        model_inputs = [current_musical_sequence, conditioning_input]\n",
    "        print(\"Using text conditioning input shape:\", conditioning_input.shape)\n",
    "    elif conditioning_input.shape[-1] == total_audio_features_dim and conditioning_input.ndim == 3:\n",
    "         # Audio conditioning (sequence)\n",
    "         # For sequential conditioning, we might need a different model architecture\n",
    "         # or a way to combine the audio features sequence with the musical sequence over time.\n",
    "         # Given the current model expects a single conditioning vector repeated,\n",
    "         # this approach with sequential audio conditioning will require model modification.\n",
    "         # For now, let's adapt the generation to work with the current model by\n",
    "         # averaging the audio features if sequential audio conditioning is provided,\n",
    "         # or modifying the model to handle sequential conditioning properly.\n",
    "\n",
    "         # Since the model architecture is defined to take a single vector conditioning input,\n",
    "         # we need to convert the sequential audio features to a single vector.\n",
    "         # Averaging over time and projecting is a possible approach, but might lose temporal info.\n",
    "         print(\"Warning: Model architecture is designed for single vector conditioning. Averaging audio features.\")\n",
    "         averaged_audio_features = np.mean(conditioning_input, axis=1) # Shape (1, total_audio_features_dim)\n",
    "\n",
    "         # Need a projection layer if total_audio_features_dim != conditioning_feature_dim\n",
    "         if total_audio_features_dim != conditioning_feature_dim:\n",
    "             print(\"Warning: Projecting averaged audio features to match conditioning_feature_dim.\")\n",
    "             # Define a simple Dense layer for this purpose\n",
    "             audio_projection_layer = Dense(conditioning_feature_dim, activation='relu')\n",
    "             # Need to build this layer if it's the first time it's called\n",
    "             if not audio_projection_layer.built:\n",
    "                  audio_projection_layer.build(input_shape=(None, total_audio_features_dim))\n",
    "             conditioning_input_vector = audio_projection_layer(averaged_audio_features)\n",
    "         else:\n",
    "             conditioning_input_vector = averaged_audio_features # Use directly if dimensions match\n",
    "\n",
    "         model_inputs = [current_musical_sequence, conditioning_input_vector]\n",
    "         print(\"Using averaged and projected audio conditioning input shape:\", conditioning_input_vector.shape)\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Unexpected conditioning input shape.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    for _ in range(generation_steps):\n",
    "        # Predict the next time step of musical features\n",
    "        predicted_mfcc = model.predict(model_inputs) # Model expects [musical_input, conditioning_input]\n",
    "\n",
    "\n",
    "        # Append the predicted MFCC to the generated sequence (only the new time step)\n",
    "        generated_sequence = np.append(generated_sequence, predicted_mfcc[:, -1:, :], axis=1)\n",
    "\n",
    "        # Update the current musical sequence by removing the oldest time step and adding the new one\n",
    "        # Ensure the sequence length remains constant\n",
    "        current_musical_sequence = np.append(current_musical_sequence[:, 1:, :], predicted_mfcc[:, -1:, :], axis=1)\n",
    "\n",
    "        # If using sequential audio conditioning, need to update the conditioning input for the next step as well\n",
    "        # This would require a different model architecture that processes both sequences.\n",
    "        # With the current model expecting a single conditioning vector, this update is not applicable\n",
    "        # and the single conditioning vector remains constant throughout generation.\n",
    "\n",
    "\n",
    "    return generated_sequence.squeeze() # Remove the batch dimension\n",
    "\n",
    "\n",
    "def generate_music_from_input(model, user_input, input_type, generation_steps=200, sr=22050, max_padding=174, n_mfcc=20, n_chroma=12, conditioning_feature_dim=128):\n",
    "    \"\"\"Generates music based on user input (text or audio).\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras conditional music generation model.\n",
    "        user_input: The user's input (string for text, file path for audio).\n",
    "        input_type: Type of input ('text_lyrics', 'text_prompt', 'audio').\n",
    "        generation_steps: Number of additional time steps to generate.\n",
    "        sr: Sampling rate for audio output.\n",
    "        max_padding: Target sequence length for padding/truncating features.\n",
    "        n_mfcc: Number of MFCCs extracted.\n",
    "        n_chroma: Number of chroma bins extracted.\n",
    "        conditioning_feature_dim: Dimension of the conditioning feature vector (for text).\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        The path to the generated audio file (MP3), or None if an error occurred.\n",
    "    \"\"\"\n",
    "    conditioning_input_for_model = None\n",
    "    processed_lyrics = None # To store generated lyrics if applicable\n",
    "\n",
    "    if input_type == 'text_lyrics' or input_type == 'text_prompt':\n",
    "        is_prompt = (input_type == 'text_prompt')\n",
    "        conditioning_input_for_model, processed_lyrics = process_text_input(user_input, conditioning_feature_dim, is_prompt=is_prompt)\n",
    "        if conditioning_input_for_model is None:\n",
    "            print(\"Error processing text input for conditioning.\")\n",
    "            return None\n",
    "        # For text conditioning, repeat the single vector for each step if the model architecture requires it\n",
    "        # Our current model concatenates the repeated vector in the model definition, so we pass the single vector.\n",
    "\n",
    "    elif input_type == 'audio':\n",
    "        print(\"Processing user audio input for conditioning.\")\n",
    "        # Process user audio to get features\n",
    "        user_audio_features_sequence = process_user_audio_for_conditioning(user_input, max_padding, n_mfcc, n_chroma)\n",
    "        if user_audio_features_sequence is None:\n",
    "            print(\"Error processing user audio input.\")\n",
    "            return None\n",
    "\n",
    "        # For the current model architecture that expects a single conditioning vector,\n",
    "        # we need to convert the sequential audio features to a single vector.\n",
    "        # A simple approach is to average over time and project.\n",
    "        averaged_audio_features = np.mean(user_audio_features_sequence, axis=1) # Shape (1, total_audio_features_dim)\n",
    "\n",
    "        # Need a projection layer if total_audio_features_dim != conditioning_feature_dim\n",
    "        # Define a simple Dense layer for this purpose\n",
    "        total_audio_features = n_mfcc + n_chroma # Update total features dimension\n",
    "        audio_projection_layer = Dense(conditioning_feature_dim, activation='relu')\n",
    "        # Need to build this layer if it's the first time it's called\n",
    "        if not audio_projection_layer.built:\n",
    "             audio_projection_layer.build(input_shape=(None, total_audio_features))\n",
    "\n",
    "        conditioning_input_for_model = audio_projection_layer(averaged_audio_features)\n",
    "        print(f\"Projected audio conditioning vector shape: {conditioning_input_for_model.shape}\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid input type. Please use 'text_lyrics', 'text_prompt', or 'audio'.\")\n",
    "        return None\n",
    "\n",
    "    # Ensure conditioning_input_for_model is not None before proceeding\n",
    "    if conditioning_input_for_model is None:\n",
    "         print(\"Conditioning input for model is None. Cannot proceed.\")\n",
    "         return None\n",
    "\n",
    "    # Generate a starting musical sequence (e.g., take a random sample from the training data)\n",
    "    if 'X_train_music' not in globals() or X_train_music.shape[0] == 0:\n",
    "         print(\"Training data (X_train_music) is not available or empty. Cannot generate a starting sequence.\")\n",
    "         return None\n",
    "\n",
    "\n",
    "    start_index = np.random.randint(0, X_train_music.shape[0])\n",
    "    # The starting sequence should have the same feature dimension as the model's musical input\n",
    "    # which is just MFCCs based on the model definition\n",
    "    start_sequence = X_train_music[start_index:start_index+1, :, :]\n",
    "\n",
    "\n",
    "    # Generate new musical features using the trained conditional GRU model\n",
    "    print(\"\\nGenerating new conditional musical features...\")\n",
    "    # The model expects musical_input (MFCCs) and conditioning_input\n",
    "    # The conditioning_input is now a single vector (1, conditioning_feature_dim)\n",
    "    generated_musical_features = generate_conditional_mfccs(model, start_sequence, conditioning_input_for_model, generation_steps)\n",
    "\n",
    "\n",
    "    print(f\"Shape of the generated musical features: {generated_musical_features.shape}\")\n",
    "\n",
    "    # Convert the generated musical features (MFCCs) back to an audio time-series\n",
    "    print(\"\\nConverting generated musical features to audio...\")\n",
    "    # Ensure the generated features are only MFCCs if the inverse transform expects only MFCCs\n",
    "    # If the generation model outputs combined features, we would need to separate or use a different inverse transform\n",
    "    # Assuming the model is trained to output only MFCCs for the musical output layer\n",
    "    audio_time_series_conditional = librosa.feature.inverse.mfcc_to_audio(generated_musical_features.T, sr=sr)\n",
    "\n",
    "\n",
    "    # Define the output filename (WAV and MP3)\n",
    "    output_filename_wav = 'generated_music_conditional.wav'\n",
    "    output_filename_mp3 = 'generated_music_conditional.mp3'\n",
    "\n",
    "\n",
    "    # Save the generated audio to a WAV file first\n",
    "    print(f\"Saving generated conditional audio to {output_filename_wav}...\")\n",
    "    sf.write(output_filename_wav, audio_time_series_conditional, sr)\n",
    "    print(f\"Generated conditional audio saved successfully to {output_filename_wav}\")\n",
    "\n",
    "\n",
    "    # Convert the WAV file to MP3\n",
    "    print(f\"Converting {output_filename_wav} to {output_filename_mp3}...\")\n",
    "    try:\n",
    "        audio = AudioSegment.from_wav(output_filename_wav)\n",
    "        audio.export(output_filename_mp3, format=\"mp3\")\n",
    "        print(f\"Conversion to MP3 successful. Output saved to {output_filename_mp3}\")\n",
    "        # Optionally, remove the intermediate WAV file\n",
    "        # os.remove(output_filename_wav)\n",
    "        # print(f\"Removed intermediate WAV file: {output_filename_wav}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MP3 conversion: {e}\")\n",
    "        return None # Return None if MP3 conversion fails\n",
    "\n",
    "\n",
    "    # Optionally, save the generated lyrics to a text file if lyrics were generated\n",
    "    if processed_lyrics and input_type == 'text_prompt':\n",
    "        lyrics_output_filename = 'generated_lyrics.txt'\n",
    "        with open(lyrics_output_filename, 'w') as f:\n",
    "            f.write(processed_lyrics)\n",
    "        print(f\"Generated lyrics saved to {lyrics_output_filename}\")\n",
    "\n",
    "\n",
    "    return output_filename_mp3 # Return the path to the MP3 file\n",
    "\n",
    "# --- Re-run feature extraction, model definition, and training to ensure variables are defined ---\n",
    "# Define the paths to the extracted datasets (using the recheck directories)\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck' # This directory might be empty\n",
    "\n",
    "bird_sound_files = []\n",
    "music_instrument_files = []\n",
    "\n",
    "# Identify bird sound files from the second dataset (assuming 'Voice of Birds' contains bird sounds)\n",
    "print(f\"Searching for bird sound files in: {extracted_path_two}\") # Debug print\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        if 'Voice of Birds' in root and name.endswith('.mp3'):\n",
    "            bird_sound_files.append(os.path.join(root, name))\n",
    "print(f\"Found {len(bird_sound_files)} bird sound files.\") # Debug print\n",
    "\n",
    "\n",
    "# Identify music instrument files from the first dataset (assuming Test_submission and Train_submission contain music instruments)\n",
    "print(f\"Searching for music instrument files in: {extracted_path_one}\") # Debug print\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        if ('Test_submission' in root or 'Train_submission' in root) and (name.endswith('.wav') or name.endswith('.mp3')):\n",
    "             # Further check if the file name indicates a music instrument\n",
    "            if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords']):\n",
    "                 music_instrument_files.append(os.path.join(root, name))\n",
    "print(f\"Found {len(music_instrument_files)} music instrument files.\") # Debug print\n",
    "\n",
    "\n",
    "# Use only music instrument files for feature extraction for the music generation model\n",
    "files_to_process_music = music_instrument_files\n",
    "\n",
    "features_music = []\n",
    "\n",
    "print(\"Extracting MFCC features from music instrument files for training music generation model...\")\n",
    "def extract_mfcc_features_only(file_path, target_sequence_length, n_mfcc=20):\n",
    "    \"\"\"Extract only MFCC features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Pad or truncate MFCCs to a fixed length\n",
    "        if mfccs.shape[1] < target_sequence_length:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, target_sequence_length - mfccs.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :target_sequence_length]\n",
    "\n",
    "        return mfccs.T # Transpose to have shape (time steps, n_mfcc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "for i, file_path in enumerate(files_to_process_music):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processing music instrument file {i + 1}/{len(files_to_process_music)}\")\n",
    "    mfccs = extract_mfcc_features_only(file_path, max_padding, n_mfcc=n_mfcc)\n",
    "    if mfccs is not None:\n",
    "        features_music.append(mfccs)\n",
    "\n",
    "\n",
    "# Convert music features to numpy array\n",
    "features_music = np.array(features_music)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "# Add a check to ensure features_music is not empty before splitting\n",
    "if len(features_music) > 0:\n",
    "    X_train_music, X_val_music = train_test_split(features_music, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    print(\"No music features extracted. Cannot proceed with model training and generation.\")\n",
    "    X_train_music = np.array([]) # Initialize as empty array to prevent errors\n",
    "    X_val_music = np.array([]) # Initialize as empty array to prevent errors\n",
    "\n",
    "\n",
    "print(\"\\nMusic feature extraction complete.\")\n",
    "print(f\"Total music samples: {len(features_music)}\")\n",
    "print(f\"Shape of music features array: {features_music.shape}\")\n",
    "print(f\"Shape of training music features: {X_train_music.shape}\")\n",
    "print(f\"Shape of validation music features: {X_val_music.shape}\")\n",
    "\n",
    "# Define the dimensions for the music generation model\n",
    "# Add a check to ensure X_train_music is not empty before accessing shape\n",
    "if X_train_music.shape[0] > 0:\n",
    "    musical_feature_dim = X_train_music.shape[2] # Number of MFCCs for musical output\n",
    "    musical_sequence_length = X_train_music.shape[1] # Length of the MFCC sequences\n",
    "\n",
    "    # Define the conditioning input dimensions\n",
    "    # For this model architecture, the conditioning input is a single vector.\n",
    "    # It will be either a text embedding or an averaged+projected audio feature vector.\n",
    "    conditioning_feature_dim = 128 # Dimension of the conditioning feature vector\n",
    "\n",
    "    # Define the input layers for the conditional music generation model\n",
    "    # Input 1: Musical features (MFCCs) - the sequence we are building upon\n",
    "    musical_input = Input(shape=(musical_sequence_length, musical_feature_dim), name='musical_input')\n",
    "    # Input 2: Conditioning input (single vector) - text embedding or processed audio feature vector\n",
    "    conditioning_input = Input(shape=(conditioning_feature_dim,), name='conditioning_input')\n",
    "\n",
    "\n",
    "    # Repeat the conditioning input across the musical sequence length\n",
    "    repeated_conditioning = tf.keras.layers.RepeatVector(musical_sequence_length)(conditioning_input)\n",
    "\n",
    "    # Concatenate the musical input with the repeated conditioning input\n",
    "    combined_input = Concatenate(axis=-1)([musical_input, repeated_conditioning])\n",
    "\n",
    "    # GRU layers for the sequence generation\n",
    "    gru_1 = GRU(128, activation='relu', return_sequences=True)(combined_input)\n",
    "    dropout_1 = Dropout(0.2)(gru_1)\n",
    "\n",
    "    gru_2 = GRU(128, activation='relu', return_sequences=True)(dropout_1)\n",
    "    dropout_2 = Dropout(0.2)(gru_2)\n",
    "\n",
    "    # TimeDistributed Dense layer to output the generated musical features (MFCCs) for each time step\n",
    "    output_layer = TimeDistributed(Dense(musical_feature_dim, activation='linear'))(dropout_2)\n",
    "\n",
    "    # Define the conditioned music generation model\n",
    "    model_gen_conditioned = Model(inputs=[musical_input, conditioning_input], outputs=output_layer)\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam(clipvalue=1.0) # Add gradient clipping\n",
    "    model_gen_conditioned.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "    # Print the model summary\n",
    "    model_gen_conditioned.summary()\n",
    "\n",
    "\n",
    "    # Create dummy conditioning data for training the conditioned music generation model\n",
    "    # The dummy conditioning data needs to match the expected input shape for conditioning_input\n",
    "    dummy_conditioning_train = np.random.rand(X_train_music.shape[0], conditioning_feature_dim)\n",
    "    dummy_conditioning_val = np.random.rand(X_val_music.shape[0], conditioning_feature_dim)\n",
    "\n",
    "    # Train the conditioned GRU generation model\n",
    "    print(\"\\nTraining the conditioned GRU music generation model with dummy input...\")\n",
    "    history_gen_conditioned = model_gen_conditioned.fit(\n",
    "        [X_train_music, dummy_conditioning_train],\n",
    "        X_train_music, # The target output is the same musical sequence\n",
    "        epochs=50, # Increased epochs as training with dummy data is quick\n",
    "        batch_size=32,\n",
    "        validation_data=([X_val_music, dummy_conditioning_val], X_val_music) # Use validation data\n",
    "    )\n",
    "\n",
    "    print(\"\\nTraining complete.\")\n",
    "\n",
    "    # --- Demonstrate Music Generation from User Audio (using dummy audio file) ---\n",
    "    # Note: Replace 'path/to/your/audio.wav' with the actual path to a user-recorded audio file\n",
    "    # We need to provide a dummy audio file path for demonstration if a real one isn't available.\n",
    "    # For demonstration, let's create a dummy audio file if it doesn't exist.\n",
    "    dummy_audio_path = '/content/dummy_user_audio.wav'\n",
    "    if not os.path.exists(dummy_audio_path):\n",
    "        print(f\"Creating a dummy audio file at {dummy_audio_path} for demonstration.\")\n",
    "        # Create a short silent WAV file as a placeholder\n",
    "        dummy_audio_ts = np.zeros(sr * 5) # 5 seconds of silence\n",
    "        sf.write(dummy_audio_path, dummy_audio_ts, sr)\n",
    "\n",
    "\n",
    "    user_audio_input_path = dummy_audio_path # Use the dummy audio file for demonstration\n",
    "    print(f\"\\nAttempting to generate music from audio file: {user_audio_input_path}\")\n",
    "\n",
    "    if os.path.exists(user_audio_input_path):\n",
    "         # Use the updated generate_music_from_input function\n",
    "         generated_file_audio_mp3 = generate_music_from_input(model_gen_conditioned, user_audio_input_path, 'audio',\n",
    "                                                      generation_steps=200, sr=sr, max_padding=max_padding, n_mfcc=n_mfcc, n_chroma=n_chroma, conditioning_feature_dim=conditioning_feature_dim)\n",
    "         if generated_file_audio_mp3:\n",
    "             print(f\"\\nMusic generated from audio saved to: {generated_file_audio_mp3}\")\n",
    "         else:\n",
    "             print(\"\\nFailed to generate music from audio.\")\n",
    "    else:\n",
    "        print(f\"\\nUser audio file not found at {user_audio_input_path}. Skipping audio-conditioned generation example.\")\n",
    "else:\n",
    "    print(\"Cannot define and train the model as no music features were extracted.\")\n",
    "\n",
    "# --- Demonstrate Lyric Generation and Music Generation from a Text Prompt ---\n",
    "# Assuming model_gen_conditioned, generate_music_from_input,\n",
    "# conditioning_feature_dim, max_padding, n_mfcc, sr are defined.\n",
    "# Also assuming process_text_input and generate_conditional_mfccs are defined.\n",
    "\n",
    "user_lyric_idea_prompt = \"Write a song about a journey through a forest.\"\n",
    "print(f\"\\nAttempting to generate lyrics and music from prompt: '{user_lyric_idea_prompt}'\")\n",
    "\n",
    "# The generate_music_from_input function with input_type='text_prompt'\n",
    "# will call the placeholder lyric generation and then use the resulting lyrics for conditioning.\n",
    "generated_file_from_lyric_idea = generate_music_from_input(model_gen_conditioned, user_lyric_idea_prompt, 'text_prompt',\n",
    "                                                         generation_steps=200, sr=sr, max_padding=max_padding, n_mfcc=n_mfcc, conditioning_feature_dim=conditioning_feature_dim)\n",
    "\n",
    "if generated_file_from_lyric_idea:\n",
    "    print(f\"\\nMusic generated from lyric idea prompt saved to: {generated_file_from_lyric_idea}\")\n",
    "else:\n",
    "    print(\"\\nFailed to generate music from lyric idea prompt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0749bc33"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, GRU, Input, TimeDistributed, Bidirectional\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This needs to be consistent with feature extraction\n",
    "n_chroma = 12 # Number of chroma bins\n",
    "total_audio_features_dim = n_mfcc + n_chroma # Total features for audio conditioning\n",
    "\n",
    "# Define the dimensions for the music generation model\n",
    "musical_feature_dim = n_mfcc # The model output is MFCCs\n",
    "musical_sequence_length = max_padding # The model input sequence length\n",
    "\n",
    "# Define the conditioning input dimensions\n",
    "text_conditioning_dim = 128 # Dimension of the text embedding\n",
    "\n",
    "# --- Redefine the Conditional Music Generation Model to accept Sequential Audio Conditioning ---\n",
    "\n",
    "# Input 1: Musical features (MFCCs) - the sequence we are building upon\n",
    "musical_input = Input(shape=(musical_sequence_length, musical_feature_dim), name='musical_input')\n",
    "\n",
    "# Input 2: Text conditioning input (single vector)\n",
    "text_conditioning_input = Input(shape=(text_conditioning_dim,), name='text_conditioning_input')\n",
    "\n",
    "# Input 3: Audio conditioning input (sequence of features)\n",
    "audio_conditioning_input = Input(shape=(musical_sequence_length, total_audio_features_dim), name='audio_conditioning_input')\n",
    "\n",
    "\n",
    "# Process text conditioning (repeat the single vector across the musical sequence length)\n",
    "repeated_text_conditioning = tf.keras.layers.RepeatVector(musical_sequence_length)(text_conditioning_input)\n",
    "\n",
    "# Concatenate musical input with repeated text conditioning and audio conditioning input\n",
    "# This is one way to combine them. Another would be separate processing branches.\n",
    "# Concatenate [musical_input, repeated_text_conditioning, audio_conditioning_input]\n",
    "combined_input = Concatenate(axis=-1)([musical_input, repeated_text_conditioning, audio_conditioning_input])\n",
    "\n",
    "\n",
    "# GRU layers for the sequence generation, now taking the combined input\n",
    "# We can use Bidirectional GRU for potentially better context\n",
    "gru_1 = Bidirectional(GRU(128, activation='relu', return_sequences=True))(combined_input)\n",
    "dropout_1 = Dropout(0.2)(gru_1)\n",
    "\n",
    "gru_2 = Bidirectional(GRU(128, activation='relu', return_sequences=True))(dropout_1)\n",
    "dropout_2 = Dropout(0.2)(gru_2)\n",
    "\n",
    "# TimeDistributed Dense layer to output the generated musical features (MFCCs) for each time step\n",
    "# The output still needs to match the musical_feature_dim\n",
    "output_layer = TimeDistributed(Dense(musical_feature_dim, activation='linear'))(dropout_2)\n",
    "\n",
    "# Define the new conditioned music generation model\n",
    "# The model now has three inputs: musical_input, text_conditioning_input, audio_conditioning_input\n",
    "model_gen_conditioned_v2 = Model(inputs=[musical_input, text_conditioning_input, audio_conditioning_input], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=1.0) # Add gradient clipping\n",
    "model_gen_conditioned_v2.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Print the model summary\n",
    "model_gen_conditioned_v2.summary()\n",
    "\n",
    "print(\"\\nNew conditional music generation model (v2) architecture defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68957062"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31946897"
   },
   "outputs": [],
   "source": [
    "!curl -L \"https://storage.googleapis.com/kaggle-data-sets/2768513/4784071/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250827%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250827T024359Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=1a1ddbaf1b0e32e9aabd16dd7e2f70fd8fe2ae40dcdf8e65453881aba3d1db0ccc133c315998513fdbb1093f834d23234a8a8ab08924dcc653b3b75d0841cf7e6d013fd68b33d60fee3a355e88b56f26362ff5f6a57d1f50c75d8a8e64bb28db4ba8918a3ffce070c310fd1c12f0648ce4b3768481ca3faceefbe0ae1d9d5db37123030768f70da56cae3988736fa857ce149fc39f0489b001495f73889e5bbb0fa093f3b0fb5e9854fad6fe9f74115979c4747898315a284f42df649583810b62e7c86b49d2ec2c98c3203ff90f7d2c19201b15b9fb57386197c55f28a0c56c6a537e100a85647b53b8307ddfa00810766c7a248fa3b81c2f6edd3ead8d00da\" -o archive.zip\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "archive_path_one = '/content/archive.zip'\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "\n",
    "os.makedirs(extracted_path_one, exist_ok=True)\n",
    "\n",
    "print(f\"Attempting to extract '{archive_path_one}' to '{extracted_path_one}'\")\n",
    "try:\n",
    "    with zipfile.ZipFile(archive_path_one, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_path_one)\n",
    "    print(\"Extraction of archive.zip successful.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: '{archive_path_one}' is a bad zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction of '{archive_path_one}': {e}\")\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_one}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8b72f47b"
   },
   "outputs": [],
   "source": [
    "!pip install pydub ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6702888d"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, GRU, Input, TimeDistributed\n",
    "from pydub import AudioSegment # Import AudioSegment for MP3 conversion\n",
    "from transformers import pipeline # Import pipeline for using pre-trained models\n",
    "\n",
    "# Define sampling rate (needed for audio processing and saving)\n",
    "sr = 22050 # Common sampling rate, adjust if your audio files use a different rate\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This will need to be determined based on the desired sequence length or maximum length of audio files\n",
    "n_chroma = 12 # Number of chroma bins\n",
    "total_audio_features_dim = n_mfcc + n_chroma # Total features for audio conditioning\n",
    "\n",
    "def extract_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "    \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract Chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "        # Pad or truncate combined features to a fixed length\n",
    "        if combined_features.shape[1] < target_sequence_length:\n",
    "            combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "        else:\n",
    "            combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "        return combined_features.T # Transpose to have shape (time steps, total_features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_user_audio_for_conditioning(audio_file_path, target_sequence_length, n_mfcc, n_chroma):\n",
    "    \"\"\"Processes user audio input and returns features suitable for conditioning.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path: Path to the user's audio file.\n",
    "        target_sequence_length: The expected sequence length for model input (e.g., max_padding).\n",
    "        n_mfcc: The number of MFCCs to extract.\n",
    "        n_chroma: The number of chroma bins to extract.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the processed audio features (shape: (1, target_sequence_length, total_feature_dimension)),\n",
    "        or None if processing fails.\n",
    "    \"\"\"\n",
    "    processed_features = extract_features(audio_file_path, target_sequence_length, n_mfcc, n_chroma)\n",
    "\n",
    "    if processed_features is not None:\n",
    "        # Add a batch dimension\n",
    "        processed_features = processed_features[np.newaxis, :, :] # Shape (1, target_sequence_length, total_feature_dimension)\n",
    "        print(f\"Processed user audio features shape for conditioning: {processed_features.shape}\")\n",
    "        return processed_features\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Updated function to generate lyrics using a language model ---\n",
    "# Initialize the text generation pipeline\n",
    "# Using a smaller model for faster execution, e.g., 'gpt2' or 'distilgpt2'\n",
    "try:\n",
    "    lyric_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "    print(\"Text generation pipeline (distilgpt2) initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing text generation pipeline: {e}\")\n",
    "    lyric_generator = None # Set to None if initialization fails\n",
    "\n",
    "\n",
    "def generate_lyrics_with_language_model(prompt):\n",
    "    \"\"\"Generates lyrics based on a prompt using a language model.\"\"\"\n",
    "    if lyric_generator:\n",
    "        print(f\"Generating lyrics for prompt: '{prompt}' using DistilGPT-2.\")\n",
    "        try:\n",
    "            # Generate text with a reasonable length and without generating too many sequences\n",
    "            generated_text = lyric_generator(prompt, max_length=100, num_return_sequences=1, truncation=True)\n",
    "            lyrics = generated_text[0]['generated_text']\n",
    "            # Basic post-processing: remove the prompt from the generated text if it's repeated\n",
    "            if lyrics.startswith(prompt):\n",
    "                lyrics = lyrics[len(prompt):].strip()\n",
    "            print(\"Lyric generation successful.\")\n",
    "            return lyrics\n",
    "        except Exception as e:\n",
    "            print(f\"Error during lyric generation: {e}\")\n",
    "            return f\"Error generating lyrics for prompt: '{prompt}'\" # Return an error message\n",
    "    else:\n",
    "        print(\"Lyric generator not initialized. Cannot generate lyrics.\")\n",
    "        return f\"Cannot generate lyrics for prompt: '{prompt}' (generator not available)\"\n",
    "\n",
    "\n",
    "# Function to get text embeddings\n",
    "# We'll use a placeholder for a text embedding model\n",
    "def get_text_embedding(text, embedding_dim):\n",
    "    \"\"\"Placeholder function to get a fixed-size embedding for text.\"\"\"\n",
    "    print(f\"Getting embedding for text: '{text[:50]}...' using a placeholder model.\")\n",
    "    return np.random.rand(1, embedding_dim) # Shape (1, embedding_dim)\n",
    "\n",
    "def process_text_input(user_input, embedding_dim, is_prompt=False):\n",
    "    \"\"\"Processes user text input (lyrics or prompt) and returns a conditioning embedding.\"\"\"\n",
    "    if is_prompt:\n",
    "        lyrics = generate_lyrics_with_language_model(user_input)\n",
    "    else:\n",
    "        lyrics = user_input # User provided lyrics directly\n",
    "\n",
    "    conditioning_embedding = get_text_embedding(lyrics, embedding_dim)\n",
    "\n",
    "    return conditioning_embedding, lyrics\n",
    "\n",
    "\n",
    "def generate_conditional_mfccs(model, start_sequence, conditioning_input, generation_steps):\n",
    "    \"\"\"Generates new MFCC sequences using the trained conditional model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras conditional generation model.\n",
    "        start_sequence: A numpy array representing the initial musical sequence\n",
    "                        (shape: (1, sequence_length, musical_feature_dim)).\n",
    "        conditioning_input: The conditioning input (text embedding or audio features).\n",
    "                            Shape varies based on input_type.\n",
    "        generation_steps: The number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated musical MFCC sequence.\n",
    "    \"\"\"\n",
    "    generated_sequence = np.copy(start_sequence)\n",
    "    current_musical_sequence = np.copy(start_sequence)\n",
    "    sequence_length = start_sequence.shape[1] # Get the expected sequence length\n",
    "\n",
    "    # Assuming the model takes a list of inputs: [musical_input, conditioning_input]\n",
    "    # The conditioning_input's shape depends on whether it's text or audio\n",
    "    # If text, it's (1, conditioning_feature_dim)\n",
    "    # If audio, it's (1, sequence_length, total_audio_features_dim)\n",
    "\n",
    "    # Determine the input structure for the model based on the shape of conditioning_input\n",
    "    if conditioning_input.shape[-1] == conditioning_feature_dim and conditioning_input.ndim == 2:\n",
    "        # Text conditioning (single vector)\n",
    "        model_inputs = [current_musical_sequence, conditioning_input]\n",
    "        print(\"Using text conditioning input shape:\", conditioning_input.shape)\n",
    "    elif conditioning_input.shape[-1] == total_audio_features_dim and conditioning_input.ndim == 3:\n",
    "         # Audio conditioning (sequence)\n",
    "         # For sequential conditioning, we might need a different model architecture\n",
    "         # or a way to combine the audio features sequence with the musical sequence over time.\n",
    "         # Given the current model expects a single conditioning vector repeated,\n",
    "         # this approach with sequential audio conditioning will require model modification.\n",
    "         # For now, let's adapt the generation to work with the current model by\n",
    "         # averaging the audio features if sequential audio conditioning is provided,\n",
    "         # or modifying the model to handle sequential conditioning properly.\n",
    "\n",
    "         # Since the model architecture is defined to take a single vector conditioning input,\n",
    "         # we need to convert the sequential audio features to a single vector.\n",
    "         # Averaging over time and projecting is a possible approach, but might lose temporal info.\n",
    "         print(\"Warning: Model architecture is designed for single vector conditioning. Averaging audio features.\")\n",
    "         averaged_audio_features = np.mean(conditioning_input, axis=1) # Shape (1, total_audio_features_dim)\n",
    "\n",
    "         # Need a projection layer if total_audio_features_dim != conditioning_feature_dim\n",
    "         if total_audio_features_dim != conditioning_feature_dim:\n",
    "             print(\"Warning: Projecting averaged audio features to match conditioning_feature_dim.\")\n",
    "             # Define a simple Dense layer for this purpose\n",
    "             audio_projection_layer = Dense(conditioning_feature_dim, activation='relu')\n",
    "             # Need to build this layer if it's the first time it's called\n",
    "             if not audio_projection_layer.built:\n",
    "                  audio_projection_layer.build(input_shape=(None, total_audio_features_dim))\n",
    "             conditioning_input_vector = audio_projection_layer(averaged_audio_features)\n",
    "         else:\n",
    "             conditioning_input_vector = averaged_audio_features # Use directly if dimensions match\n",
    "\n",
    "         model_inputs = [current_musical_sequence, conditioning_input_vector]\n",
    "         print(\"Using averaged and projected audio conditioning input shape:\", conditioning_input_vector.shape)\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Unexpected conditioning input shape.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    for _ in range(generation_steps):\n",
    "        # Predict the next time step of musical features\n",
    "        predicted_mfcc = model.predict(model_inputs) # Model expects [musical_input, conditioning_input]\n",
    "\n",
    "\n",
    "        # Append the predicted MFCC to the generated sequence (only the new time step)\n",
    "        generated_sequence = np.append(generated_sequence, predicted_mfcc[:, -1:, :], axis=1)\n",
    "\n",
    "        # Update the current musical sequence by removing the oldest time step and adding the new one\n",
    "        # Ensure the sequence length remains constant\n",
    "        current_musical_sequence = np.append(current_musical_sequence[:, 1:, :], predicted_mfcc[:, -1:, :], axis=1)\n",
    "\n",
    "        # If using sequential audio conditioning, need to update the conditioning input for the next step as well\n",
    "        # This would require a different model architecture that processes both sequences.\n",
    "        # With the current model expecting a single conditioning vector, this update is not applicable\n",
    "        # and the single conditioning vector remains constant throughout generation.\n",
    "\n",
    "\n",
    "    return generated_sequence.squeeze() # Remove the batch dimension\n",
    "\n",
    "\n",
    "def generate_music_from_input(model, user_input, input_type, generation_steps=200, sr=22050, max_padding=174, n_mfcc=20, n_chroma=12, conditioning_feature_dim=128):\n",
    "    \"\"\"Generates music based on user input (text or audio).\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras conditional music generation model.\n",
    "        user_input: The user's input (string for text, file path for audio).\n",
    "        input_type: Type of input ('text_lyrics', 'text_prompt', 'audio').\n",
    "        generation_steps: Number of additional time steps to generate.\n",
    "        sr: Sampling rate for audio output.\n",
    "        max_padding: Target sequence length for padding/truncating features.\n",
    "        n_mfcc: Number of MFCCs extracted.\n",
    "        n_chroma: Number of chroma bins extracted.\n",
    "        conditioning_feature_dim: Dimension of the conditioning feature vector (for text).\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        The path to the generated audio file (MP3), or None if an error occurred.\n",
    "    \"\"\"\n",
    "    conditioning_input_for_model = None\n",
    "    processed_lyrics = None # To store generated lyrics if applicable\n",
    "\n",
    "    if input_type == 'text_lyrics' or input_type == 'text_prompt':\n",
    "        is_prompt = (input_type == 'text_prompt')\n",
    "        conditioning_input_for_model, processed_lyrics = process_text_input(user_input, conditioning_feature_dim, is_prompt=is_prompt)\n",
    "        if conditioning_input_for_model is None:\n",
    "            print(\"Error processing text input for conditioning.\")\n",
    "            return None\n",
    "        # For text conditioning, repeat the single vector for each step if the model architecture requires it\n",
    "        # Our current model concatenates the repeated vector in the model definition, so we pass the single vector.\n",
    "\n",
    "    elif input_type == 'audio':\n",
    "        print(\"Processing user audio input for conditioning.\")\n",
    "        # Process user audio to get features\n",
    "        user_audio_features_sequence = process_user_audio_for_conditioning(user_input, max_padding, n_mfcc, n_chroma)\n",
    "        if user_audio_features_sequence is None:\n",
    "            print(\"Error processing user audio input.\")\n",
    "            return None\n",
    "\n",
    "        # For the current model architecture that expects a single conditioning vector,\n",
    "        # we need to convert the sequential audio features to a single vector.\n",
    "        # A simple approach is to average over time and project.\n",
    "        averaged_audio_features = np.mean(user_audio_features_sequence, axis=1) # Shape (1, total_audio_features_dim)\n",
    "\n",
    "        # Need a projection layer if total_audio_features_dim != conditioning_feature_dim\n",
    "        # Define a simple Dense layer for this purpose\n",
    "        total_audio_features = n_mfcc + n_chroma # Update total features dimension\n",
    "        audio_projection_layer = Dense(conditioning_feature_dim, activation='relu')\n",
    "        # Need to build this layer if it's the first time it's called\n",
    "        if not audio_projection_layer.built:\n",
    "             audio_projection_layer.build(input_shape=(None, total_audio_features))\n",
    "\n",
    "        conditioning_input_for_model = audio_projection_layer(averaged_audio_features)\n",
    "        print(f\"Projected audio conditioning vector shape: {conditioning_input_for_model.shape}\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid input type. Please use 'text_lyrics', 'text_prompt', or 'audio'.\")\n",
    "        return None\n",
    "\n",
    "    # Ensure conditioning_input_for_model is not None before proceeding\n",
    "    if conditioning_input_for_model is None:\n",
    "         print(\"Conditioning input for model is None. Cannot proceed.\")\n",
    "         return None\n",
    "\n",
    "    # Generate a starting musical sequence (e.g., take a random sample from the training data)\n",
    "    if 'X_train_music' not in globals() or X_train_music.shape[0] == 0:\n",
    "         print(\"Training data (X_train_music) is not available or empty. Cannot generate a starting sequence.\")\n",
    "         return None\n",
    "\n",
    "\n",
    "    start_index = np.random.randint(0, X_train_music.shape[0])\n",
    "    # The starting sequence should have the same feature dimension as the model's musical input\n",
    "    # which is just MFCCs based on the model definition\n",
    "    start_sequence = X_train_music[start_index:start_index+1, :, :]\n",
    "\n",
    "\n",
    "    # Generate new musical features using the trained conditional GRU model\n",
    "    print(\"\\nGenerating new conditional musical features...\")\n",
    "    # The model expects musical_input (MFCCs) and conditioning_input\n",
    "    # The conditioning_input is now a single vector (1, conditioning_feature_dim)\n",
    "    generated_musical_features = generate_conditional_mfccs(model, start_sequence, conditioning_input_for_model, generation_steps)\n",
    "\n",
    "\n",
    "    print(f\"Shape of the generated musical features: {generated_musical_features.shape}\")\n",
    "\n",
    "    # Convert the generated musical features (MFCCs) back to an audio time-series\n",
    "    print(\"\\nConverting generated musical features to audio...\")\n",
    "    # Ensure the generated features are only MFCCs if the inverse transform expects only MFCCs\n",
    "    # If the generation model outputs combined features, we would need to separate or use a different inverse transform\n",
    "    # Assuming the model is trained to output only MFCCs for the musical output layer\n",
    "    audio_time_series_conditional = librosa.feature.inverse.mfcc_to_audio(generated_musical_features.T, sr=sr)\n",
    "\n",
    "\n",
    "    # Define the output filename (WAV and MP3)\n",
    "    output_filename_wav = 'generated_music_conditional.wav'\n",
    "    output_filename_mp3 = 'generated_music_conditional.mp3'\n",
    "\n",
    "\n",
    "    # Save the generated audio to a WAV file first\n",
    "    print(f\"Saving generated conditional audio to {output_filename_wav}...\")\n",
    "    sf.write(output_filename_wav, audio_time_series_conditional, sr)\n",
    "    print(f\"Generated conditional audio saved successfully to {output_filename_wav}\")\n",
    "\n",
    "\n",
    "    # Convert the WAV file to MP3\n",
    "    print(f\"Converting {output_filename_wav} to {output_filename_mp3}...\")\n",
    "    try:\n",
    "        audio = AudioSegment.from_wav(output_filename_wav)\n",
    "        audio.export(output_filename_mp3, format=\"mp3\")\n",
    "        print(f\"Conversion to MP3 successful. Output saved to {output_filename_mp3}\")\n",
    "        # Optionally, remove the intermediate WAV file\n",
    "        # os.remove(output_filename_wav)\n",
    "        # print(f\"Removed intermediate WAV file: {output_filename_wav}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MP3 conversion: {e}\")\n",
    "        return None # Return None if MP3 conversion fails\n",
    "\n",
    "\n",
    "    # Optionally, save the generated lyrics to a text file if lyrics were generated\n",
    "    if processed_lyrics and input_type == 'text_prompt':\n",
    "        lyrics_output_filename = 'generated_lyrics.txt'\n",
    "        with open(lyrics_output_filename, 'w') as f:\n",
    "            f.write(processed_lyrics)\n",
    "        print(f\"Generated lyrics saved to {lyrics_output_filename}\")\n",
    "\n",
    "\n",
    "    return output_filename_mp3 # Return the path to the MP3 file\n",
    "\n",
    "# --- Re-run feature extraction, model definition, and training to ensure variables are defined ---\n",
    "# Define the paths to the extracted datasets (using the recheck directories)\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck' # This directory might be empty\n",
    "\n",
    "bird_sound_files = []\n",
    "music_instrument_files = []\n",
    "\n",
    "# Identify bird sound files from the second dataset (assuming 'Voice of Birds' contains bird sounds)\n",
    "print(f\"Searching for bird sound files in: {extracted_path_two}\") # Debug print\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        if 'Voice of Birds' in root and name.endswith('.mp3'):\n",
    "            bird_sound_files.append(os.path.join(root, name))\n",
    "print(f\"Found {len(bird_sound_files)} bird sound files.\") # Debug print\n",
    "\n",
    "\n",
    "# Identify music instrument files from the first dataset (assuming Test_submission and Train_submission contain music instruments)\n",
    "print(f\"Searching for music instrument files in: {extracted_path_one}\") # Debug print\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        if ('Test_submission' in root or 'Train_submission' in root) and (name.endswith('.wav') or name.endswith('.mp3')):\n",
    "             # Further check if the file name indicates a music instrument\n",
    "            if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords']):\n",
    "                 music_instrument_files.append(os.path.join(root, name))\n",
    "print(f\"Found {len(music_instrument_files)} music instrument files.\") # Debug print\n",
    "\n",
    "\n",
    "# Use only music instrument files for feature extraction for the music generation model\n",
    "files_to_process_music = music_instrument_files\n",
    "\n",
    "features_music = []\n",
    "\n",
    "print(\"Extracting MFCC features from music instrument files for training music generation model...\")\n",
    "def extract_mfcc_features_only(file_path, target_sequence_length, n_mfcc=20):\n",
    "    \"\"\"Extract only MFCC features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Pad or truncate MFCCs to a fixed length\n",
    "        if mfccs.shape[1] < target_sequence_length:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, target_sequence_length - mfccs.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :target_sequence_length]\n",
    "\n",
    "        return mfccs.T # Transpose to have shape (time steps, n_mfcc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "for i, file_path in enumerate(files_to_process_music):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processing music instrument file {i + 1}/{len(files_to_process_music)}\")\n",
    "    mfccs = extract_mfcc_features_only(file_path, max_padding, n_mfcc=n_mfcc)\n",
    "    if mfccs is not None:\n",
    "        features_music.append(mfccs)\n",
    "\n",
    "\n",
    "# Convert music features to numpy array\n",
    "features_music = np.array(features_music)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "# Add a check to ensure features_music is not empty before splitting\n",
    "if len(features_music) > 0:\n",
    "    X_train_music, X_val_music = train_test_split(features_music, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    print(\"No music features extracted. Cannot proceed with model training and generation.\")\n",
    "    X_train_music = np.array([]) # Initialize as empty array to prevent errors\n",
    "    X_val_music = np.array([]) # Initialize as empty array to prevent errors\n",
    "\n",
    "\n",
    "print(\"\\nMusic feature extraction complete.\")\n",
    "print(f\"Total music samples: {len(features_music)}\")\n",
    "print(f\"Shape of music features array: {features_music.shape}\")\n",
    "print(f\"Shape of training music features: {X_train_music.shape}\")\n",
    "print(f\"Shape of validation music features: {X_val_music.shape}\")\n",
    "\n",
    "# Define the dimensions for the music generation model\n",
    "# Add a check to ensure X_train_music is not empty before accessing shape\n",
    "if X_train_music.shape[0] > 0:\n",
    "    musical_feature_dim = X_train_music.shape[2] # Number of MFCCs for musical output\n",
    "    musical_sequence_length = X_train_music.shape[1] # Length of the MFCC sequences\n",
    "\n",
    "    # Define the conditioning input dimensions\n",
    "    # For this model architecture, the conditioning input is a single vector.\n",
    "    # It will be either a text embedding or an averaged+projected audio feature vector.\n",
    "    conditioning_feature_dim = 128 # Dimension of the conditioning feature vector\n",
    "\n",
    "    # Define the input layers for the conditional music generation model\n",
    "    # Input 1: Musical features (MFCCs) - the sequence we are building upon\n",
    "    musical_input = Input(shape=(musical_sequence_length, musical_feature_dim), name='musical_input')\n",
    "    # Input 2: Conditioning input (single vector) - text embedding or processed audio feature vector\n",
    "    conditioning_input = Input(shape=(conditioning_feature_dim,), name='conditioning_input')\n",
    "\n",
    "\n",
    "    # Repeat the conditioning input across the musical sequence length\n",
    "    repeated_conditioning = tf.keras.layers.RepeatVector(musical_sequence_length)(conditioning_input)\n",
    "\n",
    "    # Concatenate the musical input with the repeated conditioning input\n",
    "    combined_input = Concatenate(axis=-1)([musical_input, repeated_conditioning])\n",
    "\n",
    "    # GRU layers for the sequence generation\n",
    "    gru_1 = GRU(128, activation='relu', return_sequences=True)(combined_input)\n",
    "    dropout_1 = Dropout(0.2)(gru_1)\n",
    "\n",
    "    gru_2 = GRU(128, activation='relu', return_sequences=True)(dropout_1)\n",
    "    dropout_2 = Dropout(0.2)(gru_2)\n",
    "\n",
    "    # TimeDistributed Dense layer to output the generated musical features (MFCCs) for each time step\n",
    "    output_layer = TimeDistributed(Dense(musical_feature_dim, activation='linear'))(dropout_2)\n",
    "\n",
    "    # Define the conditioned music generation model\n",
    "    model_gen_conditioned = Model(inputs=[musical_input, conditioning_input], outputs=output_layer)\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam(clipvalue=1.0) # Add gradient clipping\n",
    "    model_gen_conditioned.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "    # Print the model summary\n",
    "    model_gen_conditioned.summary()\n",
    "\n",
    "\n",
    "    # Create dummy conditioning data for training the conditioned music generation model\n",
    "    # The dummy conditioning data needs to match the expected input shape for conditioning_input\n",
    "    dummy_conditioning_train = np.random.rand(X_train_music.shape[0], conditioning_feature_dim)\n",
    "    dummy_conditioning_val = np.random.rand(X_val_music.shape[0], conditioning_feature_dim)\n",
    "\n",
    "    # Train the conditioned GRU generation model\n",
    "    print(\"\\nTraining the conditioned GRU music generation model with dummy input...\")\n",
    "    history_gen_conditioned = model_gen_conditioned.fit(\n",
    "        [X_train_music, dummy_conditioning_train],\n",
    "        X_train_music, # The target output is the same musical sequence\n",
    "        epochs=50, # Increased epochs as training with dummy data is quick\n",
    "        batch_size=32,\n",
    "        validation_data=([X_val_music, dummy_conditioning_val], X_val_music) # Use validation data\n",
    "    )\n",
    "\n",
    "    print(\"\\nTraining complete.\")\n",
    "\n",
    "    # --- Demonstrate Music Generation from User Audio (using dummy audio file) ---\n",
    "    # Note: Replace 'path/to/your/audio.wav' with the actual path to a user-recorded audio file\n",
    "    # We need to provide a dummy audio file path for demonstration if a real one isn't available.\n",
    "    # For demonstration, let's create a dummy audio file if it doesn't exist.\n",
    "    dummy_audio_path = '/content/dummy_user_audio.wav'\n",
    "    if not os.path.exists(dummy_audio_path):\n",
    "        print(f\"Creating a dummy audio file at {dummy_audio_path} for demonstration.\")\n",
    "        # Create a short silent WAV file as a placeholder\n",
    "        dummy_audio_ts = np.zeros(sr * 5) # 5 seconds of silence\n",
    "        sf.write(dummy_audio_path, dummy_audio_ts, sr)\n",
    "\n",
    "\n",
    "    user_audio_input_path = dummy_audio_path # Use the dummy audio file for demonstration\n",
    "    print(f\"\\nAttempting to generate music from audio file: {user_audio_input_path}\")\n",
    "\n",
    "    if os.path.exists(user_audio_input_path):\n",
    "         # Use the updated generate_music_from_input function\n",
    "         generated_file_audio_mp3 = generate_music_from_input(model_gen_conditioned, user_audio_input_path, 'audio',\n",
    "                                                      generation_steps=200, sr=sr, max_padding=max_padding, n_mfcc=n_mfcc, n_chroma=n_chroma, conditioning_feature_dim=conditioning_feature_dim)\n",
    "         if generated_file_audio_mp3:\n",
    "             print(f\"\\nMusic generated from audio saved to: {generated_file_audio_mp3}\")\n",
    "         else:\n",
    "             print(\"\\nFailed to generate music from audio.\")\n",
    "    else:\n",
    "        print(f\"\\nUser audio file not found at {user_audio_input_path}. Skipping audio-conditioned generation example.\")\n",
    "else:\n",
    "    print(\"Cannot define and train the model as no music features were extracted.\")\n",
    "\n",
    "# --- Demonstrate Lyric Generation and Music Generation from a Text Prompt ---\n",
    "# Assuming model_gen_conditioned, generate_music_from_input,\n",
    "# conditioning_feature_dim, max_padding, n_mfcc, sr are defined.\n",
    "# Also assuming process_text_input and generate_conditional_mfccs are defined.\n",
    "\n",
    "user_lyric_idea_prompt = \"Write a song about a journey through a forest.\"\n",
    "print(f\"\\nAttempting to generate lyrics and music from prompt: '{user_lyric_idea_prompt}'\")\n",
    "\n",
    "# The generate_music_from_input function with input_type='text_prompt'\n",
    "# will call the placeholder lyric generation and then use the resulting lyrics for conditioning.\n",
    "generated_file_from_lyric_idea = generate_music_from_input(model_gen_conditioned, user_lyric_idea_prompt, 'text_prompt',\n",
    "                                                         generation_steps=200, sr=sr, max_padding=max_padding, n_mfcc=n_mfcc, conditioning_feature_dim=conditioning_feature_dim)\n",
    "\n",
    "if generated_file_from_lyric_idea:\n",
    "    print(f\"\\nMusic generated from lyric idea prompt saved to: {generated_file_from_lyric_idea}\")\n",
    "else:\n",
    "    print(\"\\nFailed to generate music from lyric idea prompt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d2c600d",
    "outputId": "c0e9b600-d7b5-495a-d440-f88d3164c835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading archive.zip from https://storage.googleapis.com/kaggle-data-sets/2768513/4784071/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250827%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250827T024359Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=1a1ddbaf1b0e32e9aabd16dd7e2f70fd8fe2ae40dcdf8e65453881aba3d1db0ccc133c315998513fdbb1093f834d23234a8a8ab08924dcc653b3b75d0841cf7e6d013fd68b33d60fee3a355e825bd969480898d9d06d8969b16c475c55f28a0c56c6a537e100a85647b53b8307ddfa00810766c7a248fa3b81c2f6edd3ead8d00da...\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   220  100   220    0     0   1104      0 --:--:-- --:--:-- --:--:--  1105\n",
      "Downloading dataset_second.zip from https://storage.googleapis.com/kaggle-data-sets/2750746/4753077/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250827%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250827T024259Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=6a1bc74b093049c74c92a8d2d572a9222a8e5f225175c3d71bdcf9ddd1371b56382e176ad942bfab0db6351eaec280cbf9cb6f4b3a99cf96d4c73753f05d54958db32fbf765280cc47b280da3700caf22c84590285ec390dd27e57c4ac6152107cd8148ee15da87e2f2a42a7fc059d1ee65825bd969480898d9d06d8969b16c475c55981300d3ec48520fad01323a62a45c82c8b193f6e11c83bae0c2ca63174f142f1b51ed209243c7dec994ed256ef1ca71868bf08f6bbcb5e71241ff9e694a599ddc3d8ceb1cad9ef99ae0614b0fe01cec125d880870532b54549238fbb69f0f36edaddf7799bdea379965474d31f74306d81d4abaa3499d1fee2c29125c5...\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   220  100   220    0     0   1104      0 --:--:-- --:--:-- --:--:--  1105\n",
      "\n",
      "Attempting to extract 'archive.zip' to '/content/dataset_one_extracted_recheck'\n",
      "Error: 'archive.zip' is a bad zip file.\n",
      "\n",
      "Attempting to extract 'dataset_second.zip' to '/content/dataset_second_extracted_recheck'\n",
      "Error: 'dataset_second.zip' is a bad zip file.\n",
      "\n",
      "Files in '/content/dataset_one_extracted_recheck':\n",
      "\n",
      "Files in '/content/dataset_second_extracted_recheck':\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Define the download URLs and target filenames\n",
    "url_one = \"https://storage.googleapis.com/kaggle-data-sets/2768513/4784071/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250827%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250827T024359Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=1a1ddbaf1b0e32e9aabd16dd7e2f70fd8fe2ae40dcdf8e65453881aba3d1db0ccc133c315998513fdbb1093f834d23234a8a8ab08924dcc653b3b75d0841cf7e6d013fd68b33d60fee3a355e825bd969480898d9d06d8969b16c475c55f28a0c56c6a537e100a85647b53b8307ddfa00810766c7a248fa3b81c2f6edd3ead8d00da\"\n",
    "filename_one = 'archive.zip'\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "\n",
    "url_two = \"https://storage.googleapis.com/kaggle-data-sets/2750746/4753077/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250827%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250827T024259Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=6a1bc74b093049c74c92a8d2d572a9222a8e5f225175c3d71bdcf9ddd1371b56382e176ad942bfab0db6351eaec280cbf9cb6f4b3a99cf96d4c73753f05d54958db32fbf765280cc47b280da3700caf22c84590285ec390dd27e57c4ac6152107cd8148ee15da87e2f2a42a7fc059d1ee65825bd969480898d9d06d8969b16c475c55981300d3ec48520fad01323a62a45c82c8b193f6e11c83bae0c2ca63174f142f1b51ed209243c7dec994ed256ef1ca71868bf08f6bbcb5e71241ff9e694a599ddc3d8ceb1cad9ef99ae0614b0fe01cec125d880870532b54549238fbb69f0f36edaddf7799bdea379965474d31f74306d81d4abaa3499d1fee2c29125c5\"\n",
    "filename_two = 'dataset_second.zip'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck'\n",
    "\n",
    "\n",
    "# Download the first file\n",
    "print(f\"Downloading {filename_one} from {url_one}...\")\n",
    "get_ipython().system(f'curl -L \"{url_one}\" -o {filename_one}')\n",
    "\n",
    "# Download the second file\n",
    "print(f\"Downloading {filename_two} from {url_two}...\")\n",
    "get_ipython().system(f'curl -L \"{url_two}\" -o {filename_two}')\n",
    "\n",
    "# Create extraction directories if they don't exist\n",
    "os.makedirs(extracted_path_one, exist_ok=True)\n",
    "os.makedirs(extracted_path_two, exist_ok=True)\n",
    "\n",
    "# Attempt extraction and list contents\n",
    "print(f\"\\nAttempting to extract '{filename_one}' to '{extracted_path_one}'\")\n",
    "try:\n",
    "    with zipfile.ZipFile(filename_one, 'r') as zip_ref:\n",
    "        print(\"Contents of archive.zip:\")\n",
    "        zip_ref.printdir()\n",
    "        zip_ref.extractall(extracted_path_one)\n",
    "    print(\"Extraction of archive.zip successful.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: '{filename_one}' is a bad zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction of '{filename_one}': {e}\")\n",
    "\n",
    "\n",
    "print(f\"\\nAttempting to extract '{filename_two}' to '{extracted_path_two}'\")\n",
    "try:\n",
    "    with zipfile.ZipFile(filename_two, 'r') as zip_ref:\n",
    "        print(\"Contents of dataset_second.zip:\")\n",
    "        zip_ref.printdir()\n",
    "        zip_ref.extractall(extracted_path_two)\n",
    "    print(\"Extraction of dataset_second.zip successful.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: '{filename_two}' is a bad zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction of '{filename_two}': {e}\")\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_one}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_two}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ab9cfa49",
    "outputId": "a5dd79f4-6a4c-493b-9014-06bcb5d2c6aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100   220  100   220    0     0   1560      0 --:--:-- --:--:-- --:--:--  1571\n"
     ]
    }
   ],
   "source": [
    "!curl -L \"https://storage.googleapis.com/kaggle-data-sets/2768513/4784071/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250827%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250827T024359Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=1a1ddbaf1b0e32e9aabd16dd7e2f70fd8fe2ae40dcdf8e65453881aba3d1db0ccc133c315998513fdbb1093f8625bd969480898d9d06d8969b16c475c55f28a0c56c6a537e100a85647b53b8307ddfa00810766c7a248fa3b81c2f6edd3ead8d00da\" -o archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "863ba492",
    "outputId": "0bbba3e1-e01b-4037-b181-b7408bbe3221"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to extract '/content/archive.zip' to '/content/dataset_one_extracted_recheck'\n",
      "Error: '/content/archive.zip' is a bad zip file.\n",
      "\n",
      "Files in '/content/dataset_one_extracted_recheck':\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "archive_path_one = '/content/archive.zip'\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "\n",
    "os.makedirs(extracted_path_one, exist_ok=True)\n",
    "\n",
    "print(f\"Attempting to extract '{archive_path_one}' to '{extracted_path_one}'\")\n",
    "try:\n",
    "    with zipfile.ZipFile(archive_path_one, 'r') as zip_ref:\n",
    "        print(\"Contents of archive.zip:\")\n",
    "        zip_ref.printdir()\n",
    "        zip_ref.extractall(extracted_path_one)\n",
    "    print(\"Extraction of archive.zip successful.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: '{archive_path_one}' is a bad zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction of '{archive_path_one}': {e}\")\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_one}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ff5ceaa",
    "outputId": "91f0b1b8-0206-4325-dc06-090de06ee48f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   220  100   220    0     0   1301      0 --:--:-- --:--:-- --:--:--  1309\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   220  100   220    0     0    908      0 --:--:-- --:--:-- --:--:--   909\n",
      "Attempting to re-extract '/content/archive.zip' to '/content/dataset_one_extracted_recheck'\n",
      "Error: '/content/archive.zip' is a bad zip file.\n",
      "\n",
      "Attempting to re-extract '/content/dataset_second.zip' to '/content/dataset_second_extracted_recheck'\n",
      "Error: '/content/dataset_second.zip' is a bad zip file.\n",
      "\n",
      "Files in '/content/dataset_one_extracted_recheck':\n",
      "\n",
      "Files in '/content/dataset_second_extracted_recheck':\n"
     ]
    }
   ],
   "source": [
    "!curl -L \"https://storage.googleapis.com/kaggle-data-sets/2768513/4784071/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250827%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250827T024359Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=1a1ddbaf1b0e32e9aabd16dd7e2f70fd8fe2ae40dcdf8e65453881aba3d1db0ccc133c315998513fdbb1093f834d23234a8a8ab08924dcc653b3b75d0841cf7e6d013fd68b33d60fee3a355e88b56f26362ff5f6a57d1f50c75d8a8e64bb28db4ba8918a3ffce070c310fd1c12f0648ce4b3768481ca3faceefbe0ae1d9d5db37123030768f70da56cae3988736fa857ce149fc39f0489b001495f73889e5bbb0fa093f3b0fb5e9854fad6fe9f74115979c4747898315a284f42df649583810b62e7c86b49d2ec2c98c3203ff90f7d2c19201b15b9fb57386197c55f28a0c56c6a537e100a85647b53b8307ddfa00810766c7a248fa3b81c2f6edd3ead8d00da\" -o archive.zip\n",
    "!curl -L \"https://storage.googleapis.com/kaggle-data-sets/2750746/4753077/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250827%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250827T024259Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=6a1bc74b093049c74c92a8d2d572a9222a8e5f225175c3d71bdcf9ddd1371b56382e176ad942bfab0db6351eaec280cbf9cb6f4b3a99cf96d4c73753f05d54958db32fbf765280cc47b280da3700caf22c84590285ec390dd27e57c4ac6152107cd8148ee15da87e2f2a42a7fc059d1ee65825bd969480898d9d06d8969b16c475c55981300d3ec48520fad01323a62a45c82c8b193f6e11c83bae0c2ca63174f142f1b51ed209243c7dec994ed256ef1ca71868bf08f6bbcb5e71241ff9e694a599ddc3d8ceb1cad9ef99ae0614b0fe01cec125d880870532b54549238fbb69f0f36edaddf7799bdea379965474d31f74306d81d4abaa3499d1fee2c29125c5\" -o dataset_second.zip\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "archive_path_one = '/content/archive.zip'\n",
    "archive_path_two = '/content/dataset_second.zip'\n",
    "\n",
    "extracted_path_one = '/content/dataset_one_extracted_recheck'\n",
    "extracted_path_two = '/content/dataset_second_extracted_recheck'\n",
    "\n",
    "os.makedirs(extracted_path_one, exist_ok=True)\n",
    "os.makedirs(extracted_path_two, exist_ok=True)\n",
    "\n",
    "print(f\"Attempting to re-extract '{archive_path_one}' to '{extracted_path_one}'\")\n",
    "try:\n",
    "    with zipfile.ZipFile(archive_path_one, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_path_one)\n",
    "    print(\"Extraction of archive.zip successful.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: '{archive_path_one}' is a bad zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction of '{archive_path_one}': {e}\")\n",
    "\n",
    "\n",
    "print(f\"\\nAttempting to re-extract '{archive_path_two}' to '{extracted_path_two}'\")\n",
    "try:\n",
    "    with zipfile.ZipFile(archive_path_two, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_path_two)\n",
    "    print(\"Extraction of dataset_second.zip successful.\")\n",
    "except zipfile.BadZipFile:\n",
    "    print(f\"Error: '{archive_path_two}' is a bad zip file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during extraction of '{archive_path_two}': {e}\")\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_one}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_one):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))\n",
    "\n",
    "print(f\"\\nFiles in '{extracted_path_two}':\")\n",
    "for root, dirs, files in os.walk(extracted_path_two):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b8aba90",
    "outputId": "5c471a80-1a63-4a43-d718-bf1254e4f256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.12), please consider upgrading to the latest version (0.3.13).\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/soumendraprasad/musical-instruments-sound-dataset?dataset_version_number=3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.40G/5.40G [01:33<00:00, 61.9MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"soumendraprasad/musical-instruments-sound-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00f5a9d4",
    "outputId": "9aae8972-ebef-4285-8be9-10fb526958a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing files in the downloaded dataset directory: /root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Metadata_Test.csv\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Metadata_Train.csv\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-66411-1111-285.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_00MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (224).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (125).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_00SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_bartok_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-C7-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/asian-gong-102397.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ragtime-logo-standard-version-116100.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/lonely-dance-riff3-31323.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-74610-1111-00154.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_01HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/rheyne-rhodes-2-70369.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (34).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (82).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick2_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_pachelbel_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_NH_V.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_beethoven1_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_mozart_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-60405-1111-279.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (55).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick5_KN_Lage2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-F6-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (164).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick2_KN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/17_emil-telmanyi_bwv1005.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (21).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (29).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (22).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-73609-1111-00153.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (19).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_swing_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (23).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/bass-loops-006-with-drums-long-loop-120-bpm-6111.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jazz_5_80BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick1_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (12).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (10).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick12_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (86).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (103).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (238).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (290).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (52).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-D6-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_11SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (25).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-64600-1111-300.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/orchestral-drums-28182.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (184).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-53303-1111-186.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_mozart_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-74610-1111-232.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (132).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick7_FN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/066166_qui-c39est-qu39est-tombe-loop-t85wav-39366.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (140).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (11).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_joplin_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/21_emil-telmanyi_bwv1005.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/cold-wednesday-73830.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_swing_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (73).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/reggae_4_120BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_bartok_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-50110-1111-245.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/059960_sonido2-sincopa-alta-7mp3-47316.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_10KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (263).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-E6-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick4_MBSH.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (171).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/17_emil-telmanyi_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (109).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_chords_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_E1_muted5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick3_MN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-65506-1111-215.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_beethoven1_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_G_muted5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/freedom-in-sight-130bpm-32759.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (251).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-chromatic-scale-c1-to-b1-102315.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (163).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (28).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (123).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/latin_7_150BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_pop_2_120BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (112).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (130).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-59309-1111-192.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/dramatic-piano-10950.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_john-garner_bwv1002_mov7.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_joplin_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (8).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (7).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (51).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_classical_1_60BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-E5-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (2).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_07KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (120).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_joplin_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (82).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (48).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (242).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/grand-piano-thing-74701.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (143).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (99).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_scale_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_john-garner_bwv1002_mov6.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_scale_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_03MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_john-garner_bwv1002_mov3.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/170-beat-box-hpf-loop-103412.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (228).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/dnb-bass2-106724.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/lovely-but-damaged-110583.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/11_emil-telmanyi_bwv1002.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (195).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick9_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_joplin_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/dnb-bassline-170-bpm-85155.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/15_ko-donghwi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-73609-1111-231.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_beethoven1_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (55).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (25).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (106).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_chords_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-A5-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (54).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-56401-1111-00119.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_emil-telmanyi_bwv1004.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-58403-1111-277.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick7_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (72).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_mozart_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-50205-1111-253.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_bartok_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_NH_V.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_02MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_joplin_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (23).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (91).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (317).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_beethoven1_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (23).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_minji-kim_bwv1003_mov3.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (291).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (266).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_john-garner_bwv1002_mov6.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick4_MBSH.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (42).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (158).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/12_ray-chen_bwv1004_mov5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_B_muted5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (74).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (65).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_bartok_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick2_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (98).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (5).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick6_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-B4-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_chords_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (52).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/eilegeiya-111391.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (49).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_chords_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick5_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_chords_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/latin_1_200BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick3_FSBDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_pachelbel_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_joplin_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (91).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_pachelbel_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (89).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-45105-1111-162.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-B6-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (31).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-55305-1111-266.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (303).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-69605-1111-305.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-harp-fantasy-43032.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_NH_VII.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_07MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-64600-1111-00144.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (29).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (86).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_silei-li_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick1_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-64409-1111-00127.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_beethoven2_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (57).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_E1_muted5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_classical_3_50BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-71607-1111-229.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/classical_7_60BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-71512-1111-00143.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_beethoven1_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/pop_4_120BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick6_MN_Lage2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_country_4_80BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/12_emil-telmanyi_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/13_emil-telmanyi_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (276).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-49109-1111-00088.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (54).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_swing_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (17).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick7_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_mozart_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-E5-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/rock_8_120BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_scale_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/18_emil-telmanyi_bwv1002.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (250).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jazz_1_160BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (11).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-49204-1111-252.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/056514_drum-40118.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (27).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (71).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (239).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/14_emil-telmanyi_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_chords_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (57).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/16_emil-telmanyi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-48203-1111-251.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_classical_2_50BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (40).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (24).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-52302-1111-00107.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_scale_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (218).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_pachelbel_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (80).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/80s-drums-fl-studio-70248.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_beethoven2_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (79).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (45).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_pachelbel_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (37).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick6_FHBV.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (326).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick6_FHBV.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-67412-1111-208.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (54).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (3).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_emil-telmanyi_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (34).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_minji-kim_bwv1003_mov4.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (38).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (45).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (76).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (126).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (44).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (88).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-G5-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/9_emil-telmanyi_bwv1005.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-C6-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (1).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-43103-1111-160.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (124).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (59).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2018-11-15-22563.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-D6-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/12_emil-telmanyi_bwv1002.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (37).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_beethoven1_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (62).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-54304-1111-187.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (115).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_chords_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/minimal-beat-81778.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_emil-telmanyi_bwv1005.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (96).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_beethoven2_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-G6-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (64).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/beat-of-time-128705.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-48203-1111-173.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/068320_hang-drum-2wav-80568.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (264).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_ray-chen_bwv1004_mov5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (275).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_scale_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (22).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_G_muted5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick8_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-51111-1111-00090.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/country_5_80BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-70511-1111-220.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (18).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_latin_1_160BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_pachelbel_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (15).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (110).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (226).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (135).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/arpeggio-01-36024.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (4).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (131).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-B5-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick4_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_scale_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-59404-1111-00122.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (127).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/folk_3_180BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick1_FVSDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick5_FBVDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (61).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/old-record-player-effect-14887.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (177).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (106).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/latin_2_120BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/19_emil-telmanyi_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-53303-1111-264.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick3_KSBDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (131).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (107).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_swing_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_john-garner_bwv1002_mov3.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (101).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-D5-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-40100-1111-157.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_beethoven1_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_beethoven1_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/9_emil-telmanyi_bwv1002.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-56401-1111-197.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/hectic-808-drum-loop-32989.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (96).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (34).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/rock_7_120BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (309).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (75).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_08KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick10_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_beethoven2_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_10SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick4_FBSH.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (42).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/heavy-thump-drum-line-89744.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/kokaji-beatbox-32048.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-52302-1111-185.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (100).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/065339_metal-bass-drum-90850.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_swing_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_emil-telmanyi_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_G_V_bending.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/13_emil-telmanyi_bwv1005.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (69).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (57).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (25).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_joplin_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-C7-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick6_MHBV.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_emil-telmanyi_bwv1005.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_swing_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/11_emil-telmanyi_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (45).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick6_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_beethoven2_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (98).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_G_fret_0-20.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (26).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (11).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_scale_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_scale_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (249).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick3_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-55400-1111-00118.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (19).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_D_fret_0-20.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (102).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_scale_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (48).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (52).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (76).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/20_emil-telmanyi_bwv1005.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (322).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/17_emil-telmanyi_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_G_fret_0-20.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (323).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (328).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (37).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (79).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_beethoven1_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_swing_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_pachelbel_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-64505-1111-292.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_latin_3_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (128).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/15_emil-telmanyi_bwv1002.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/converted_to_wav_file.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick1_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_ska_2_150BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_02SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick2_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/8_ray-chen_bwv1004_mov5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (42).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-67508-1111-00139.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_ska_4_150BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (55).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-42102-1111-00081.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_swing_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/pop_7_70BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick8_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_scale_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (300).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (272).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-6076.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (36).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_bartok_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jazz_3_140BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_mozart_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_pachelbel_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_joplin_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (25).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (13).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_scale_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_mozart_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_G_V_slide.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (59).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/grinder-drum-loop-6697.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_metal_4_130BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-D5-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_swing_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_beethoven1_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_chords_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-resonance-23-42081.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_NH_XII.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (2).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/keys-of-relaxation-21135.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (96).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (38).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_02KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/misterio1-128347.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick5_KBVDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/8_emil-telmanyi_bwv1002.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_scale_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (87).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (283).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-C6-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_joplin_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick12_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (84).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-72608-1111-00152.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2-E1-Minor 04.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (287).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/rock_5_120BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (93).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (7).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_G_V_bending.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_joplin.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-61502-1111-211.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (114).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-F5-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-57402-1111-00120.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/country_3_120BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/distorted-kick-1-98539.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/latin_5_140BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (70).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick3_FSBDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (6).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-68509-1111-00140.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (221).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (126).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (105).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-76612-1111-312.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (120).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick6_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_06KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_beethoven2_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (259).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (192).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-67412-1111-286.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (155).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (121).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (148).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (35).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (310).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (30).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_swing_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick6_MHBV.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (150).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-57212-1111-182.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/pop_2_140BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (51).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_pachelbel_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-62407-1111-203.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_joplin_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (133).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (85).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (86).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (58).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_minji-kim_bwv1003_mov2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (22).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/reggae_3_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_beethoven1_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/drum-solo-150-bpm-by-prettysleepy-art-12975.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-A6-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_swing_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick6_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/12_emil-telmanyi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-B5-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/10_drumloop_minimal-32725.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (41).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_beethoven1_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-C5-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (113).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/metal_2_180BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick1_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (12).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_scale_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick4_FBSH.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/classical_6_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (35).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/metal_8_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_latin_5_140BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_07MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (20).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/04-47367.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (40).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_NH_IX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick8_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick7_MN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_07KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (208).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_NH_XII.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (126).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_pachelbel_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_swing_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (28).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (21).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_emil-telmanyi_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_scale_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_beethoven1_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (32).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_11HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick2_KVSH.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_chords_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (132).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_swing_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_bartok_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_02SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/140-bpm-amen-break-original-processed-6945.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_12KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_chords_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jaws-of-life-27722.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (48).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (37).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-53208-1111-178.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (46).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_mozart_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_joplin_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-groove-quartal-38660.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (24).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_joplin_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_scale_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_pachelbel_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-F5-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-D7-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_oliver-colbentson_bwv1006_mov4.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/halloween-piano-sample-69075.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/classical_1_80BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jazz_3_120BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_metal_3_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (141).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (63).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_beethoven2_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-metal-2-43037.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_beethoven2_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jazz_7_180BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-A5-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_09MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (193).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-C7-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (77).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ska_4_150BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/catchy-electronic-drum-solo-100-bpm-by-prettysleepy-108532.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_beethoven2_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (14).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_classical_5_70BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (313).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_05MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (153).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/drum-loop-140bpm-73728 (1).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/14_emil-telmanyi_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick2_KVSH.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-B5-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (26).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-66507-1111-294.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-B5-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (26).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/big-cinematic-impact-94799.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_13SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (68).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_oliver-colbentson_bwv1006_mov4.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (44).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/10_ko-donghwi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (63).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-64600-1111-222.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (55).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_swing_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_joplin_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/17_emil-telmanyi_bwv1002.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_beethoven1_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_silei-li_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_mozart_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/saloon-piano-honky-tonk-14540.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-51301-1111-184.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-normal-d4wav-14838.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/16_ko-donghwi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_joplin_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_bartok_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jazz_7_140BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_pachelbel_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (67).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/deep-trap-beat-66414.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_00HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick11_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_05HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_beethoven1_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick12_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (20).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (59).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick9_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_beethoven1_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_scale_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/darkbeat76bpm-87710.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-B4-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_bartok_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (35).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_swing_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_beethoven2_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-64505-1111-214.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (12).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (33).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_swing_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_bartok_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/pop_5_78BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick1_MVSDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_emil-telmanyi_bwv1002.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-A5-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_chords_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_D_fret_0-20.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_DN_III_VIII_XIV.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/emotional-piano-005-am-80-97777.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_scale_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick9_MN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (81).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/10_karen-gomyo_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (302).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_E1_fret_0-20.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/hip-hop-drum-loop-23-75663.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-47202-1111-172.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick3_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_country_5_80BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_07HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-59500-1111-00131.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (247).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-53208-1111-00100.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/10_silei-li_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_jazz_8_110BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_A_muted5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_kinga-augustyn_bwv1005_mov2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (47).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_joplin.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/latin_6_140BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (61).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (36).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-46106-1111-241.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (110).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/c-aeol-12984.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_chords_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_swing_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (312).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-64409-1111-205.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/16_emil-telmanyi_bwv1002.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (74).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_scale_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jazz_4_88BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_silei-li_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_bartok_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/lonely-piano-clip-32779.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-73609-1111-309.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_scale_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/pop_1_130BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (42).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/12_ko-donghwi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (243).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (67).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/14_emil-telmanyi_bwv1005.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/horror-chase-music-loop-67634.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (95).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_oliver-colbentson_bwv1006_mov5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (14).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-logo-108963.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (3).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_joplin_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_scale_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-75611-1111-311.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (311).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_beethoven2_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick9_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_beethoven1_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/18_emil-telmanyi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (77).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (34).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_metal_7_75BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (17).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/another-sadmosphere-108461.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick5_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/metal_3_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_V_vibrato_slow-fast.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (45).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (44).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (47).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-67603-1111-00147.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (96).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_joplin_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_pachelbel_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_10HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_03HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick2_FVSH.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-D4-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-62312-1111-00117.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/11_ray-chen_bwv1004_mov5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (197).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_country_2_110BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (69).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick9_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (53).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (32).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/intro_ezezez-74457.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (139).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-46201-1111-00093.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick9_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/heartbeat-fast-slowdown-31706.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-60501-1111-210.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1-E1-Major 00.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-E5-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_chords_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2-E1-Minor 01.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_bartok_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (22).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/happy-ballerine-piano-keys-26784.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (46).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-A6-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (58).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-arp-72850.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (30).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (123).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-62407-1111-281.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/drum-groove-71987.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick4_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (115).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick2_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_beethoven1_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (20).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_mozart_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (17).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/grave-metronomico-99387.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_scale_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/country_3_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_04SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (176).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (94).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (43).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (23).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (130).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick1_FN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick1_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (75).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (81).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (50).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (60).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (203).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick3_MN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_01HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_mozart_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (15).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick7_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_12HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (36).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/8_emil-telmanyi_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_D_fret_0-20.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (124).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_bartok_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/electronic-drum-loop-by-prettysleepy-art-12918.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (13).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_jazz_1_160BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_bartok_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_joplin_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_bartok_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (233).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/8_emil-telmanyi_bwv1005.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (73).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (123).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-51301-1111-262.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick9_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (65).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_pachelbel_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/classical_6_75BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (66).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick1_MVSDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (103).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (215).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (41).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (34).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/tired-ghosts-piano-65013.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_08HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (161).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (262).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_bartok_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/8_kinga-augustyn_bwv1005_mov2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (116).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_joplin_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-D6-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick7_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_05KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ba-dum-bum-all-74740.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (5).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_beethoven1_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1-E1-Major 06.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/boogie-woogie-logo-116102.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (83).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-47202-1111-250.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-C6-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (83).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick8_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_emil-telmanyi_bwv1002.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_mozart_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (237).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_beethoven1_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (67).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/grunge-bounce-drum-loop-40464.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_scale_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (1).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-B4-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_00MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (315).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_beethoven1_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/flying-over-the-hill-handpan-atmos-19989.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (89).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_folk_2_75BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/country_4_80BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (36).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_mozart_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_chords_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-chord-45626.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-G6-ff-1cR.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (231).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (78).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_NH_IV.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (288).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (112).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (129).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_chords_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/dance-vol-1-7082.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-71512-1111-299.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (178).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/reggae_2_120BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (62).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (95).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (183).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (205).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/classical_4_60BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-E7-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_mozart_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (142).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (49).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-D4-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (31).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (48).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (175).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/classical_2_60BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-62312-1111-273.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (30).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/day-112826.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick9_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (29).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick12_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_mozart_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (9).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-F6-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/046578_tribal-drum-rhythms-03wav-68096.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (102).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-B4-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/crazy-rhythm-loop-120-bpm-002-68061.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (253).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-46106-1111-163.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_john-garner_bwv1002_mov8.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-E5-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-C7-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (127).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick5_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick6_MHBV.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (109).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (40).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_pachelbel_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (306).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_beethoven1_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (72).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jingle-intro-theme-outro-rhodes-piano-15771.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (232).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (120).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-F5-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/16_emil-telmanyi_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (194).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (38).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-51301-1111-00106.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/cool-hiphop-beat-43890.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick7_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_mozart_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_pachelbel_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (49).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (8).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-E6-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (79).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (125).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_swing_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (119).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/on-rd-piano-loop-2-106459.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_bartok_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_bartok_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_NH_IV.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (82).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-59500-1111-209.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick1_KVSDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-55305-1111-188.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (93).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_reggae_4_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_classical_6_75BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (113).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (91).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/emotional-sad-piano-for-an-intro-or-a-documentary-suspense-90-bpm-121528.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/14_karen-gomyo_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_pachelbel.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_emil-telmanyi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/muvibeat8_130bpm-14338.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/happy-loop-6978.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_mozart_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (39).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick3_MSBDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_beethoven2_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-E6-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_00SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/electro-drum-beat-78-bpm-99903.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/orch-006-cymbal-rollwav-14781.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-66411-1111-00129.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-61311-1111-194.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/reggae_1_95BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick8_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-G5-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/battle-preparations-6726.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_ko-donghwi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_A_fret_0-20.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2-E1-Minor 02.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (129).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (165).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/10_emil-telmanyi_bwv1002.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick5_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_chords_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (4).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (289).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_john-garner_bwv1002_mov5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick10_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_oliver-colbentson_bwv1006_mov7.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick11_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_pachelbel_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-61406-1111-00124.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (3).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-G6-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_mozart_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-53303-1111-00108.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jazz_1_180BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_kinga-augustyn_bwv1005_mov2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick10_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick1_FVSDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (74).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_swing_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-71512-1111-221.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-resonance-18-42083.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-E5-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (23).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/22_emil-telmanyi_bwv1005.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_chords_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_scale_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_beethoven1_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/hybrid-drum-groove-41326.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/agressive-straight-drums-105-bpm-99895.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick12_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (4).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-56401-1111-275.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_swing_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (87).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick6_KHBV.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_pachelbel_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ghost-scare-vintage-6062.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick12_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/metal_6_130BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (8).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (107).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ska_4_180BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_classical_4_60BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (30).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_jazz_2_170BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-A4-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_folk_3_130BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (17).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/classical_8_120BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_scale_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick8_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (45).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (103).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-54209-1111-00101.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/epic-logo-6906.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-F6-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (213).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/17_emil-telmanyi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (66).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (24).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (102).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-66507-1111-00138.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_beethoven2_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_E1_fret_0-20.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (62).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (252).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/9_emil-telmanyi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-A5-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/dr_ni-timpani_drums-klangraum-wort-6790.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_swing_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-63504-1111-213.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/latin_2_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick4_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (214).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (61).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/hi-hat-rhythm-89806.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (5).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-65410-1111-00128.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_bartok_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/029500_morning-rain-piano-65875.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_reggae_3_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_swing_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (71).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_ray-chen_bwv1004_mov5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_beethoven2_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_beethoven1_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_11MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (72).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (1).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (49).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-49109-1111-166.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/8bit-sample-69080.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/14_ray-chen_bwv1004_mov5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_swing_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (19).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_chords_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick7_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-68604-1111-226.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jazz_5_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/latin_5_180BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (91).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_mozart_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/cinematic-handpan-100bpm-25170.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (94).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/10_emil-telmanyi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (108).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-72608-1111-308.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (29).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_joplin_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (51).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (156).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (308).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-sequence-2-66643.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-sadmosphere-12629.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (47).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-E6-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/bongo-and-drum-instrumental-music-21295.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (296).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/reverse-piano-3-note-tune-96852.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/dramatic-reveal-21469.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_beethoven2_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_chords_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/pop_8_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (16).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_G_V_bending.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/latin_4_130BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-crash-sound-37898.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick4_FN_Lage2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_metal_1_160BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (69).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (36).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick3_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick7_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/emotional-piano-001-d-90-66506.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick6_KHBV.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick1_FN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/15_emil-telmanyi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick9_FN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_beethoven2_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-52112-1111-247.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_09HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/crazy-rhythm-loop-120-bpm-006-68062.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_00HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (293).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (3).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-67508-1111-217.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_scale_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-B4-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_bartok_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (49).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/dream-magic-prolonged-94891.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick6_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/on-rd-piano-loop-1-89745.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-F6-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (11).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-75611-1111-00155.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/drums-dembow-rd-loop-26-13497.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (2).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (70).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_bartok_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_05MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/11_ko-donghwi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/18_emil-telmanyi_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/18_emil-telmanyi_bwv1005.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-60310-1111-271.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/latin_6_120BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_mozart_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick4_FN_Lage2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_emil-telmanyi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_04HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/rock_2_115BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_pachelbel_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (82).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_minji-kim_bwv1003_mov1.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (41).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_emil-telmanyi_bwv1002.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (154).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (122).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-50205-1111-175.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (93).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (63).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (160).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-56211-1111-00103.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (318).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (6).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_rock_5_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_pachelbel_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-68604-1111-00148.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_06HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_mozart_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (52).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_swing_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_A_fret_0-20.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-61311-1111-00116.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (202).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (126).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_beethoven1_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/funny-light-optimistic-111775.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_scale_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/drum-loop-43751.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick3_KSBDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/country_2_150BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (53).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-march-66246.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick4_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-45105-1111-00084.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_latin_8_160BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/dr_ni_india_01-klang-raum-wort-29461.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (151).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/808-boom-in-c-46330.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_chords_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick4_FBSH.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick12_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-B5-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/horror-piano-hits-73998.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_joplin_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_joplin_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (47).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1-E1-Major 10.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/african-98600.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/pop_7_60BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/11_karen-gomyo_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (133).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_05SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-50300-1111-00105.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (75).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/boje-3-87816.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_beethoven2_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_john-garner_bwv1002_mov4.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (103).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2-E1-Minor 03.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-66602-1111-00146.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_beethoven2_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2-E1-Minor 06.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_metal_8_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (107).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/19_emil-telmanyi_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (210).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ska_2_180BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_D_muted5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (18).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_chords_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_scale_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (169).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (58).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick12_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (63).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (114).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/prepared-piano-impro-2-69901.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick1_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_reggae_1_95BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-47107-1111-00086.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jinglepiano-105261.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (10).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (110).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-G5-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/metal_4_130BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (295).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick6_KHBV.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick3_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_G_fret_0-20.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/8_emil-telmanyi_bwv1004.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_V_vibrato_slow-fast.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (95).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-57307-1111-268.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (20).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_jazz_6_150BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (39).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_pachelbel_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick5_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (260).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/metal_6_110BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (12).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_john-garner_bwv1002_mov2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_swing_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (87).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/chasing-drums-100-bpm-41050.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-56211-1111-259.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_latin_7_130BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_13HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (121).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick3_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick7_FN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_beethoven1_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (11).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/sad-piano-love-story-71443.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-55210-1111-00102.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_mozart_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/heartbeat-foley-34902.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_03MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-41101-1111-236.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (133).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-piece-for-busy-businessmen-61081.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-60310-1111-00115.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/drum-beat-02-36276.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-D5-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_swing_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (62).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (115).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-57307-1111-00112.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (17).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/c_minor_prog-105132.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (35).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick4_KBSH.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_swing_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/classical_3_50BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/pop_1_160BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (109).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick3_MN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (281).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_john-garner_bwv1002_mov1.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_bartok_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_pachelbel_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/metal_3_135BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1-E1-Major 01.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-G5-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (16).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (72).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (3).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (104).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (146).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (2).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_chords_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_pachelbel_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-A5-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_E_muted5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (279).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-60405-1111-201.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/drums-rap-type-1-oldschool-loop-22-13062.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-67603-1111-303.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (162).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (47).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (56).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/8_silei-li_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick11_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_mozart_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/paino-31466.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/rock_3_115BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (73).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_03SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_scale_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (65).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/latin_8_180BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_01MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/9_kinga-augustyn_bwv1005_mov2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_swing.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_pachelbel_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_09SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (75).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (1).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/hard-rock-logo-108960.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_NH_V.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-E5-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/bar-bq-chicken-drumpsticks-129354.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_joplin_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (131).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_pachelbel_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/drum-beat-bpm-120-113150.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (39).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (43).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-45200-1111-00092.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/genesis-piano-chord-loop-28260.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/174-txls4-106297.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick2_MVSH.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/15_karen-gomyo_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/12_emil-telmanyi_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (128).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_bartok_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-E5-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-F6-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (58).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_swing_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick2_KN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-69605-1111-00149.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_A_muted5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-69605-1111-227.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_B_muted5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/8_emil-telmanyi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (116).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (31).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (19).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (117).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_mozart_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (28).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-B5-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-57212-1111-00104.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (54).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-C5-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_06KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-G5-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (71).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-46201-1111-249.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-76612-1111-00156.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_bartok_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-53208-1111-256.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_swing_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (55).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_05HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (32).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/18_karen-gomyo_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_joplin_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-70606-1111-306.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick6_MN_Lage2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/18_emil-telmanyi_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_john-garner_bwv1002_mov2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (28).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_emil-telmanyi_bwv1004.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (246).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (39).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (10).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (62).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-62503-1111-212.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/dystopian-drum-12331.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_swing_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (271).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/hihat-loop-bpm140-107127.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ska_3_180BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_scale_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (29).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/reggae_1_120BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/realizations-or-fighting-70370.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_scale_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick2_KN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (209).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_scale_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-65506-1111-00137.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick3_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick1_MVSDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (320).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_pop_5_78BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (7).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-67603-1111-225.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/11_silei-li_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (16).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1-E1-Major 09.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_bartok_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_bartok_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (21).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (167).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (111).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_swing_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/16_karen-gomyo_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_03KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (80).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_chords_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/folk_1_110BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_john-garner_bwv1002_mov4.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-64505-1111-00136.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (244).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-55210-1111-180.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-C5-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/11_emil-telmanyi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/13_emil-telmanyi_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (111).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (273).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_NH_IV.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-C6-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (10).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-65601-1111-223.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-48108-1111-165.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/pop_5_90BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-47107-1111-242.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (77).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_pachelbel_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick8_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (133).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_ska_1_150BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-harp-sequence-66667.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-A4-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (92).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-C5-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/dr-tribal-percussion-triplet-loop-high-passed-106bpm-25935.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (34).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_G_muted5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_mozart_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (319).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (121).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-41101-1111-158.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-67412-1111-00130.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_minji-kim_bwv1003_mov1.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-D5-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (284).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (229).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/kastimes-drum-sample-7-35739.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (124).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (101).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (33).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_john-garner_bwv1002_mov1.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (173).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (88).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jazz_2_170BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-50110-1111-00089.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-bells-4-97479.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (134).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-47107-1111-164.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-67508-1111-295.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/loop-12-107632.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-F5-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_bartok_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-D6-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-48108-1111-243.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-47202-1111-00094.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (45).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_pop_3_130BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (119).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_pachelbel_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_mozart_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (59).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick1_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/giant-kit-77973.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (36).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (277).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_mozart_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_chords_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-59309-1111-00114.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_beethoven1_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_joplin_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick10_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (191).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-52207-1111-00099.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_13KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_DN_III_VIII_XIV.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick9_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick2_FVSH.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (108).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-A5-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick7_MN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-C6-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (26).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/dark-evil-piano-32205.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-A5-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-F6-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (31).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_beethoven1_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-D6-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick2_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/808-d3-38858.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (211).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_06SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (13).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_emil-telmanyi_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/beautiful-random-minor-arp-119378.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick4_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (157).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick2_KVSH.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (9).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/boss-rhythm-1-37451.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (78).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (46).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-F5-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-66411-1111-207.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_scale_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_chords_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (85).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/19_karen-gomyo_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_E_muted5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (46).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_chords_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_bartok_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_chords_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-65601-1111-301.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (314).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/9_ray-chen_bwv1004_mov5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-F6-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_NH_VII.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (137).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_chords_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick1_FVSDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-F4-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (87).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_latin_4_130BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (98).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_beethoven2_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (118).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_beethoven1_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_joplin_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-51206-1111-176.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/9_emil-telmanyi_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/drum-beat-loop-3-96537.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (60).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2-E1-Minor 07.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-45105-1111-240.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_pachelbel_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (105).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (3).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (101).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-70511-1111-298.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (321).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_metal_5_150BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_swing_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jingle-16-9634.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_01MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (124).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (21).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/reggae_4_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (199).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-suspenses-14427.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (190).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (24).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_beethoven2_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-A6-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick3_MSBDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (90).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-55210-1111-258.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (18).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/drum-roll-please-6386.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (70).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/14_emil-telmanyi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (53).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (16).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jazz_8_110BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (182).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_04KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-loop-10809.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (122).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_beethoven1_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-70606-1111-228.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-54209-1111-179.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (144).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_oliver-colbentson_bwv1006_mov3.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_02HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_swing_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_emil-telmanyi_bwv1004.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-C6-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (66).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/17_ko-donghwi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_joplin_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick7_FN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (112).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (100).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (185).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/drum-intro-1-113201.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_chords_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (33).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_beethoven2_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_rock_3_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_scale_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (56).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_02HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_D_muted5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_01KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/8_ko-donghwi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick11_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_joplin_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/loopx2-90402.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_E1_fret_0-20.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-59500-1111-287.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-D6-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-41101-1111-00080.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_11KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (119).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick4_KBSH.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick7_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-C5-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_08SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick1_KVSDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (280).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (48).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (86).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (58).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (98).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/latin_7_130BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-49109-1111-244.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_A_fret_0-20.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-62503-1111-00134.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (71).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (35).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_beethoven2_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (92).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (94).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_oliver-colbentson_bwv1006_mov6.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_beethoven1_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick5_MBVDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick11_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_bartok_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (152).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-45200-1111-170.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (64).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (24).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (129).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (234).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-56306-1111-267.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (217).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-G5-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_joplin_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-43103-1111-238.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (172).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick9_KN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_swing_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (31).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-G5-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_bartok_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (220).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick6_FHBV.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jazz_6_180BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-D4-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (27).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_emil-telmanyi_bwv1005.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (77).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (131).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_E_muted5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/9_silei-li_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (27).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/country_5_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (269).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick7_MN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_03SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-A5-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-E5-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_latin_6_120BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (118).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (30).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (1).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_beethoven2_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (38).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (65).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_pachelbel_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/latin_3_120BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_beethoven2_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (278).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_D_muted5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (121).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (53).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (56).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-42102-1111-237.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (225).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_beethoven1_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (20).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_04KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_bartok_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick12_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (222).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick7_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-54304-1111-00109.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (147).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (13).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_06MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (81).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_bartok_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/alien-beeper-103420.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (64).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_beethoven2_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (46).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/classical_3_60BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick6_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (248).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (25).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2-E1-Minor 00.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-58403-1111-199.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (38).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (4).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_joplin_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/classical_2_50BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (9).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (13).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-magic-logo-13621.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/19_emil-telmanyi_bwv1005.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_country_3_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-solo-big-hall-10s-loop-27710.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_joplin_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (2).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (119).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-A5-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (299).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_B_fret_0-20.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/strange-piano-73881.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (330).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-48203-1111-00095.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick6_MN_Lage2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_mozart_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (53).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_scale_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_bartok_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (74).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/15_emil-telmanyi_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_10MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_reggae_2_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_beethoven1_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_DN_III_VIII_XIV.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (118).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick5_MBVDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/054399_8039s-old-school-rap-drum-loop-80433.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_beethoven2_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_beethoven2_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (265).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (15).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (66).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (18).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_jazz_5_80BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-58308-1111-269.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-G6-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_04HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_joplin_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-C7-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/action-drums-78-low-67673.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_scale_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_scale_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (52).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick5_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_chords_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (168).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (270).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (79).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (14).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/13_emil-telmanyi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_08MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_chords_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_01KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (99).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (85).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick5_KBVDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-F6-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_joplin_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (4).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_scale_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-E7-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (14).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_05SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-A6-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (99).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-63504-1111-00135.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (54).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-63408-1111-00126.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_country_1_110BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (90).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (15).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-59404-1111-200.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/sad-piano-58774.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_07HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (100).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_folk_1_80BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/035568_upright-piano-71570.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-60501-1111-288.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (30).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (115).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/deep-drums-42782.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-single-keys-close-79965.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (99).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-50205-1111-00097.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/detuned-piano-39311.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/102-bpm-boom-groove-82498.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (44).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (108).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-50300-1111-261.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (14).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_mozart_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_mozart_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (123).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (198).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/atari-st-beat-09-106443.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/rainy-day-in-a-recliner-piano-26139.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_04MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_beethoven2_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (81).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jazz_8_120BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (32).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/industrial-drums-2-115bpm-80848.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (5).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-61311-1111-272.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (60).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (59).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-D6-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-71607-1111-307.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-50300-1111-183.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_swing_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick1_KVSDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick9_KN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (104).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (12).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-A6-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_bartok_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_minji-kim_bwv1003_mov2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-48108-1111-00087.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-B5-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_pop_1_130BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick7_KN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (48).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_pachelbel_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (61).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/120bpm_kick-build-up-98848.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (200).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick9_MN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (80).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (122).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_E1_muted5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/metal_4_180BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_06SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_mozart_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (42).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_minji-kim_bwv1003_mov3.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (106).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (60).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-51206-1111-00098.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (201).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-52207-1111-255.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_12MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/metal_2_150BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (127).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_12SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/metal_1_180BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/logo-piano-125764.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (26).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (159).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick2_FVSH.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (128).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_joplin_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/reggae_2_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_bartok_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/marching-drums-drum-solo2-fx-99061.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (106).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (324).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/nocturneNr2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ska_1_180BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_05KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick10_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick6_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-64409-1111-283.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/mattc-hectik-beat-box-01qw-99855.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-G6-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (60).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (298).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_bartok_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick9_MN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (41).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-60405-1111-00123.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/9_ko-donghwi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick2_MVSH.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-F5-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-C5-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/country_1_150BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1-E1-Major 07.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick4_FN_Lage2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-57212-1111-260.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (35).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (189).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_swing_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (78).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick2_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (50).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/14_ko-donghwi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (258).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/pop_4_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (90).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-49204-1111-00096.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (196).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (12).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_beethoven2_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (111).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (116).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/folk_2_75BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_NH_IX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (97).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/chill-drum-loop-6887.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (61).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/pop_3_150BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick9_FN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_classical_8_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick3_FSBDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/10_emil-telmanyi_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (84).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (89).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_07SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick2_MVSH.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (10).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_E_fret_0-20.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_chords_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (16).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (149).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/drum-01-69203.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_minji-kim_bwv1003_mov4.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (110).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (8).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_rock_8_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_john-garner_bwv1002_mov3.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (304).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-62312-1111-195.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-61406-1111-280.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-D7-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/metal_7_75BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_bartok_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (21).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/16_emil-telmanyi_bwv1005.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (132).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_john-garner_bwv1002_mov4.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (180).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/pop_6_110BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_john-garner_bwv1002_mov7.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick5_KBVDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_classical_7_50BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_john-garner_bwv1002_mov2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (117).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (28).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_minji-kim_bwv1003_mov1.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_kinga-augustyn_bwv1005_mov2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/classical_5_70BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1-E1-Major 04.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_mozart_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_oliver-colbentson_bwv1006_mov1.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_00KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_scale_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (125).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-56211-1111-181.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-52207-1111-177.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick10_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick5_MBVDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick10_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_scale_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/10_emil-telmanyi_bwv1005.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (254).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/glitch-beat-001-128-bpm-99953.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_bartok_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (22).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_pachelbel_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_pachelbel_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/quintfall.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (50).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_emil-telmanyi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano_pattern_50014_108bpm-27614.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/metal_7_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (40).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/new-9-42628.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/16_emil-telmanyi_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-D7-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jazz_6_150BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (216).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_beethoven1_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/12_silei-li_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_joplin_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (166).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/metal_8_125BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_mozart.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ska_3_150BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_metal_2_150BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/109-bpm-70s-style-drum-loop-76138.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-A4-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_swing_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-63504-1111-291.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (37).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick1_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick6_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_beethoven1_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/metal_1_160BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_chords_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-A4-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (127).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-54209-1111-257.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (18).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_beethoven2_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick4_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_oliver-colbentson_bwv1006_mov7.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_00KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/dirty-rising-tube-32714.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_chords_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (297).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (17).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (39).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (67).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (100).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_bartok_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/15_emil-telmanyi_bwv1005.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/latin_4_160BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (204).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-60501-1111-00132.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (40).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-38809.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_beethoven1_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_beethoven2_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_chords_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/13_emil-telmanyi_bwv1002.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-61406-1111-202.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/on-rd-bruce-drums-1-89708.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-46106-1111-00085.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_beethoven2_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (111).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/orcs-loop-21568.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-55400-1111-274.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/muvibeat9_130bpm-14339.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_bartok_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_joplin_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (94).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/pop_3_130BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (301).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (76).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (227).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-59309-1111-270.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-51111-1111-246.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (19).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (267).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (65).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (130).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/pop_2_120BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick3_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/hip-hop-drum-loop-24-75662.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (57).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-E6-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (122).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-B5-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_joplin_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_beethoven2_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-52112-1111-169.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_chords_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_pachelbel_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jungleloop-87697.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_ko-donghwi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick3_KSBDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (136).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (186).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_jazz_4_70BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_scale_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick4_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick9_FN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-A4-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-59404-1111-278.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-C6-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/dark-piano-tension-6057.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (32).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (54).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (108).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (285).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/drums-buildup-3-84503.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (109).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/metronomico-45663.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-B6-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-66602-1111-302.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_swing_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/country_4_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_mozart_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_joplin_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (129).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick2_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_mozart_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (7).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-52112-1111-00091.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (286).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/pathetique_mono.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (93).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (241).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-51111-1111-168.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-62503-1111-290.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (261).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_pachelbel_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (42).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_ray-chen_bwv1004_mov5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick1_FN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_mozart_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_pachelbel_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-40100-1111-235.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jazz_2_200BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (76).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_chords_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_mozart_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_bartok_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-45200-1111-248.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_beethoven2_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (102).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_swing_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (113).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (114).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick5_FBVDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (294).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_minji-kim_bwv1003_mov2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_bartok_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-76612-1111-234.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/lets-dance-126506.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (88).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/pop_8_85BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_pop_7_60BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_swing_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/epic_battle_music_1-6275.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (206).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_joplin_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/classical_4_80BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/crazy-rhythm-loop-120-bpm-0015-68064.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (97).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (5).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_pop_4_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-57402-1111-198.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_beethoven1_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-63408-1111-204.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_bartok_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_scale_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/fillin-fill-timbal-edm-reggae-loop-18-12766.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-D5-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/friendly-melody-14015.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_joplin_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (64).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_john-garner_bwv1002_mov5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/reversed-distorted-piano-70785.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-harp-28522.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_beethoven1_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (223).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_john-garner_bwv1002_mov1.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-A4-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick2_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/funky-drummer-drum-kit-94738.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-C7-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_mozart_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (56).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (11).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (13).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (44).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (245).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick1_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (63).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-G5-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (257).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-57307-1111-190.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_chords_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_beethoven1_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/hard-edm-drum-loop-140-bpm-37696.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (51).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/boss-sp-303-drum-rhythm-200-bpm-8437.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_02MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/lckk_118_drum_01_full-43553.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_scale_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick11_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_scale.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-A6-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (113).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_scale_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1-E1-Major 03.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-55400-1111-196.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (130).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (41).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (58).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-49204-1111-174.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (40).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_pachelbel_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (27).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/heavy-tribal-beat-76580.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (56).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_swing_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_chords_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/classical_1_60BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/classical_5_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/folk_2_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (117).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (33).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_chords_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (44).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_swing_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_beethoven2_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_03KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (84).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_bartok_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-62407-1111-00125.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-65410-1111-206.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/classical_8_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_beethoven2_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_mozart_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/toy-piano-27311.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/10_emil-telmanyi_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_scale_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (219).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_mozart_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/bass-loops-003-with-drums-long-loop-120-bpm-24371.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_NH_IX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_scale_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/break-delay-88443.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (51).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_swing_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_04MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (27).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-44104-1111-239.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_joplin_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/9_emil-telmanyi_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_oliver-colbentson_bwv1006_mov3.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (6).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick11_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick7_KN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/menu-music-28480.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (19).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_02KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-70606-1111-00150.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_rock_4_110BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_oliver-colbentson_bwv1006_mov5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (325).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (9).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/kick-drum-f-14574.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (57).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_swing_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (8).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_beethoven1_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-63408-1111-282.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_beethoven2_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-58308-1111-00113.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/cool-drum-beat-loop-109650.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/country_2_110BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/13_ko-donghwi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (70).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick11_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_bartok_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/drum-loop-140bpm-73728.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_beethoven2_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-44104-1111-161.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (69).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-75611-1111-233.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/10_ray-chen_bwv1004_mov5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2-E1-Minor 08.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/pop_6_90BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_E_fret_0-20.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick5_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_pachelbel_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (187).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/joyful-messy-piano-116715.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_beethoven2_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/rock_6_120BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-B6-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-55305-1111-00110.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (105).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (43).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/metal_5_150BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_beethoven2_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-66507-1111-216.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (107).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1-E1-Major 08.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (68).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-65410-1111-284.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (230).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/metal_5_180BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (78).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-G6-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (104).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick3_MSBDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (114).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (6).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_B_fret_0-20.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_B_muted5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-51206-1111-254.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-F5-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_bartok_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick9_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_joplin_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/loop-16247.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_mozart_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_oliver-colbentson_bwv1006_mov1.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (207).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (97).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-69510-1111-00141.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_mozart_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/jazz_4_70BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_pachelbel_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_john-garner_bwv1002_mov7.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_pachelbel_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick4_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (33).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_13MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-70511-1111-00142.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-54304-1111-265.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_beethoven2_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-C6-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (29).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_04SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-D6-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (88).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-68509-1111-296.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-c4-major-scale-102084.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick5_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/rock_4_125BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (26).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick7_KN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/060311_rubbing-drum-sticksaif-82905.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_beethoven1_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (274).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/latin_8_160BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (21).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-B5-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (74).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/digital-pulse-140-2-122269.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-42102-1111-159.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-A6-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (18).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (70).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (327).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick5_KN_Lage2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (118).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_ko-donghwi_bwv1001.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (53).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/this-is-war-version-e-95411.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-chords-130-bpm-51099.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1-E1-Major 02.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-bell-sound-1-27144.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/8_emil-telmanyi_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-G6-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick4_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_oliver-colbentson_bwv1006_mov7.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_beethoven2_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (134).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_mozart_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_joplin_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_V_vibrato_slow-fast.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/james-brown-type-drum-loop-120bpm-129551.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-67592.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_pachelbel_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (125).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (55).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (305).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-68604-1111-304.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (117).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick4_KBSH.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (32).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-50110-1111-167.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_rock_6_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (85).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (38).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_pachelbel_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (83).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (116).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/classical_7_50BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/13_karen-gomyo_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_E_fret_0-20.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-sequence-66644.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/rising-into-darkness-16314.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (43).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (50).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_chords_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (72).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_john-garner_bwv1002_mov8.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (292).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (316).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (33).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_joplin_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/11_emil-telmanyi_bwv1005.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/funk-bitxl-op-i1-34289.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_beethoven2_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_beethoven1_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (188).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick11_FN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-E5-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (2).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (22).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_rock_7_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (39).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_beethoven1_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (16).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (282).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (7).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (56).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick8_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_mozart_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_jazz_3_120BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_minji-kim_bwv1003_mov3.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_metal_6_110BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (68).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_beethoven2_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/hitting-ride-bell-with-a-finger-98356.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (31).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (212).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_beethoven2_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_scale_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (179).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick5_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (25).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_minji-kim_bwv1003_mov4.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/next-chapter-piano-ident-21438.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-69510-1111-219.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_beethoven1_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_mozart_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_chords_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (49).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_06MIX.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (68).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-72608-1111-230.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (69).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/the-last-piano-112677.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_mozart_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (75).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_mozart_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (71).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (64).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/kc-mus152-jungle-beat-63962.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-60310-1111-193.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (28).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (20).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_joplin_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_NH_VII.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1-E1-Major 05.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_swing_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-58403-1111-00121.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-43103-1111-00082.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (104).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick6_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/dumdum-105423.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (112).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-loop-130bpm-43342.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/13_ray-chen_bwv1004_mov5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-A6-pp-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (66).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_scale_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-56306-1111-00111.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_06HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_G_V_slide.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (59).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_pop_6_90BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-68509-1111-218.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_chords_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-F6-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (15).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (41).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-harp-sequence-28500.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-56306-1111-189.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_john-garner_bwv1002_mov8.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_joplin_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (120).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (27).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (5).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (97).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (50).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_03HHtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick4_MBSH.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick2_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-B4-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (50).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_pachelbel_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_pachelbel_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_oliver-colbentson_bwv1006_mov3.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/promenade_mono.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_G_V_slide.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick10_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (235).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_A_muted5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/folk_1_80BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (268).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_oliver-colbentson_bwv1006_mov6.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-40100-1111-00079.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (145).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_NH_XII.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_pachelbel_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/do-80236.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_latin_2_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_scale_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (73).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/pathetique_poly.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_chords_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (255).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/epic-extreme-hit-109132.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-65601-1111-00145.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_swing_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_bartok_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_pachelbel_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_joplin_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/15_emil-telmanyi_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-E6-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_09KDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/latin_1_160BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/piano-43703.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-F5-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/happy-day-in-beach-hand-panwav-14755.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (236).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-58308-1111-191.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_beethoven2_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_sound (329).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-46201-1111-171.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-69510-1111-297.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/beatboxlong-72000.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (181).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/1_emil-telmanyi_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (57).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_chords_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_jazz_7_140BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (10).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_beethoven1_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sounds (52).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room6_MUS_bartok_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (90).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-D5-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (83).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_ska_3_150BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-65506-1111-293.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/folk_3_130BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-52302-1111-263.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/RealDrum01_01SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (89).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (95).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (43).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/12_karen-gomyo_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (307).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/11_emil-telmanyi_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (92).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/17_karen-gomyo_bwv1006.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (132).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (68).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (1).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (9).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/country_1_110BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-G5-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_beethoven1_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (51).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (15).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_B_fret_0-20.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_emil-telmanyi_bwv1003.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_swing_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (128).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (174).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (256).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_joplin_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/140-bpm-amen-break-original-processed-6945 (1).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/brushes-loop-77bpm-by-canis-et-circensis-91191.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (67).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (47).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_bartok_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick5_FBVDN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (84).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/odddrumloop-91606.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/reggae_3_120BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (46).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (4).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/DRUM_SOUND (43).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/nigh-in-detroit-2-34979.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (24).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (73).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick9_KN_Lage.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick10_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-71607-1111-00151.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (9).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/gabber-breakbeat-145bpm-wav-103637.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (170).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick8_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (138).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/TechnoDrum01_01SDtrain.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_pop_8_85BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-57402-1111-276.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/FS_Lick5_KN_Lage2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (80).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (37).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room7_MUS_chords_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Violin_Sound (6).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-F4-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2-E1-Minor 05.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_45SD (23).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-F4-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/hipsnare-85814.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (101).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_43HH (92).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/slow_rock_2_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/violin_sound (240).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_joplin_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-E7-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room4_MUS_chords_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-61502-1111-00133.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Vn-ord-D6-pp-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/0_oliver-colbentson_bwv1006_mov1.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/12_emil-telmanyi_bwv1005.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-44104-1111-00083.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room5_MUS_swing_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/2_john-garner_bwv1002_mov5.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-61502-1111-289.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room8_MUS_chords_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room3_MUS_beethoven1_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/VIOLIN_SOUND (105).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/LP_Lick3_MN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (8).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/AR_Lick3_KN.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-74610-1111-310.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room2_MUS_beethoven2_DEV_ipad.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/G53-66602-1111-224.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/latin_3_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/Va-ord-F5-mf-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/ROOM_room1_MUS_beethoven2_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/WaveDrum02_39KD (14).wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission/14_emil-telmanyi_bwv1002.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/ROOM_room6_MUS_pachelbel_DEV_iphone.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/phaze-giant-bass-69687.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/guitar-swellsolo-34241.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/ROOM_room7_MUS_beethoven1_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/percussion-spirit-drums-by-prettysleepy-art-13788.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/transition-piano-34391.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/guitar-song-57789.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/guitar-solo-27194.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/muvibeat5_130bpm-14335.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/paft-drunk-freestyle-drum-22227.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/ROOM-room8-MUS-beethoven2.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/Sad-Violin-G-www.fesliyanstudios.com.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/intro-music-black-box-simple-guitar-12701.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/ROOM_room6_MUS_pachelbel_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/time-break-drum-only-83822.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/Va-ord-F5-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/100bpm-808-like-drum-loop-74838.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/guitar-solo-5999.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/rock_6_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/short-melancholic-theme-on-piano-34024.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/psy-bass-beat-sample-101275.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/guitar-solo-74247.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/rock_8_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/Sad-Violin-H-www.fesliyanstudios.com.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/whatever-you-say-piano-27291.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/rock_2_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/short-logo-108964.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/Sad-Violin-J-Slow-www.fesliyanstudios.com.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/rock_7_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/guitar-chords-70663.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/Sad-Violin-D-www.fesliyanstudios.com.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/Va-ord-F4-mf-3c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/ska_2_150BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/muvibeat4_130bpm-14334.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/tribe-drum-loop-103173.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/muvibeat3_130bpm-14333.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/Sad-Violin-A-www.fesliyanstudios.com.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/pop-drums-loops-3-11278.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/Va-ord-G5-ff-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/ROOM-room8-MUS-beethoven1.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/yuwu-quiz-113046.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/Sad-Violin-Slow-K-www.fesliyanstudios.com.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/Va-ord-F3-mf-4c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/ROOM_room6_MUS_mozart_DEV_redmi.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/warm-piano-logo-116098.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/rock_4_110BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/hip-hop-drum-loop-22-33572.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/Va-ord-F4-ff-2c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/ROOM_room6_MUS_mozart_DEV_stereomic.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/simple-loopable-beat-88509.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/ROOM-room8-MUS-chords.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/rock_5_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/Va-ord-E4-mf-3c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/hip-hop-drum-loop-25-33571.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/rock-drum-loop-85371.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/pulsing-rhythm-31775.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/Sad-Violin-Lower-F-www.fesliyanstudios.com.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/acoustic-guitar-logo-13084.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/Va-ord-F6-mf-1c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/Va-ord-F5-mf-3c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/Va-ord-E4-pp-3c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/wondercue-111933.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/ska_1_150BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/si-80238.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/pop-rock-beat-fs-27798.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/ROOM_room6_MUS_mozart_DEV_lg.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/Sad-Violin-C-www.fesliyanstudios.com.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/ROOM_room6_MUS_pachelbel_DEV_amazon.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/world-drum-beat-76217.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/Sad-Violin-Moderate-Speed-I-www.fesliyanstudios.com.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/kitchen-drum200616_0006_01-39711.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/guitar-soundtrack-55233.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/Sad-Violin-Fast-E-www.fesliyanstudios.com.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/rock_3_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/rock_1_100BPM.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/darbuka-drum-percussion-64018.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/soft-piano-100-bpm-121529.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/Va-ord-E4-ff-4c.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/wingrandpiano-96338.wav\n",
      "/root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission/guitar-intro-110935.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Assuming the path from kagglehub.dataset_download is the parent directory\n",
    "# You might need to adjust this path based on the actual structure within the downloaded dataset\n",
    "downloaded_dataset_path = path # Use the 'path' variable from the previous cell\n",
    "\n",
    "print(f\"Listing files in the downloaded dataset directory: {downloaded_dataset_path}\")\n",
    "\n",
    "# Walk through the directory and print all file paths\n",
    "for root, dirs, files in os.walk(downloaded_dataset_path):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fbdf7ad"
   },
   "source": [
    "# Task\n",
    "Develop a music generation model conditioned on both audio and text inputs. The model should generate music in MP3 format and accompanying lyrics in a text file. Use the audio files from \"/content/archive.zip\" and \"/content/dataset_second.zip\" and the dataset downloaded from \"https://storage.googleapis.com/kaggle-data-sets/2768513/4784071/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20250827%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20250827T024359Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=1a1ddbaf1b0e32e9aabd16dd7e2f70fd8fe2ae40dcdf8e65453881aba3d1db0ccc133c315998513fdbb1093f834d23234a8a8ab08924dcc653b3b75d0841cf7e6d013fd68b33d60fee3a355e88b56f26362ff5f6a57d1f50c75d8a8e64bb28db4ba8918a3ffce070c310fd1c12f0648ce4b3768481ca3faceefbe0ae1d9d5db37123030768f70da56cae3988736fa857ce149fc39f0489b001495f73889e5bbb0fa093f3b0fb5e9854fad6fe9f74115979c4747898315a284f42df649583810b62e7c86b49d2ec2c98c3203ff90f7d2c19201b15b9fb57386197c55f28a0c56c6a537e100a85647b53b8307ddfa00810766c7a248fa3b81c2f6edd3ead8d00da\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56bb64bf"
   },
   "source": [
    "## Refine audio feature processing for conditioning\n",
    "\n",
    "### Subtask:\n",
    "Update the feature extraction function to handle the downloaded audio files and extract relevant features (MFCCs and Chroma).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beacd128"
   },
   "source": [
    "**Reasoning**:\n",
    "Update the feature extraction function to handle the newly downloaded dataset path and extract both MFCC and Chroma features, ensuring padding/truncation and correct output shape, and print the shape of extracted features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69479eda",
    "outputId": "ef1665e9-2678-4dec-c048-76a5f01bcd5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for music instrument files in: /root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3\n",
      "Found 2029 music instrument files in the downloaded dataset.\n",
      "\n",
      "Extracting features from sample music instrument files...\n",
      "Extracted features shape for TechnoDrum01_00MIX.wav: (174, 32)\n",
      "Extracted features shape for violin_sound (224).wav: (174, 32)\n",
      "Extracted features shape for WaveDrum02_39KD (125).wav: (174, 32)\n",
      "Extracted features shape for RealDrum01_00SDtrain.wav: (174, 32)\n",
      "Extracted features shape for ROOM_room7_MUS_bartok_DEV_redmi.wav: (174, 32)\n",
      "\n",
      "Verified shape of first extracted sample: (174, 32)\n",
      "Expected shape: (174, 32)\n",
      "Feature shape matches expected shape.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Assuming 'path' from the kagglehub download is available and contains the dataset root\n",
    "# The actual audio files are likely within subdirectories like 'Train_submission'\n",
    "downloaded_dataset_root = path # Use the path obtained from kagglehub.dataset_download\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This will need to be determined based on the desired sequence length or maximum length of audio files\n",
    "n_chroma = 12 # Number of chroma bins\n",
    "total_audio_features_dim = n_mfcc + n_chroma # Total features\n",
    "\n",
    "def extract_features_updated(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "    \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract Chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "        # Pad or truncate combined features to a fixed length\n",
    "        if combined_features.shape[1] < target_sequence_length:\n",
    "            combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "        else:\n",
    "            combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "        return combined_features.T # Transpose to have shape (time steps, total_features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Collect music instrument files from the downloaded dataset\n",
    "music_instrument_files_updated = []\n",
    "\n",
    "# Walk through the downloaded dataset directory to find audio files\n",
    "print(f\"Searching for music instrument files in: {downloaded_dataset_root}\")\n",
    "for root, dirs, files in os.walk(downloaded_dataset_root):\n",
    "    for name in files:\n",
    "        # Assuming music instrument files are in 'Train_submission' or 'Test_submission'\n",
    "        # and have .wav or .mp3 extensions\n",
    "        if ('Train_submission' in root or 'Test_submission' in root) and (name.endswith('.wav') or name.endswith('.mp3')):\n",
    "             # Further check if the file name indicates a music instrument (optional but good practice)\n",
    "            if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                 music_instrument_files_updated.append(os.path.join(root, name))\n",
    "\n",
    "print(f\"Found {len(music_instrument_files_updated)} music instrument files in the downloaded dataset.\")\n",
    "\n",
    "# Extract features from a sample of the collected files to verify\n",
    "sample_files_to_process = music_instrument_files_updated[:5] # Process a few sample files\n",
    "\n",
    "print(\"\\nExtracting features from sample music instrument files...\")\n",
    "extracted_sample_features = []\n",
    "for file_path in sample_files_to_process:\n",
    "    features = extract_features_updated(file_path, max_padding, n_mfcc, n_chroma)\n",
    "    if features is not None:\n",
    "        extracted_sample_features.append(features)\n",
    "        print(f\"Extracted features shape for {os.path.basename(file_path)}: {features.shape}\")\n",
    "    else:\n",
    "        print(f\"Failed to extract features for {os.path.basename(file_path)}\")\n",
    "\n",
    "# Verify the shape of one of the extracted samples\n",
    "if extracted_sample_features:\n",
    "    print(f\"\\nVerified shape of first extracted sample: {extracted_sample_features[0].shape}\")\n",
    "    expected_shape = (max_padding, total_audio_features_dim)\n",
    "    print(f\"Expected shape: {expected_shape}\")\n",
    "    if extracted_sample_features[0].shape == expected_shape:\n",
    "        print(\"Feature shape matches expected shape.\")\n",
    "    else:\n",
    "        print(\"Feature shape does NOT match expected shape.\")\n",
    "else:\n",
    "    print(\"\\nNo features were successfully extracted from sample files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 654
    },
    "id": "8abf08e1",
    "outputId": "09f11f6c-eff9-4852-aa1b-1c138eadfb48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ text_conditioning_… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ musical_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ repeat_vector       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ text_conditionin… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ audio_conditioning… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ musical_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ repeat_vector[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ audio_conditioni… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">238,080</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">296,448</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">174</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,140</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ text_conditioning_… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ musical_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m, \u001b[38;5;34m20\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ repeat_vector       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ text_conditionin… │\n",
       "│ (\u001b[38;5;33mRepeatVector\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ audio_conditioning… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m, \u001b[38;5;34m180\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ musical_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ repeat_vector[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ audio_conditioni… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m238,080\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m296,448\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ bidirectional_1[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m174\u001b[0m, \u001b[38;5;34m20\u001b[0m)   │      \u001b[38;5;34m5,140\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">539,668</span> (2.06 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m539,668\u001b[0m (2.06 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">539,668</span> (2.06 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m539,668\u001b[0m (2.06 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New conditional music generation model (v2) architecture defined.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, GRU, Input, TimeDistributed, Bidirectional\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This needs to be consistent with feature extraction\n",
    "n_chroma = 12 # Number of chroma bins\n",
    "total_audio_features_dim = n_mfcc + n_chroma # Total features for audio conditioning\n",
    "\n",
    "# Define the dimensions for the music generation model\n",
    "musical_feature_dim = n_mfcc # The model output is MFCCs\n",
    "musical_sequence_length = max_padding # The model input sequence length\n",
    "\n",
    "# Define the conditioning input dimensions\n",
    "text_conditioning_dim = 128 # Dimension of the text embedding\n",
    "\n",
    "# --- Redefine the Conditional Music Generation Model to accept Sequential Audio Conditioning ---\n",
    "\n",
    "# Input 1: Musical features (MFCCs) - the sequence we are building upon\n",
    "musical_input = Input(shape=(musical_sequence_length, musical_feature_dim), name='musical_input')\n",
    "\n",
    "# Input 2: Text conditioning input (single vector)\n",
    "text_conditioning_input = Input(shape=(text_conditioning_dim,), name='text_conditioning_input')\n",
    "\n",
    "# Input 3: Audio conditioning input (sequence of features)\n",
    "audio_conditioning_input = Input(shape=(musical_sequence_length, total_audio_features_dim), name='audio_conditioning_input')\n",
    "\n",
    "\n",
    "# Process text conditioning (repeat the single vector across the musical sequence length)\n",
    "repeated_text_conditioning = tf.keras.layers.RepeatVector(musical_sequence_length)(text_conditioning_input)\n",
    "\n",
    "# Concatenate musical input with repeated text conditioning and audio conditioning input\n",
    "# This is one way to combine them. Another would be separate processing branches.\n",
    "# Concatenate [musical_input, repeated_text_conditioning, audio_conditioning_input]\n",
    "combined_input = Concatenate(axis=-1)([musical_input, repeated_text_conditioning, audio_conditioning_input])\n",
    "\n",
    "\n",
    "# GRU layers for the sequence generation, now taking the combined input\n",
    "# We can use Bidirectional GRU for potentially better context\n",
    "gru_1 = Bidirectional(GRU(128, activation='relu', return_sequences=True))(combined_input)\n",
    "dropout_1 = Dropout(0.2)(gru_1)\n",
    "\n",
    "gru_2 = Bidirectional(GRU(128, activation='relu', return_sequences=True))(dropout_1)\n",
    "dropout_2 = Dropout(0.2)(gru_2)\n",
    "\n",
    "# TimeDistributed Dense layer to output the generated musical features (MFCCs) for each time step\n",
    "# The output still needs to match the musical_feature_dim\n",
    "output_layer = TimeDistributed(Dense(musical_feature_dim, activation='linear'))(dropout_2)\n",
    "\n",
    "# Define the new conditioned music generation model\n",
    "# The model now has three inputs: musical_input, text_conditioning_input, audio_conditioning_input\n",
    "model_gen_conditioned_v2 = Model(inputs=[musical_input, text_conditioning_input, audio_conditioning_input], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=1.0) # Add gradient clipping\n",
    "model_gen_conditioned_v2.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Print the model summary\n",
    "model_gen_conditioned_v2.summary()\n",
    "\n",
    "print(\"\\nNew conditional music generation model (v2) architecture defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08ac90fd",
    "outputId": "b2776250-c1d4-4866-8299-5e038002913e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the dataset to ensure path is available...\n",
      "Dataset path: /kaggle/input/musical-instruments-sound-dataset\n",
      "Searching for music instrument files in: /kaggle/input/musical-instruments-sound-dataset/Train_submission/Train_submission\n",
      "Found 1965 music instrument files in training submission.\n",
      "Searching for music instrument files in: /kaggle/input/musical-instruments-sound-dataset/Test_submission/Test_submission\n",
      "Found 2029 music instrument files in test submission.\n",
      "\n",
      "Extracting musical features (MFCCs) and audio conditioning features (MFCC+Chroma) from music instrument files...\n",
      "Processing file 100/2029\n",
      "Processing file 200/2029\n",
      "Processing file 300/2029\n",
      "Processing file 400/2029\n",
      "Processing file 500/2029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 600/2029\n",
      "Processing file 700/2029\n",
      "Processing file 800/2029\n",
      "Processing file 900/2029\n",
      "Processing file 1000/2029\n",
      "Processing file 1100/2029\n",
      "Processing file 1200/2029\n",
      "Processing file 1300/2029\n",
      "Processing file 1400/2029\n",
      "Processing file 1500/2029\n",
      "Processing file 1600/2029\n",
      "Processing file 1700/2029\n",
      "Processing file 1800/2029\n",
      "Processing file 1900/2029\n",
      "Processing file 2000/2029\n",
      "\n",
      "Feature extraction and data splitting complete.\n",
      "Shape of training musical features (MFCCs): (1623, 174, 20)\n",
      "Shape of validation musical features (MFCCs): (406, 174, 20)\n",
      "Shape of training audio conditioning features (MFCC+Chroma): (1623, 174, 32)\n",
      "Shape of validation audio conditioning features (MFCC+Chroma): (406, 174, 32)\n",
      "\n",
      "Shape of dummy training text conditioning data: (1623, 128)\n",
      "Shape of dummy validation text conditioning data: (406, 128)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import kagglehub # Import kagglehub to get the dataset path\n",
    "\n",
    "# Download the dataset and directly assign the path within this cell\n",
    "print(\"Downloading the dataset to ensure path is available...\")\n",
    "try:\n",
    "    downloaded_dataset_root = kagglehub.dataset_download(\"soumendraprasad/musical-instruments-sound-dataset\")\n",
    "    print(f\"Dataset path: {downloaded_dataset_root}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    downloaded_dataset_root = None # Set to None if download fails\n",
    "\n",
    "\n",
    "if downloaded_dataset_root is None or not os.path.exists(downloaded_dataset_root):\n",
    "    print(\"Failed to get the dataset path. Cannot proceed with data preparation.\")\n",
    "else:\n",
    "    # Define feature extraction parameters (should be consistent with model definition)\n",
    "    n_mfcc = 20\n",
    "    max_padding = 174\n",
    "    n_chroma = 12 # Although not used for musical features output, needed for audio conditioning feature extraction\n",
    "    total_audio_features_dim = n_mfcc + n_chroma\n",
    "    text_conditioning_dim = 128\n",
    "\n",
    "    # Define helper functions within this cell\n",
    "    def extract_mfcc_features_only(file_path, target_sequence_length, n_mfcc=20):\n",
    "        \"\"\"Extract only MFCC features from an audio file.\"\"\"\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "            # Pad or truncate MFCCs to a fixed length\n",
    "            if mfccs.shape[1] < target_sequence_length:\n",
    "                mfccs = np.pad(mfccs, ((0, 0), (0, target_sequence_length - mfccs.shape[1])), mode='constant')\n",
    "            else:\n",
    "                mfccs = mfccs[:, :target_sequence_length]\n",
    "\n",
    "            return mfccs.T # Transpose to have shape (time steps, n_mfcc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "        \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "            # Extract MFCC features\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "            # Extract Chroma features\n",
    "            chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "            # Concatenate features\n",
    "            combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "            # Pad or truncate combined features to a fixed length\n",
    "            if combined_features.shape[1] < target_sequence_length:\n",
    "                combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "            else:\n",
    "                combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "            return combined_features.T # Transpose to have shape (time steps, total_features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def process_user_audio_for_conditioning(audio_file_path, target_sequence_length, n_mfcc, n_chroma):\n",
    "        \"\"\"Processes user audio input and returns features suitable for conditioning.\n",
    "\n",
    "        Args:\n",
    "            audio_file_path: Path to the user's audio file.\n",
    "            target_sequence_length: The expected sequence length for model input (e.g., max_padding).\n",
    "            n_mfcc: The number of MFCCs to extract.\n",
    "            n_chroma: The number of chroma bins to extract.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            A numpy array containing the processed audio features (shape: (1, target_sequence_length, total_feature_dimension)),\n",
    "            or None if processing fails.\n",
    "        \"\"\"\n",
    "        processed_features = extract_features(audio_file_path, target_sequence_length, n_mfcc, n_chroma)\n",
    "\n",
    "        if processed_features is not None:\n",
    "            # Add a batch dimension\n",
    "            processed_features = processed_features[np.newaxis, :, :] # Shape (1, target_sequence_length, total_feature_dimension)\n",
    "            # print(f\"Processed user audio features shape for conditioning: {processed_features.shape}\") # Debug print\n",
    "            return processed_features\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Function to extract combined features (MFCC + Chroma) for audio conditioning data\n",
    "    def extract_combined_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "        \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "            # Extract MFCC features\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "            # Extract Chroma features\n",
    "            chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "            # Concatenate features\n",
    "            combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "            # Pad or truncate combined features to a fixed length\n",
    "            if combined_features.shape[1] < target_sequence_length:\n",
    "                combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "            else:\n",
    "                combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "            return combined_features.T # Transpose to have shape (time steps, total_features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    # Collect music instrument files from the downloaded dataset\n",
    "    music_instrument_files = []\n",
    "\n",
    "    # Assuming the structure from the previous file listing\n",
    "    train_submission_path = os.path.join(downloaded_dataset_root, 'Train_submission', 'Train_submission')\n",
    "    test_submission_path = os.path.join(downloaded_dataset_root, 'Test_submission', 'Test_submission') # Assuming Test_submission is also relevant for data\n",
    "\n",
    "    # Collect files from Train_submission\n",
    "    if os.path.exists(train_submission_path):\n",
    "        print(f\"Searching for music instrument files in: {train_submission_path}\")\n",
    "        for root, dirs, files in os.walk(train_submission_path):\n",
    "            for name in files:\n",
    "                if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                     # Optional: further filter by keywords if needed\n",
    "                    if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                         music_instrument_files.append(os.path.join(root, name))\n",
    "        print(f\"Found {len(music_instrument_files)} music instrument files in training submission.\")\n",
    "    else:\n",
    "        print(f\"Warning: Training submission directory not found at {train_submission_path}\")\n",
    "\n",
    "\n",
    "    # Collect files from Test_submission (if it exists and contains relevant audio)\n",
    "    # You might decide to use test data only for evaluation later, but for now, let's include relevant audio\n",
    "    if os.path.exists(test_submission_path):\n",
    "         print(f\"Searching for music instrument files in: {test_submission_path}\")\n",
    "         for root, dirs, files in os.walk(test_submission_path):\n",
    "             for name in files:\n",
    "                 if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                      # Optional: further filter by keywords if needed\n",
    "                     if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                          music_instrument_files.append(os.path.join(root, name))\n",
    "         print(f\"Found {len(music_instrument_files)} music instrument files in test submission.\")\n",
    "    else:\n",
    "         print(f\"Warning: Test submission directory not found at {test_submission_path}\")\n",
    "\n",
    "\n",
    "    # Use the collected music instrument files for feature extraction\n",
    "    files_to_process_music = music_instrument_files\n",
    "\n",
    "    # Prepare lists to store extracted features for training\n",
    "    X_train_music_features = [] # For musical input (MFCCs only)\n",
    "    X_train_audio_conditioning_features = [] # For audio conditioning input (MFCC + Chroma)\n",
    "\n",
    "\n",
    "    print(\"\\nExtracting musical features (MFCCs) and audio conditioning features (MFCC+Chroma) from music instrument files...\")\n",
    "\n",
    "    # Process a subset of files for faster testing, or all files for full training\n",
    "    # process_limit = 100 # Set a limit for testing\n",
    "    # files_to_process_subset = files_to_process_music[:process_limit]\n",
    "    files_to_process_subset = files_to_process_music # Process all files\n",
    "\n",
    "\n",
    "    for i, file_path in enumerate(files_to_process_subset):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processing file {i + 1}/{len(files_to_process_subset)}\")\n",
    "\n",
    "        # Extract MFCC features for the musical input/output\n",
    "        mfccs_only = extract_mfcc_features_only(file_path, max_padding, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract combined features for the audio conditioning input\n",
    "        combined_features = extract_combined_features(file_path, max_padding, n_mfcc=n_mfcc, n_chroma=n_chroma)\n",
    "\n",
    "\n",
    "        if mfccs_only is not None and combined_features is not None:\n",
    "            X_train_music_features.append(mfccs_only)\n",
    "            X_train_audio_conditioning_features.append(combined_features)\n",
    "\n",
    "\n",
    "    # Convert extracted features to numpy arrays\n",
    "    X_train_music_features = np.array(X_train_music_features)\n",
    "    X_train_audio_conditioning_features = np.array(X_train_audio_conditioning_features)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    # Ensure that the split is consistent for both sets of features\n",
    "    if len(X_train_music_features) > 0:\n",
    "        X_train_music, X_val_music, X_train_audio_cond, X_val_audio_cond = train_test_split(\n",
    "            X_train_music_features, X_train_audio_conditioning_features, test_size=0.2, random_state=42\n",
    "        )\n",
    "    else:\n",
    "        print(\"No features were successfully extracted. Cannot split data.\")\n",
    "        # Initialize empty arrays to prevent errors in subsequent steps\n",
    "        X_train_music = np.array([])\n",
    "        X_val_music = np.array([])\n",
    "        X_train_audio_cond = np.array([])\n",
    "        X_val_audio_cond = np.array([])\n",
    "\n",
    "\n",
    "    print(\"\\nFeature extraction and data splitting complete.\")\n",
    "    print(f\"Shape of training musical features (MFCCs): {X_train_music.shape}\")\n",
    "    print(f\"Shape of validation musical features (MFCCs): {X_val_music.shape}\")\n",
    "    print(f\"Shape of training audio conditioning features (MFCC+Chroma): {X_train_audio_cond.shape}\")\n",
    "    print(f\"Shape of validation audio conditioning features (MFCC+Chroma): {X_val_audio_cond.shape}\")\n",
    "\n",
    "    # Create dummy text conditioning data for training\n",
    "    # This data is not based on actual text input yet, just for model training setup\n",
    "    # It should match the expected shape of the text_conditioning_input layer\n",
    "    dummy_text_conditioning_train = np.random.rand(X_train_music.shape[0], text_conditioning_dim)\n",
    "    dummy_text_conditioning_val = np.random.rand(X_val_music.shape[0], text_conditioning_dim)\n",
    "\n",
    "\n",
    "    print(f\"\\nShape of dummy training text conditioning data: {dummy_text_conditioning_train.shape}\")\n",
    "    print(f\"Shape of dummy validation text conditioning data: {dummy_text_conditioning_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "da1feec2",
    "outputId": "aeb0dd20-3fdb-4881-cda0-f39876918b63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the conditioned GRU music generation model (v2)...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_gen_conditioned_v2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-414881948.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train the conditioned GRU generation model (v2) with multiple inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTraining the conditioned GRU music generation model (v2)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m history_gen_conditioned_v2 = model_gen_conditioned_v2.fit(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mX_train_music\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_text_conditioning_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_audio_cond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Inputs: [musical_input, text_conditioning_input, audio_conditioning_input]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mX_train_music\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Target output: musical features (MFCCs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_gen_conditioned_v2' is not defined"
     ]
    }
   ],
   "source": [
    "# Assume the necessary variables (model_gen_conditioned_v2, X_train_music, X_val_music, dummy_text_conditioning_train, dummy_text_conditioning_val, X_train_audio_cond, X_val_audio_cond) are defined from the previous step.\n",
    "\n",
    "# Train the conditioned GRU generation model (v2) with multiple inputs\n",
    "print(\"\\nTraining the conditioned GRU music generation model (v2)...\")\n",
    "history_gen_conditioned_v2 = model_gen_conditioned_v2.fit(\n",
    "    [X_train_music, dummy_text_conditioning_train, X_train_audio_cond], # Inputs: [musical_input, text_conditioning_input, audio_conditioning_input]\n",
    "    X_train_music, # Target output: musical features (MFCCs)\n",
    "    epochs=50, # You might need to adjust epochs based on convergence\n",
    "    batch_size=32,\n",
    "    validation_data=([X_val_music, dummy_text_conditioning_val, X_val_audio_cond], X_val_music) # Validation data\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "9a74da6d",
    "outputId": "29fbc76c-7954-4cba-b9c1-89091702f800"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (ipython-input-1339703810.py, line 53)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1339703810.py\"\u001b[0;36m, line \u001b[0;32m53\u001b[0m\n\u001b[0;31m    gru_2 = Bidirectional(GRU(128, activation='relu', return_sequences=True)(dropout_1)\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, GRU, Input, TimeDistributed, Bidirectional\n",
    "import kagglehub\n",
    "\n",
    "# Define sampling rate (needed for audio processing and saving)\n",
    "sr = 22050 # Common sampling rate, adjust if your audio files use a different rate\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This needs to be consistent with feature extraction\n",
    "n_chroma = 12 # Number of chroma bins\n",
    "total_audio_features_dim = n_mfcc + n_chroma # Total features for audio conditioning\n",
    "\n",
    "# Define the dimensions for the music generation model\n",
    "musical_feature_dim = n_mfcc # The model output is MFCCs\n",
    "musical_sequence_length = max_padding # The model input sequence length\n",
    "\n",
    "# Define the conditioning input dimensions\n",
    "text_conditioning_dim = 128 # Dimension of the text embedding\n",
    "\n",
    "# --- Redefine the Conditional Music Generation Model to accept Sequential Audio Conditioning ---\n",
    "\n",
    "# Input 1: Musical features (MFCCs) - the sequence we are building upon\n",
    "musical_input = Input(shape=(musical_sequence_length, musical_feature_dim), name='musical_input')\n",
    "\n",
    "# Input 2: Text conditioning input (single vector)\n",
    "text_conditioning_input = Input(shape=(text_conditioning_dim,), name='text_conditioning_input')\n",
    "\n",
    "# Input 3: Audio conditioning input (sequence of features)\n",
    "audio_conditioning_input = Input(shape=(musical_sequence_length, total_audio_features_dim), name='audio_conditioning_input')\n",
    "\n",
    "\n",
    "# Process text conditioning (repeat the single vector across the musical sequence length)\n",
    "repeated_text_conditioning = tf.keras.layers.RepeatVector(musical_sequence_length)(text_conditioning_input)\n",
    "\n",
    "# Concatenate musical input with repeated text conditioning and audio conditioning input\n",
    "# This is one way to combine them. Another would be separate processing branches.\n",
    "# Concatenate [musical_input, repeated_text_conditioning, audio_conditioning_input]\n",
    "combined_input = Concatenate(axis=-1)([musical_input, repeated_text_conditioning, audio_conditioning_input])\n",
    "\n",
    "\n",
    "# GRU layers for the sequence generation, now taking the combined input\n",
    "# We can use Bidirectional GRU for potentially better context\n",
    "gru_1 = Bidirectional(GRU(128, activation='relu', return_sequences=True))(combined_input)\n",
    "dropout_1 = Dropout(0.2)(gru_1)\n",
    "\n",
    "gru_2 = Bidirectional(GRU(128, activation='relu', return_sequences=True)(dropout_1)\n",
    "dropout_2 = Dropout(0.2)(gru_2)\n",
    "\n",
    "# TimeDistributed Dense layer to output the generated musical features (MFCCs) for each time step\n",
    "# The output still needs to match the musical_feature_dim\n",
    "output_layer = TimeDistributed(Dense(musical_feature_dim, activation='linear'))(dropout_2)\n",
    "\n",
    "# Define the new conditioned music generation model\n",
    "# The model now has three inputs: musical_input, text_conditioning_input, audio_conditioning_input\n",
    "model_gen_conditioned_v2 = Model(inputs=[musical_input, text_conditioning_input, audio_conditioning_input], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=1.0) # Add gradient clipping\n",
    "model_gen_conditioned_v2.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "print(\"\\nNew conditional music generation model (v2) architecture defined.\")\n",
    "\n",
    "# --- Data Preparation ---\n",
    "# Download the dataset and directly assign the path within this cell\n",
    "print(\"Downloading the dataset to ensure path is available...\")\n",
    "try:\n",
    "    downloaded_dataset_root = kagglehub.dataset_download(\"soumendraprasad/musical-instruments-sound-dataset\")\n",
    "    print(f\"Dataset path: {downloaded_dataset_root}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    downloaded_dataset_root = None # Set to None if download fails\n",
    "\n",
    "\n",
    "if downloaded_dataset_root is None or not os.path.exists(downloaded_dataset_root):\n",
    "    print(\"Failed to get the dataset path. Cannot proceed with data preparation.\")\n",
    "else:\n",
    "    # Define feature extraction parameters (should be consistent with model definition)\n",
    "    n_mfcc = 20\n",
    "    max_padding = 174\n",
    "    n_chroma = 12 # Although not used for musical features output, needed for audio conditioning feature extraction\n",
    "    total_audio_features_dim = n_mfcc + n_chroma\n",
    "    text_conditioning_dim = 128\n",
    "\n",
    "    # Define helper functions within this cell\n",
    "    def extract_mfcc_features_only(file_path, target_sequence_length, n_mfcc=20):\n",
    "        \"\"\"Extract only MFCC features from an audio file.\"\"\"\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "            # Pad or truncate MFCCs to a fixed length\n",
    "            if mfccs.shape[1] < target_sequence_length:\n",
    "                mfccs = np.pad(mfccs, ((0, 0), (0, target_sequence_length - mfccs.shape[1])), mode='constant')\n",
    "            else:\n",
    "                mfccs = mfccs[:, :target_sequence_length]\n",
    "\n",
    "            return mfccs.T # Transpose to have shape (time steps, n_mfcc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "        \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "            # Extract MFCC features\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "            # Extract Chroma features\n",
    "            chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "            # Concatenate features\n",
    "            combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "            # Pad or truncate combined features to a fixed length\n",
    "            if combined_features.shape[1] < target_sequence_length:\n",
    "                combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "            else:\n",
    "                combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "            return combined_features.T # Transpose to have shape (time steps, total_features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def process_user_audio_for_conditioning(audio_file_path, target_sequence_length, n_mfcc, n_chroma):\n",
    "        \"\"\"Processes user audio input and returns features suitable for conditioning.\n",
    "\n",
    "        Args:\n",
    "            audio_file_path: Path to the user's audio file.\n",
    "            target_sequence_length: The expected sequence length for model input (e.g., max_padding).\n",
    "            n_mfcc: The number of MFCCs to extract.\n",
    "            n_chroma: The number of chroma bins to extract.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            A numpy array containing the processed audio features (shape: (1, target_sequence_length, total_feature_dimension)),\n",
    "            or None if processing fails.\n",
    "        \"\"\"\n",
    "        processed_features = extract_features(audio_file_path, target_sequence_length, n_mfcc, n_chroma)\n",
    "\n",
    "        if processed_features is not None:\n",
    "            # Add a batch dimension\n",
    "            processed_features = processed_features[np.newaxis, :, :] # Shape (1, target_sequence_length, total_feature_dimension)\n",
    "            # print(f\"Processed user audio features shape for conditioning: {processed_features.shape}\") # Debug print\n",
    "            return processed_features\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Function to extract combined features (MFCC + Chroma) for audio conditioning data\n",
    "    def extract_combined_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "        \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "            # Extract MFCC features\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "            # Extract Chroma features\n",
    "            chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "            # Concatenate features\n",
    "            combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "            # Pad or truncate combined features to a fixed length\n",
    "            if combined_features.shape[1] < target_sequence_length:\n",
    "                combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "            else:\n",
    "                combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "            return combined_features.T # Transpose to have shape (time steps, total_features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    # Collect music instrument files from the downloaded dataset\n",
    "    music_instrument_files = []\n",
    "\n",
    "    # Assuming the structure from the previous file listing\n",
    "    train_submission_path = os.path.join(downloaded_dataset_root, 'Train_submission', 'Train_submission')\n",
    "    test_submission_path = os.path.join(downloaded_dataset_root, 'Test_submission', 'Test_submission') # Assuming Test_submission is also relevant for data\n",
    "\n",
    "    # Collect files from Train_submission\n",
    "    if os.path.exists(train_submission_path):\n",
    "        print(f\"Searching for music instrument files in: {train_submission_path}\")\n",
    "        for root, dirs, files in os.walk(train_submission_path):\n",
    "            for name in files:\n",
    "                if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                     # Optional: further filter by keywords if needed\n",
    "                    if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                         music_instrument_files.append(os.path.join(root, name))\n",
    "        print(f\"Found {len(music_instrument_files)} music instrument files in training submission.\")\n",
    "    else:\n",
    "        print(f\"Warning: Training submission directory not found at {train_submission_path}\")\n",
    "\n",
    "\n",
    "    # Collect files from Test_submission (if it exists and contains relevant audio)\n",
    "    # You might decide to use test data only for evaluation later, but for now, let's include relevant audio\n",
    "    if os.path.exists(test_submission_path):\n",
    "         print(f\"Searching for music instrument files in: {test_submission_path}\")\n",
    "         for root, dirs, files in os.walk(test_submission_path):\n",
    "             for name in files:\n",
    "                 if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                      # Optional: further filter by keywords if needed\n",
    "                     if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                          music_instrument_files.append(os.path.join(root, name))\n",
    "         print(f\"Found {len(music_instrument_files)} music instrument files in test submission.\")\n",
    "    else:\n",
    "         print(f\"Warning: Test submission directory not found at {test_submission_path}\")\n",
    "\n",
    "\n",
    "    # Use the collected music instrument files for feature extraction\n",
    "    files_to_process_music = music_instrument_files\n",
    "\n",
    "    # Prepare lists to store extracted features for training\n",
    "    X_train_music_features = [] # For musical input (MFCCs only)\n",
    "    X_train_audio_conditioning_features = [] # For audio conditioning input (MFCC + Chroma)\n",
    "\n",
    "\n",
    "    print(\"\\nExtracting musical features (MFCCs) and audio conditioning features (MFCC+Chroma) from music instrument files...\")\n",
    "\n",
    "    # Process a subset of files for faster testing, or all files for full training\n",
    "    # process_limit = 100 # Set a limit for testing\n",
    "    # files_to_process_subset = files_to_process_music[:process_limit]\n",
    "    files_to_process_subset = files_to_process_music # Process all files\n",
    "\n",
    "\n",
    "    for i, file_path in enumerate(files_to_process_subset):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processing file {i + 1}/{len(files_to_process_subset)}\")\n",
    "\n",
    "        # Extract MFCC features for the musical input/output\n",
    "        mfccs_only = extract_mfcc_features_only(file_path, max_padding, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract combined features for the audio conditioning input\n",
    "        combined_features = extract_combined_features(file_path, max_padding, n_mfcc=n_mfcc, n_chroma=n_chroma)\n",
    "\n",
    "\n",
    "        if mfccs_only is not None and combined_features is not None:\n",
    "            X_train_music_features.append(mfccs_only)\n",
    "            X_train_audio_conditioning_features.append(combined_features)\n",
    "\n",
    "\n",
    "    # Convert extracted features to numpy arrays\n",
    "    X_train_music_features = np.array(X_train_music_features)\n",
    "    X_train_audio_conditioning_features = np.array(X_train_audio_conditioning_features)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    # Ensure that the split is consistent for both sets of features\n",
    "    if len(X_train_music_features) > 0:\n",
    "        X_train_music, X_val_music, X_train_audio_cond, X_val_audio_cond = train_test_split(\n",
    "            X_train_music_features, X_train_audio_conditioning_features, test_size=0.2, random_state=42\n",
    "        )\n",
    "    else:\n",
    "        print(\"No features were successfully extracted. Cannot split data.\")\n",
    "        # Initialize empty arrays to prevent errors in subsequent steps\n",
    "        X_train_music = np.array([])\n",
    "        X_val_music = np.array([])\n",
    "        X_train_audio_cond = np.array([])\n",
    "        X_val_audio_cond = np.array([])\n",
    "\n",
    "\n",
    "    print(\"\\nFeature extraction and data splitting complete.\")\n",
    "    print(f\"Shape of training musical features (MFCCs): {X_train_music.shape}\")\n",
    "    print(f\"Shape of validation musical features (MFCCs): {X_val_music.shape}\")\n",
    "    print(f\"Shape of training audio conditioning features (MFCC+Chroma): {X_train_audio_cond.shape}\")\n",
    "    print(f\"Shape of validation audio conditioning features (MFCC+Chroma): {X_val_audio_cond.shape}\")\n",
    "\n",
    "    # Create dummy text conditioning data for training\n",
    "    # This data is not based on actual text input yet, just for model training setup\n",
    "    # It should match the expected shape of the text_conditioning_input layer\n",
    "    dummy_text_conditioning_train = np.random.rand(X_train_music.shape[0], text_conditioning_dim)\n",
    "    dummy_text_conditioning_val = np.random.rand(X_val_music.shape[0], text_conditioning_dim)\n",
    "\n",
    "\n",
    "    print(f\"\\nShape of dummy training text conditioning data: {dummy_text_conditioning_train.shape}\")\n",
    "    print(f\"Shape of dummy validation text conditioning data: {dummy_text_conditioning_val.shape}\")\n",
    "\n",
    "\n",
    "    # --- Train the Model ---\n",
    "    # Assume the necessary variables (model_gen_conditioned_v2, X_train_music, X_val_music, dummy_text_conditioning_train, dummy_text_conditioning_val, X_train_audio_cond, X_val_audio_cond) are defined from the previous step.\n",
    "\n",
    "    # Train the conditioned GRU generation model (v2) with multiple inputs\n",
    "    print(\"\\nTraining the conditioned GRU music generation model (v2)...\")\n",
    "    # Check if training data is available before training\n",
    "    if X_train_music.shape[0] > 0:\n",
    "        history_gen_conditioned_v2 = model_gen_conditioned_v2.fit(\n",
    "            [X_train_music, dummy_text_conditioning_train, X_train_audio_cond], # Inputs: [musical_input, text_conditioning_input, audio_conditioning_input]\n",
    "            X_train_music, # Target output: musical features (MFCCs)\n",
    "            epochs=50, # You might need to adjust epochs based on convergence\n",
    "            batch_size=32,\n",
    "            validation_data=([X_val_music, dummy_text_conditioning_val, X_val_audio_cond], X_val_music) # Validation data\n",
    "        )\n",
    "\n",
    "        print(\"\\nTraining complete.\")\n",
    "    else:\n",
    "        print(\"Training data is empty. Skipping model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "313ed806",
    "outputId": "5184ce8d-119c-4836-bd70-14cc8e3a64a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New conditional music generation model (v2) architecture defined.\n",
      "Downloading the dataset to ensure path is available...\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/soumendraprasad/musical-instruments-sound-dataset?dataset_version_number=3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.40G/5.40G [01:06<00:00, 86.6MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: /root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3\n",
      "Searching for music instrument files in: /root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Train_submission/Train_submission\n",
      "Found 1965 music instrument files in training submission.\n",
      "Searching for music instrument files in: /root/.cache/kagglehub/datasets/soumendraprasad/musical-instruments-sound-dataset/versions/3/Test_submission/Test_submission\n",
      "Found 2029 music instrument files in test submission.\n",
      "\n",
      "Extracting musical features (MFCCs) and audio conditioning features (MFCC+Chroma) from music instrument files...\n",
      "Processing file 100/2029\n",
      "Processing file 200/2029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 300/2029\n",
      "Processing file 400/2029\n",
      "Processing file 500/2029\n",
      "Processing file 600/2029\n",
      "Processing file 700/2029\n",
      "Processing file 800/2029\n",
      "Processing file 900/2029\n",
      "Processing file 1000/2029\n",
      "Processing file 1100/2029\n",
      "Processing file 1200/2029\n",
      "Processing file 1300/2029\n",
      "Processing file 1400/2029\n",
      "Processing file 1500/2029\n",
      "Processing file 1600/2029\n",
      "Processing file 1700/2029\n",
      "Processing file 1800/2029\n",
      "Processing file 1900/2029\n",
      "Processing file 2000/2029\n",
      "\n",
      "Feature extraction and data splitting complete.\n",
      "Shape of training musical features (MFCCs): (1623, 174, 20)\n",
      "Shape of validation musical features (MFCCs): (406, 174, 20)\n",
      "Shape of training audio conditioning features (MFCC+Chroma): (1623, 174, 32)\n",
      "Shape of validation audio conditioning features (MFCC+Chroma): (406, 174, 32)\n",
      "\n",
      "Shape of dummy training text conditioning data: (1623, 128)\n",
      "Shape of dummy validation text conditioning data: (406, 128)\n",
      "\n",
      "Training the conditioned GRU music generation model (v2)...\n",
      "Epoch 1/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 2s/step - loss: 5806.7993 - val_loss: 205.3537\n",
      "Epoch 2/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 2s/step - loss: 545.2823 - val_loss: 84.6018\n",
      "Epoch 3/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 2s/step - loss: 246.5953 - val_loss: 71.8022\n",
      "Epoch 4/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 2s/step - loss: 180.7016 - val_loss: 61.6561\n",
      "Epoch 5/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 2s/step - loss: 144.8722 - val_loss: 34.5995\n",
      "Epoch 6/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 2s/step - loss: 126.9815 - val_loss: 35.8066\n",
      "Epoch 7/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 2s/step - loss: 114.6899 - val_loss: 50.9071\n",
      "Epoch 8/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2s/step - loss: 107.5609 - val_loss: 57.8822\n",
      "Epoch 9/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 2s/step - loss: 102.8744 - val_loss: 36.5734\n",
      "Epoch 10/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - loss: 102.8882 - val_loss: 82.7132\n",
      "Epoch 11/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - loss: 107.4357 - val_loss: 28.5120\n",
      "Epoch 12/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - loss: 102.5879 - val_loss: 71.2333\n",
      "Epoch 13/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 2s/step - loss: 103.2741 - val_loss: 23.4019\n",
      "Epoch 14/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2s/step - loss: 101.6169 - val_loss: 75.3659\n",
      "Epoch 15/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - loss: 99.3915 - val_loss: 26.5471\n",
      "Epoch 16/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - loss: 98.2660 - val_loss: 99.5463\n",
      "Epoch 17/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 2s/step - loss: 98.5534 - val_loss: 21.6849\n",
      "Epoch 18/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 2s/step - loss: 94.8198 - val_loss: 72.9123\n",
      "Epoch 19/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - loss: 94.8672 - val_loss: 46.9487\n",
      "Epoch 20/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 2s/step - loss: 89.8654 - val_loss: 80.7640\n",
      "Epoch 21/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 2s/step - loss: 93.7507 - val_loss: 22.1473\n",
      "Epoch 22/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 2s/step - loss: 91.5537 - val_loss: 80.1118\n",
      "Epoch 23/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 2s/step - loss: 93.9793 - val_loss: 23.2725\n",
      "Epoch 24/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2s/step - loss: 92.4886 - val_loss: 74.0387\n",
      "Epoch 25/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 2s/step - loss: 90.9580 - val_loss: 19.6523\n",
      "Epoch 26/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - loss: 87.0418 - val_loss: 82.7075\n",
      "Epoch 27/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 2s/step - loss: 87.9112 - val_loss: 22.7138\n",
      "Epoch 28/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - loss: 87.5962 - val_loss: 72.7193\n",
      "Epoch 29/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 2s/step - loss: 85.2106 - val_loss: 18.3040\n",
      "Epoch 30/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 2s/step - loss: 87.8062 - val_loss: 74.2497\n",
      "Epoch 31/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 2s/step - loss: 86.1110 - val_loss: 21.4628\n",
      "Epoch 32/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 2s/step - loss: 84.6174 - val_loss: 74.6604\n",
      "Epoch 33/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - loss: 84.5441 - val_loss: 30.8567\n",
      "Epoch 34/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - loss: 76.8354 - val_loss: 74.6206\n",
      "Epoch 35/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - loss: 81.0500 - val_loss: 55.7475\n",
      "Epoch 36/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2s/step - loss: 82.8154 - val_loss: 23.7456\n",
      "Epoch 37/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - loss: 82.1476 - val_loss: 50.8614\n",
      "Epoch 38/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - loss: 80.7385 - val_loss: 21.6424\n",
      "Epoch 39/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 2s/step - loss: 79.2126 - val_loss: 61.1917\n",
      "Epoch 40/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - loss: 79.7109 - val_loss: 23.5985\n",
      "Epoch 41/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - loss: 78.2516 - val_loss: 63.2304\n",
      "Epoch 42/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - loss: 79.9208 - val_loss: 24.4845\n",
      "Epoch 43/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2s/step - loss: 77.9133 - val_loss: 37.4321\n",
      "Epoch 44/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - loss: 80.3769 - val_loss: 27.7904\n",
      "Epoch 45/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 2s/step - loss: 77.2429 - val_loss: 45.8867\n",
      "Epoch 46/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 2s/step - loss: 78.5147 - val_loss: 19.9002\n",
      "Epoch 47/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2s/step - loss: 77.6115 - val_loss: 48.2632\n",
      "Epoch 48/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2s/step - loss: 75.6513 - val_loss: 21.5994\n",
      "Epoch 49/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - loss: 79.4126 - val_loss: 55.1288\n",
      "Epoch 50/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - loss: 76.8091 - val_loss: 19.7694\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, GRU, Input, TimeDistributed, Bidirectional\n",
    "import kagglehub\n",
    "\n",
    "# Define sampling rate (needed for audio processing and saving)\n",
    "sr = 22050 # Common sampling rate, adjust if your audio files use a different rate\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This needs to be consistent with feature extraction\n",
    "n_chroma = 12 # Number of chroma bins\n",
    "total_audio_features_dim = n_mfcc + n_chroma # Total features for audio conditioning\n",
    "\n",
    "# Define the dimensions for the music generation model\n",
    "musical_feature_dim = n_mfcc # The model output is MFCCs\n",
    "musical_sequence_length = max_padding # The model input sequence length\n",
    "\n",
    "# Define the conditioning input dimensions\n",
    "text_conditioning_dim = 128 # Dimension of the text embedding\n",
    "\n",
    "# --- Redefine the Conditional Music Generation Model to accept Sequential Audio Conditioning ---\n",
    "\n",
    "# Input 1: Musical features (MFCCs) - the sequence we are building upon\n",
    "musical_input = Input(shape=(musical_sequence_length, musical_feature_dim), name='musical_input')\n",
    "\n",
    "# Input 2: Text conditioning input (single vector)\n",
    "text_conditioning_input = Input(shape=(text_conditioning_dim,), name='text_conditioning_input')\n",
    "\n",
    "# Input 3: Audio conditioning input (sequence of features)\n",
    "audio_conditioning_input = Input(shape=(musical_sequence_length, total_audio_features_dim), name='audio_conditioning_input')\n",
    "\n",
    "\n",
    "# Process text conditioning (repeat the single vector across the musical sequence length)\n",
    "repeated_text_conditioning = tf.keras.layers.RepeatVector(musical_sequence_length)(text_conditioning_input)\n",
    "\n",
    "# Concatenate musical input with repeated text conditioning and audio conditioning input\n",
    "# This is one way to combine them. Another would be separate processing branches.\n",
    "# Concatenate [musical_input, repeated_text_conditioning, audio_conditioning_input]\n",
    "combined_input = Concatenate(axis=-1)([musical_input, repeated_text_conditioning, audio_conditioning_input])\n",
    "\n",
    "\n",
    "# GRU layers for the sequence generation, now taking the combined input\n",
    "# We can use Bidirectional GRU for potentially better context\n",
    "gru_1 = Bidirectional(GRU(128, activation='relu', return_sequences=True))(combined_input)\n",
    "dropout_1 = Dropout(0.2)(gru_1)\n",
    "\n",
    "# Fix: Close the parenthesis for the second Bidirectional GRU layer\n",
    "gru_2 = Bidirectional(GRU(128, activation='relu', return_sequences=True))(dropout_1)\n",
    "dropout_2 = Dropout(0.2)(gru_2)\n",
    "\n",
    "# TimeDistributed Dense layer to output the generated musical features (MFCCs) for each time step\n",
    "# The output still needs to match the musical_feature_dim\n",
    "output_layer = TimeDistributed(Dense(musical_feature_dim, activation='linear'))(dropout_2)\n",
    "\n",
    "# Define the new conditioned music generation model\n",
    "# The model now has three inputs: musical_input, text_conditioning_input, audio_conditioning_input\n",
    "model_gen_conditioned_v2 = Model(inputs=[musical_input, text_conditioning_input, audio_conditioning_input], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=1.0) # Add gradient clipping\n",
    "model_gen_conditioned_v2.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "print(\"\\nNew conditional music generation model (v2) architecture defined.\")\n",
    "\n",
    "# --- Data Preparation ---\n",
    "# Download the dataset and directly assign the path within this cell\n",
    "print(\"Downloading the dataset to ensure path is available...\")\n",
    "try:\n",
    "    downloaded_dataset_root = kagglehub.dataset_download(\"soumendraprasad/musical-instruments-sound-dataset\")\n",
    "    print(f\"Dataset path: {downloaded_dataset_root}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    downloaded_dataset_root = None # Set to None if download fails\n",
    "\n",
    "\n",
    "if downloaded_dataset_root is None or not os.path.exists(downloaded_dataset_root):\n",
    "    print(\"Failed to get the dataset path. Cannot proceed with data preparation.\")\n",
    "else:\n",
    "    # Define feature extraction parameters (should be consistent with model definition)\n",
    "    n_mfcc = 20\n",
    "    max_padding = 174\n",
    "    n_chroma = 12 # Although not used for musical features output, needed for audio conditioning feature extraction\n",
    "    total_audio_features_dim = n_mfcc + n_chroma\n",
    "    text_conditioning_dim = 128\n",
    "\n",
    "    # Define helper functions within this cell\n",
    "    def extract_mfcc_features_only(file_path, target_sequence_length, n_mfcc=20):\n",
    "        \"\"\"Extract only MFCC features from an audio file.\"\"\"\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "            # Pad or truncate MFCCs to a fixed length\n",
    "            if mfccs.shape[1] < target_sequence_length:\n",
    "                mfccs = np.pad(mfccs, ((0, 0), (0, target_sequence_length - mfccs.shape[1])), mode='constant')\n",
    "            else:\n",
    "                mfccs = mfccs[:, :target_sequence_length]\n",
    "\n",
    "            return mfccs.T # Transpose to have shape (time steps, n_mfcc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "        \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "            # Extract MFCC features\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "            # Extract Chroma features\n",
    "            chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "            # Concatenate features\n",
    "            combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "            # Pad or truncate combined features to a fixed length\n",
    "            if combined_features.shape[1] < target_sequence_length:\n",
    "                combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "            else:\n",
    "                combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "            return combined_features.T # Transpose to have shape (time steps, total_features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def process_user_audio_for_conditioning(audio_file_path, target_sequence_length, n_mfcc, n_chroma):\n",
    "        \"\"\"Processes user audio input and returns features suitable for conditioning.\n",
    "\n",
    "        Args:\n",
    "            audio_file_path: Path to the user's audio file.\n",
    "            target_sequence_length: The expected sequence length for model input (e.g., max_padding).\n",
    "            n_mfcc: The number of MFCCs to extract.\n",
    "            n_chroma: The number of chroma bins to extract.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            A numpy array containing the processed audio features (shape: (1, target_sequence_length, total_feature_dimension)),\n",
    "            or None if processing fails.\n",
    "        \"\"\"\n",
    "        processed_features = extract_features(audio_file_path, target_sequence_length, n_mfcc, n_chroma)\n",
    "\n",
    "        if processed_features is not None:\n",
    "            # Add a batch dimension\n",
    "            processed_features = processed_features[np.newaxis, :, :] # Shape (1, target_sequence_length, total_feature_dimension)\n",
    "            # print(f\"Processed user audio features shape for conditioning: {processed_features.shape}\") # Debug print\n",
    "            return processed_features\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Function to extract combined features (MFCC + Chroma) for audio conditioning data\n",
    "    def extract_combined_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "        \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "            # Extract MFCC features\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "            # Extract Chroma features\n",
    "            chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "            # Concatenate features\n",
    "            combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "            # Pad or truncate combined features to a fixed length\n",
    "            if combined_features.shape[1] < target_sequence_length:\n",
    "                combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "            else:\n",
    "                combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "            return combined_features.T # Transpose to have shape (time steps, total_features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    # Collect music instrument files from the downloaded dataset\n",
    "    music_instrument_files = []\n",
    "\n",
    "    # Assuming the structure from the previous file listing\n",
    "    train_submission_path = os.path.join(downloaded_dataset_root, 'Train_submission', 'Train_submission')\n",
    "    test_submission_path = os.path.join(downloaded_dataset_root, 'Test_submission', 'Test_submission') # Assuming Test_submission is also relevant for data\n",
    "\n",
    "    # Collect files from Train_submission\n",
    "    if os.path.exists(train_submission_path):\n",
    "        print(f\"Searching for music instrument files in: {train_submission_path}\")\n",
    "        for root, dirs, files in os.walk(train_submission_path):\n",
    "            for name in files:\n",
    "                if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                     # Optional: further filter by keywords if needed\n",
    "                    if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                         music_instrument_files.append(os.path.join(root, name))\n",
    "        print(f\"Found {len(music_instrument_files)} music instrument files in training submission.\")\n",
    "    else:\n",
    "        print(f\"Warning: Training submission directory not found at {train_submission_path}\")\n",
    "\n",
    "\n",
    "    # Collect files from Test_submission (if it exists and contains relevant audio)\n",
    "    # You might decide to use test data only for evaluation later, but for now, let's include relevant audio\n",
    "    if os.path.exists(test_submission_path):\n",
    "         print(f\"Searching for music instrument files in: {test_submission_path}\")\n",
    "         for root, dirs, files in os.walk(test_submission_path):\n",
    "             for name in files:\n",
    "                 if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                      # Optional: further filter by keywords if needed\n",
    "                     if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                          music_instrument_files.append(os.path.join(root, name))\n",
    "         print(f\"Found {len(music_instrument_files)} music instrument files in test submission.\")\n",
    "    else:\n",
    "         print(f\"Warning: Test submission directory not found at {test_submission_path}\")\n",
    "\n",
    "\n",
    "    # Use the collected music instrument files for feature extraction\n",
    "    files_to_process_music = music_instrument_files\n",
    "\n",
    "    # Prepare lists to store extracted features for training\n",
    "    X_train_music_features = [] # For musical input (MFCCs only)\n",
    "    X_train_audio_conditioning_features = [] # For audio conditioning input (MFCC + Chroma)\n",
    "\n",
    "\n",
    "    print(\"\\nExtracting musical features (MFCCs) and audio conditioning features (MFCC+Chroma) from music instrument files...\")\n",
    "\n",
    "    # Process a subset of files for faster testing, or all files for full training\n",
    "    # process_limit = 100 # Set a limit for testing\n",
    "    # files_to_process_subset = files_to_process_music[:process_limit]\n",
    "    files_to_process_subset = files_to_process_music # Process all files\n",
    "\n",
    "\n",
    "    for i, file_path in enumerate(files_to_process_subset):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processing file {i + 1}/{len(files_to_process_subset)}\")\n",
    "\n",
    "        # Extract MFCC features for the musical input/output\n",
    "        mfccs_only = extract_mfcc_features_only(file_path, max_padding, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract combined features for the audio conditioning input\n",
    "        combined_features = extract_combined_features(file_path, max_padding, n_mfcc=n_mfcc, n_chroma=n_chroma)\n",
    "\n",
    "\n",
    "        if mfccs_only is not None and combined_features is not None:\n",
    "            X_train_music_features.append(mfccs_only)\n",
    "            X_train_audio_conditioning_features.append(combined_features)\n",
    "\n",
    "\n",
    "    # Convert extracted features to numpy arrays\n",
    "    X_train_music_features = np.array(X_train_music_features)\n",
    "    X_train_audio_conditioning_features = np.array(X_train_audio_conditioning_features)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    # Ensure that the split is consistent for both sets of features\n",
    "    if len(X_train_music_features) > 0:\n",
    "        X_train_music, X_val_music, X_train_audio_cond, X_val_audio_cond = train_test_split(\n",
    "            X_train_music_features, X_train_audio_conditioning_features, test_size=0.2, random_state=42\n",
    "        )\n",
    "    else:\n",
    "        print(\"No features were successfully extracted. Cannot split data.\")\n",
    "        # Initialize empty arrays to prevent errors in subsequent steps\n",
    "        X_train_music = np.array([])\n",
    "        X_val_music = np.array([])\n",
    "        X_train_audio_cond = np.array([])\n",
    "        X_val_audio_cond = np.array([])\n",
    "\n",
    "\n",
    "    print(\"\\nFeature extraction and data splitting complete.\")\n",
    "    print(f\"Shape of training musical features (MFCCs): {X_train_music.shape}\")\n",
    "    print(f\"Shape of validation musical features (MFCCs): {X_val_music.shape}\")\n",
    "    print(f\"Shape of training audio conditioning features (MFCC+Chroma): {X_train_audio_cond.shape}\")\n",
    "    print(f\"Shape of validation audio conditioning features (MFCC+Chroma): {X_val_audio_cond.shape}\")\n",
    "\n",
    "    # Create dummy text conditioning data for training\n",
    "    # This data is not based on actual text input yet, just for model training setup\n",
    "    # It should match the expected shape of the text_conditioning_input layer\n",
    "    dummy_text_conditioning_train = np.random.rand(X_train_music.shape[0], text_conditioning_dim)\n",
    "    dummy_text_conditioning_val = np.random.rand(X_val_music.shape[0], text_conditioning_dim)\n",
    "\n",
    "\n",
    "    print(f\"\\nShape of dummy training text conditioning data: {dummy_text_conditioning_train.shape}\")\n",
    "    print(f\"Shape of dummy validation text conditioning data: {dummy_text_conditioning_val.shape}\")\n",
    "\n",
    "\n",
    "    # --- Train the Model ---\n",
    "    # Assume the necessary variables (model_gen_conditioned_v2, X_train_music, X_val_music, dummy_text_conditioning_train, dummy_text_conditioning_val, X_train_audio_cond, X_val_audio_cond) are defined from the previous step.\n",
    "\n",
    "    # Train the conditioned GRU generation model (v2) with multiple inputs\n",
    "    print(\"\\nTraining the conditioned GRU music generation model (v2)...\")\n",
    "    # Check if training data is available before training\n",
    "    if X_train_music.shape[0] > 0:\n",
    "        history_gen_conditioned_v2 = model_gen_conditioned_v2.fit(\n",
    "            [X_train_music, dummy_text_conditioning_train, X_train_audio_cond], # Inputs: [musical_input, text_conditioning_input, audio_conditioning_input]\n",
    "            X_train_music, # Target output: musical features (MFCCs)\n",
    "            epochs=50, # You might need to adjust epochs based on convergence\n",
    "            batch_size=32,\n",
    "            validation_data=([X_val_music, dummy_text_conditioning_val, X_val_audio_cond], X_val_music) # Validation data\n",
    "        )\n",
    "\n",
    "        print(\"\\nTraining complete.\")\n",
    "    else:\n",
    "        print(\"Training data is empty. Skipping model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0a512420dd82497cb09eab8d64d2a710",
      "97238983f74e40cfb0acfe3fc2f92afa",
      "3ccd45912759433fa66b13b5ee5a67a2",
      "2dd2fe73ae0043aea468a401335af22c",
      "701652e466204b528cab623ce9b2bc1c",
      "494fed5eec8b4585b9f4722846e974fc",
      "d022ffa6674a4423904b6d802c10e314",
      "d0e1600b5ce44cc4881cc18295e9d384",
      "c836af3915ff4c2cb3ac3993688b4514",
      "a2efed31d3664fcbbd69a32a0192dfe1",
      "22eee17d2bd944a1b9a8ddcab04f423c",
      "d480740c7b96426ba93a713fca62a865",
      "0d94aa031b5a4b148e367b6b762aaa98",
      "83ecdc4613094e5497b3cbdcb9b6e6ff",
      "533175d1b6cd418a916a3d2c24b92dc8",
      "92a0f0d1fd3843ff89e122a66fe1900c",
      "6767dcba99514b2f853bee3874d78bee",
      "6da089567db84f008cf93c78e0443c78",
      "f430d59cac9f41bdb3dcc573735cc5fa",
      "a861d3d23be0474ca674651dc756a595",
      "7a48e37592504bbba41cc45b0f65e761",
      "d18e34fa041d498bab924fd7e452d957",
      "9cf7fa9e8a894dd59d99d5341874d805",
      "2dee51a948a3441a820b64bba6609684",
      "9a25a02d07254b3891f8ebc8d175d81a",
      "dedc8dae94534683935f5a231783a5c7",
      "df4cc0f35f3a4e0f8320665883a97ce1",
      "144cebf82e2c4810bc63b5a1e40f8605",
      "944e78aad84c43b6a9e36f7d93ad7297",
      "15f5b94956674dc5864603aaf8fe6667",
      "34822ce37b784ba291dba60866102646",
      "2d68dfecb10f42e492c9956ddc537872",
      "58cd1c8259ba40eb97dafb5f3b12edef",
      "86f183cc7da74c069fef1070eb59b20f",
      "fe1b6827154b420b98aad0e25feaca46",
      "6a492443c11b4b169a15057f0e849863",
      "f037303b0f3740df8bd39993c65f4b66",
      "74fbc1123bd24d6c9f51d00b4adf21ca",
      "a9beb80aa7c146c4acb84437c74c5fca",
      "7672737d811a425795541c07ddb4d029",
      "b7cce752f2d547289a03b2059540d381",
      "38f4c25749ad4805b0d0ce39b8bbf19f",
      "ee88080663d347f6a179f06f3c548ea2",
      "ba9d8c03a26a4189a5f8256e07e5423e",
      "2321c5982d6e4a87944d89be7930be29",
      "f750f99da7cf4aca8653d8699e52fbe1",
      "1ee6529c522c4ca49481e94878547c13",
      "dd8555ab0d2a44c5b670f0d34fb4dbaf",
      "5f2919ee5f854bd986986d837ffc11ae",
      "644340c0d95d40dab0a5e277310b4ac7",
      "7b6bfb3e9e9142f283c6c53fe79944ac",
      "794685dcca21481eae1256fa200cae38",
      "dfcaa960997f4dc79b7d953e77040f54",
      "0057a97fba33428e9d829a55109d4b10",
      "a4a6a6454ca6438b9a6e949965512ad9",
      "4bc897c2c50545abb72f0b78c54e07b2",
      "9b00babca830452e869147e2dc58b8e1",
      "a1053688f9b5430ba7164bae21b20024",
      "fe84f07b518c4ef698d311e3b5591268",
      "234cda9b94de42389c39600a987fb2bc",
      "f7cf7b32f6a64e0e97542cd2e9c11e0b",
      "2f20eb8c5d0b4299a232b68d18ff6f13",
      "6b303df587bd4b59aa018aeaff2b026a",
      "0e7ed1521d084284bd555ce93d7908d2",
      "119abf4555184de197fd303add32f5e2",
      "77037c6175884ef0a2b49dba372e50df",
      "f50f482a6db745dfa93ee1d2179cbc07",
      "0c1b942008e645239caafbfe5ff81696",
      "c25261865827403ca21330e1a4b95db3",
      "41952607fb0b485ea00e04dc3c9a5df5",
      "6b368b991b5d4dfb864c0bccaf2625e9",
      "93e85859ba4946b3b58526dbec628f74",
      "4dc803c84d134e4b9e0c3ac970b185d5",
      "5c8d681834af4784b80890b399e2020c",
      "c0a99efc40e4494cbcb4761cabc34ae8",
      "926cb7f54b7d4f22b53bef48d9391a76",
      "08564d2158f24551a0685e7825c7db3f"
     ]
    },
    "id": "6bc5a052",
    "outputId": "95fe59ca-0dbf-46fa-8e2f-df68a409200e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
      "  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
      "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
      "  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
      "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
      "  elif re.match('(flt)p?( \\(default\\))?$', token):\n",
      "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
      "  elif re.match('(dbl)p?( \\(default\\))?$', token):\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a512420dd82497cb09eab8d64d2a710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d480740c7b96426ba93a713fca62a865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf7fa9e8a894dd59d99d5341874d805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f183cc7da74c069fef1070eb59b20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2321c5982d6e4a87944d89be7930be29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc897c2c50545abb72f0b78c54e07b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50f482a6db745dfa93ee1d2179cbc07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generation pipeline (distilgpt2) initialized.\n",
      "Creating a dummy audio file at /content/dummy_user_audio.wav for demonstration.\n",
      "\n",
      "Attempting to generate music from text prompt and dummy audio: 'Generate a calm piano melody with a melancholic feel.'\n",
      "Generating lyrics for prompt: 'Generate a calm piano melody with a melancholic feel.' using DistilGPT-2.\n",
      "Lyric generation successful.\n",
      "Getting embedding for text: 'The only music I heard was an acoustic guitar and ...' using a placeholder model.\n",
      "Processing user audio input for conditioning.\n",
      "Processed user audio features shape for conditioning: (1, 174, 32)\n",
      "\n",
      "Generating new conditional musical features using v2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 943ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 829ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 570ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 602ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 855ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "Shape of the generated musical features: (374, 20)\n",
      "\n",
      "Converting generated musical features to audio...\n",
      "Saving generated conditional audio to generated_music_conditional.wav...\n",
      "Generated conditional audio saved successfully to generated_music_conditional.wav\n",
      "Converting generated_music_conditional.wav to generated_music_conditional.mp3...\n",
      "Conversion to MP3 successful. Output saved to generated_music_conditional.mp3\n",
      "Generated lyrics saved to generated_lyrics.txt\n",
      "\n",
      "Generated music file (MP3): generated_music_conditional.mp3\n",
      "Generated lyrics:\n",
      "The only music I heard was an acoustic guitar and a bass organ. The music was the bass piano, and the guitar. The bass piano sounded like a drum and a bass. I could hear the strings and the strings from the piano, and the guitar sounded like a coda. The piano sounds like a piano.\n",
      "The music was a little bit more mellow for the piano, but the tone was pretty mellow.\n",
      "The guitar sounded \"Amernish\" with a bit of melodic piano. The guitar sounded a little bit more mellow for the piano, but the tone was pretty mellow.\n",
      "The guitar sounded like a drum and a bass organ. The guitar sounded like a drum and a bass organ. The guitar sounded like a drum and a bass organ. The guitar sounded like a drum and a bass organ. The guitar sounded like a drum and a bass organ. The guitar sounded like a drum and a bass organ. The guitar sounded like a drum and a bass organ. The guitar sounded like a drum and a bass organ. The guitar sounded like a drum and a bass organ. The guitar sounded like a drum and a bass organ. The guitar sounded like a drum and a bass organ. The guitar sounded like a drum\n",
      "\n",
      "Attempting to generate music from provided lyrics and dummy audio: 'The stars are shining bright tonight, a peaceful, quiet scene.'\n",
      "Getting embedding for text: 'The stars are shining bright tonight, a peaceful, ...' using a placeholder model.\n",
      "Processing user audio input for conditioning.\n",
      "Processed user audio features shape for conditioning: (1, 174, 32)\n",
      "\n",
      "Generating new conditional musical features using v2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\n",
      "Shape of the generated musical features: (374, 20)\n",
      "\n",
      "Converting generated musical features to audio...\n",
      "Saving generated conditional audio to generated_music_conditional.wav...\n",
      "Generated conditional audio saved successfully to generated_music_conditional.wav\n",
      "Converting generated_music_conditional.wav to generated_music_conditional.mp3...\n",
      "Conversion to MP3 successful. Output saved to generated_music_conditional.mp3\n",
      "Generated lyrics saved to generated_lyrics.txt\n",
      "\n",
      "Generated music file (MP3) from lyrics: generated_music_conditional.mp3\n",
      "Provided lyrics:\n",
      "The stars are shining bright tonight, a peaceful, quiet scene.\n",
      "\n",
      "Attempting to generate music from dummy audio only:\n",
      "No text input provided. Using dummy text conditioning.\n",
      "Processing user audio input for conditioning.\n",
      "Processed user audio features shape for conditioning: (1, 174, 32)\n",
      "\n",
      "Generating new conditional musical features using v2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "Shape of the generated musical features: (374, 20)\n",
      "\n",
      "Converting generated musical features to audio...\n",
      "Saving generated conditional audio to generated_music_conditional.wav...\n",
      "Generated conditional audio saved successfully to generated_music_conditional.wav\n",
      "Converting generated_music_conditional.wav to generated_music_conditional.mp3...\n",
      "Conversion to MP3 successful. Output saved to generated_music_conditional.mp3\n",
      "\n",
      "Generated music file (MP3) from audio only: generated_music_conditional.mp3\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "from pydub import AudioSegment # Import AudioSegment for MP3 conversion\n",
    "from transformers import pipeline # Import pipeline for using pre-trained models\n",
    "\n",
    "# Define sampling rate (needed for audio processing and saving)\n",
    "sr = 22050 # Common sampling rate, adjust if your audio files use a different rate\n",
    "\n",
    "# Define feature extraction parameters (should be consistent with training)\n",
    "n_mfcc = 20\n",
    "max_padding = 174\n",
    "n_chroma = 12\n",
    "total_audio_features_dim = n_mfcc + n_chroma\n",
    "text_conditioning_dim = 128\n",
    "\n",
    "# Assuming model_gen_conditioned_v2 is trained and defined\n",
    "# Assuming X_train_music is defined and contains the musical features for starting sequences\n",
    "\n",
    "# --- Updated function to generate lyrics using a language model ---\n",
    "# Initialize the text generation pipeline (if not already initialized)\n",
    "if 'lyric_generator' not in globals() or lyric_generator is None:\n",
    "    try:\n",
    "        lyric_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "        print(\"Text generation pipeline (distilgpt2) initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing text generation pipeline: {e}\")\n",
    "        lyric_generator = None\n",
    "\n",
    "\n",
    "def generate_lyrics_with_language_model(prompt):\n",
    "    \"\"\"Generates lyrics based on a prompt using a language model.\"\"\"\n",
    "    if lyric_generator:\n",
    "        print(f\"Generating lyrics for prompt: '{prompt}' using DistilGPT-2.\")\n",
    "        try:\n",
    "            # Generate text with a reasonable length and without generating too many sequences\n",
    "            generated_text = lyric_generator(prompt, max_length=100, num_return_sequences=1, truncation=True)\n",
    "            lyrics = generated_text[0]['generated_text']\n",
    "            # Basic post-processing: remove the prompt from the generated text if it's repeated\n",
    "            if lyrics.startswith(prompt):\n",
    "                lyrics = lyrics[len(prompt):].strip()\n",
    "            print(\"Lyric generation successful.\")\n",
    "            return lyrics\n",
    "        except Exception as e:\n",
    "            print(f\"Error during lyric generation: {e}\")\n",
    "            return f\"Error generating lyrics for prompt: '{prompt}'\" # Return an error message\n",
    "    else:\n",
    "        print(\"Lyric generator not initialized. Cannot generate lyrics.\")\n",
    "        return f\"Cannot generate lyrics for prompt: '{prompt}' (generator not available)\"\n",
    "\n",
    "\n",
    "# Function to get text embeddings (placeholder)\n",
    "def get_text_embedding(text, embedding_dim):\n",
    "    \"\"\"Placeholder function to get a fixed-size embedding for text.\"\"\"\n",
    "    print(f\"Getting embedding for text: '{text[:50]}...' using a placeholder model.\")\n",
    "    return np.random.rand(1, embedding_dim) # Shape (1, embedding_dim)\n",
    "\n",
    "\n",
    "# Function to process text input\n",
    "def process_text_input(user_input, embedding_dim, is_prompt=False):\n",
    "    \"\"\"Processes user text input (lyrics or prompt) and returns a conditioning embedding.\"\"\"\n",
    "    if is_prompt:\n",
    "        lyrics = generate_lyrics_with_language_model(user_input)\n",
    "    else:\n",
    "        lyrics = user_input # User provided lyrics directly\n",
    "\n",
    "    conditioning_embedding = get_text_embedding(lyrics, embedding_dim)\n",
    "\n",
    "    return conditioning_embedding, lyrics\n",
    "\n",
    "\n",
    "# Function to extract combined features (MFCC + Chroma) for audio conditioning data\n",
    "def extract_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "    \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract Chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "        # Pad or truncate combined features to a fixed length\n",
    "        if combined_features.shape[1] < target_sequence_length:\n",
    "            combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "        else:\n",
    "            combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "        return combined_features.T # Transpose to have shape (time steps, total_features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to process user audio for conditioning\n",
    "def process_user_audio_for_conditioning(audio_file_path, target_sequence_length, n_mfcc, n_chroma):\n",
    "    \"\"\"Processes user audio input and returns features suitable for conditioning.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path: Path to the user's audio file.\n",
    "        target_sequence_length: The expected sequence length for model input (e.g., max_padding).\n",
    "        n_mfcc: The number of MFCCs to extract.\n",
    "        n_chroma: The number of chroma bins to extract.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the processed audio features (shape: (1, target_sequence_length, total_feature_dimension)),\n",
    "        or None if processing fails.\n",
    "    \"\"\"\n",
    "    processed_features = extract_features(audio_file_path, target_sequence_length, n_mfcc, n_chroma)\n",
    "\n",
    "    if processed_features is not None:\n",
    "        # Add a batch dimension\n",
    "        processed_features = processed_features[np.newaxis, :, :] # Shape (1, target_sequence_length, total_feature_dimension)\n",
    "        print(f\"Processed user audio features shape for conditioning: {processed_features.shape}\")\n",
    "        return processed_features\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Updated music generation function to use the new model (v2) with combined conditioning ---\n",
    "def generate_music_from_input_v2(model, user_text_input=None, user_audio_input_path=None, input_type='text_prompt', generation_steps=200, sr=22050, max_padding=174, n_mfcc=20, n_chroma=12, text_conditioning_dim=128):\n",
    "    \"\"\"Generates music based on user input (text and/or audio) using the v2 model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras conditional music generation model (v2).\n",
    "        user_text_input: The user's text input (string for lyrics or prompt).\n",
    "        user_audio_input_path: Path to the user's audio file.\n",
    "        input_type: Type of text input ('text_lyrics', 'text_prompt'). 'audio_only' is also possible if no text is provided.\n",
    "        generation_steps: Number of additional time steps to generate.\n",
    "        sr: Sampling rate for audio output.\n",
    "        max_padding: Target sequence length for padding/truncating features.\n",
    "        n_mfcc: Number of MFCCs extracted.\n",
    "        n_chroma: Number of chroma bins extracted.\n",
    "        text_conditioning_dim: Dimension of the text embedding.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the path to the generated audio file (MP3) and the generated lyrics (string),\n",
    "        or (None, None) if an error occurred.\n",
    "    \"\"\"\n",
    "    text_conditioning_input_for_model = None\n",
    "    audio_conditioning_input_for_model = None\n",
    "    generated_lyrics = None\n",
    "\n",
    "    # Process text input if provided\n",
    "    if user_text_input:\n",
    "        is_prompt = (input_type == 'text_prompt')\n",
    "        text_conditioning_input_for_model, generated_lyrics = process_text_input(user_text_input, text_conditioning_dim, is_prompt=is_prompt)\n",
    "        if text_conditioning_input_for_model is None:\n",
    "            print(\"Error processing text input for conditioning.\")\n",
    "            return None, None\n",
    "    else:\n",
    "        # If no text input, create a dummy text conditioning input (e.g., zeros or random)\n",
    "        print(\"No text input provided. Using dummy text conditioning.\")\n",
    "        text_conditioning_input_for_model = np.zeros((1, text_conditioning_dim)) # Using zeros as dummy\n",
    "\n",
    "\n",
    "    # Process audio input if provided\n",
    "    if user_audio_input_path and os.path.exists(user_audio_input_path):\n",
    "        print(\"Processing user audio input for conditioning.\")\n",
    "        audio_conditioning_input_for_model = process_user_audio_for_conditioning(user_audio_input_path, max_padding, n_mfcc, n_chroma)\n",
    "        if audio_conditioning_input_for_model is None:\n",
    "            print(\"Error processing user audio input for conditioning.\")\n",
    "            return None, None\n",
    "    else:\n",
    "        # If no audio input, create a dummy audio conditioning input (e.g., zeros)\n",
    "        print(\"No user audio file provided or found. Using dummy audio conditioning.\")\n",
    "        audio_conditioning_input_for_model = np.zeros((1, max_padding, total_audio_features_dim)) # Using zeros as dummy\n",
    "\n",
    "\n",
    "    # Ensure both conditioning inputs are available\n",
    "    if text_conditioning_input_for_model is None or audio_conditioning_input_for_model is None:\n",
    "         print(\"Missing conditioning input(s). Cannot proceed.\")\n",
    "         return None, None\n",
    "\n",
    "\n",
    "    # Generate a starting musical sequence (e.g., take a random sample from the training data)\n",
    "    if 'X_train_music' not in globals() or X_train_music.shape[0] == 0:\n",
    "         print(\"Training data (X_train_music) is not available or empty. Cannot generate a starting sequence.\")\n",
    "         return None, None\n",
    "\n",
    "\n",
    "    start_index = np.random.randint(0, X_train_music.shape[0])\n",
    "    start_sequence = X_train_music[start_index:start_index+1, :, :]\n",
    "\n",
    "\n",
    "    # Generate new musical features using the trained conditional GRU model (v2)\n",
    "    print(\"\\nGenerating new conditional musical features using v2 model...\")\n",
    "    # The model expects three inputs: [musical_input, text_conditioning_input, audio_conditioning_input]\n",
    "    generated_musical_features = generate_conditional_mfccs_v2(model, start_sequence, text_conditioning_input_for_model, audio_conditioning_input_for_model, generation_steps)\n",
    "\n",
    "\n",
    "    print(f\"Shape of the generated musical features: {generated_musical_features.shape}\")\n",
    "\n",
    "    # Convert the generated musical features (MFCCs) back to an audio time-series\n",
    "    print(\"\\nConverting generated musical features to audio...\")\n",
    "    # Assuming the model output is MFCCs\n",
    "    audio_time_series_conditional = librosa.feature.inverse.mfcc_to_audio(generated_musical_features.T, sr=sr)\n",
    "\n",
    "\n",
    "    # Define the output filename (WAV and MP3)\n",
    "    output_filename_wav = 'generated_music_conditional.wav'\n",
    "    output_filename_mp3 = 'generated_music_conditional.mp3'\n",
    "\n",
    "\n",
    "    # Save the generated audio to a WAV file first\n",
    "    print(f\"Saving generated conditional audio to {output_filename_wav}...\")\n",
    "    sf.write(output_filename_wav, audio_time_series_conditional, sr)\n",
    "    print(f\"Generated conditional audio saved successfully to {output_filename_wav}\")\n",
    "\n",
    "\n",
    "    # Convert the WAV file to MP3\n",
    "    print(f\"Converting {output_filename_wav} to {output_filename_mp3}...\")\n",
    "    try:\n",
    "        audio = AudioSegment.from_wav(output_filename_wav)\n",
    "        audio.export(output_filename_mp3, format=\"mp3\")\n",
    "        print(f\"Conversion to MP3 successful. Output saved to {output_filename_mp3}\")\n",
    "        # Optionally, remove the intermediate WAV file\n",
    "        # os.remove(output_filename_wav)\n",
    "        # print(f\"Removed intermediate WAV file: {output_filename_wav}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MP3 conversion: {e}\")\n",
    "        return None, generated_lyrics # Return None for audio path if conversion fails\n",
    "\n",
    "\n",
    "    # Optionally, save the generated lyrics to a text file if lyrics were generated\n",
    "    lyrics_output_filename = 'generated_lyrics.txt'\n",
    "    if generated_lyrics:\n",
    "        try:\n",
    "            with open(lyrics_output_filename, 'w') as f:\n",
    "                f.write(generated_lyrics)\n",
    "            print(f\"Generated lyrics saved to {lyrics_output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving generated lyrics to file: {e}\")\n",
    "\n",
    "\n",
    "    return output_filename_mp3, generated_lyrics # Return the path to the MP3 file and generated lyrics\n",
    "\n",
    "\n",
    "# --- Redefine generate_conditional_mfccs_v2 to work with the new model inputs ---\n",
    "def generate_conditional_mfccs_v2(model, start_sequence, text_conditioning_input, audio_conditioning_input, generation_steps):\n",
    "    \"\"\"Generates new MFCC sequences using the trained conditional model (v2).\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras conditional generation model (v2).\n",
    "        start_sequence: A numpy array representing the initial musical sequence\n",
    "                        (shape: (1, sequence_length, musical_feature_dim)).\n",
    "        text_conditioning_input: The text conditioning input (shape: (1, text_conditioning_dim)).\n",
    "        audio_conditioning_input: The audio conditioning input (shape: (1, sequence_length, total_audio_features_dim)).\n",
    "        generation_steps: The number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated musical MFCC sequence.\n",
    "    \"\"\"\n",
    "    generated_sequence = np.copy(start_sequence)\n",
    "    current_musical_sequence = np.copy(start_sequence)\n",
    "    sequence_length = start_sequence.shape[1] # Get the expected sequence length\n",
    "\n",
    "\n",
    "    # Assuming the model takes a list of inputs: [musical_input, text_conditioning_input, audio_conditioning_input]\n",
    "    model_inputs = [current_musical_sequence, text_conditioning_input, audio_conditioning_input]\n",
    "\n",
    "\n",
    "    for _ in range(generation_steps):\n",
    "        # Predict the next time step of musical features\n",
    "        predicted_mfcc = model.predict(model_inputs) # Model expects [musical_input, text_conditioning_input, audio_conditioning_input]\n",
    "\n",
    "\n",
    "        # Append the predicted MFCC to the generated sequence (only the new time step)\n",
    "        generated_sequence = np.append(generated_sequence, predicted_mfcc[:, -1:, :], axis=1)\n",
    "\n",
    "        # Update the current musical sequence by removing the oldest time step and adding the new one\n",
    "        # Ensure the sequence length remains constant\n",
    "        current_musical_sequence = np.append(current_musical_sequence[:, 1:, :], predicted_mfcc[:, -1:, :], axis=1)\n",
    "\n",
    "        # Note: For sequential audio conditioning, if the audio input influences\n",
    "        # the generation step-by-step, the `audio_conditioning_input` might\n",
    "        # need to be a sequence corresponding to the target output sequence.\n",
    "        # With the current model architecture (concatenating repeated text conditioning\n",
    "        # and the full audio conditioning sequence at the input), the audio_conditioning_input\n",
    "        # remains constant throughout the generation steps for a single generated sequence.\n",
    "        # If a more sophisticated sequential conditioning is needed, the model architecture\n",
    "        # and generation loop would need to be adjusted.\n",
    "\n",
    "\n",
    "    return generated_sequence.squeeze() # Remove the batch dimension\n",
    "\n",
    "# --- Example Usage (requires trained model_gen_conditioned_v2 and data) ---\n",
    "# Assume model_gen_conditioned_v2 is trained and X_train_music is defined.\n",
    "\n",
    "# Example 1: Generate music from a text prompt and dummy audio\n",
    "user_text_prompt = \"Generate a calm piano melody with a melancholic feel.\"\n",
    "dummy_audio_file_path = '/content/dummy_user_audio.wav' # Use the dummy audio file created previously or a real one\n",
    "\n",
    "# Create a dummy audio file if it doesn't exist (for demonstration)\n",
    "if not os.path.exists(dummy_audio_file_path):\n",
    "    print(f\"Creating a dummy audio file at {dummy_audio_file_path} for demonstration.\")\n",
    "    dummy_audio_ts = np.zeros(sr * 5) # 5 seconds of silence\n",
    "    sf.write(dummy_audio_file_path, dummy_audio_ts, sr)\n",
    "\n",
    "\n",
    "print(f\"\\nAttempting to generate music from text prompt and dummy audio: '{user_text_prompt}'\")\n",
    "generated_music_file, generated_lyrics_text = generate_music_from_input_v2(\n",
    "    model_gen_conditioned_v2,\n",
    "    user_text_input=user_text_prompt,\n",
    "    user_audio_input_path=dummy_audio_file_path,\n",
    "    input_type='text_prompt',\n",
    "    generation_steps=200,\n",
    "    sr=sr,\n",
    "    max_padding=max_padding,\n",
    "    n_mfcc=n_mfcc,\n",
    "    n_chroma=n_chroma,\n",
    "    text_conditioning_dim=text_conditioning_dim\n",
    ")\n",
    "\n",
    "if generated_music_file:\n",
    "    print(f\"\\nGenerated music file (MP3): {generated_music_file}\")\n",
    "    if generated_lyrics_text:\n",
    "        print(f\"Generated lyrics:\\n{generated_lyrics_text}\")\n",
    "else:\n",
    "    print(\"\\nMusic generation failed.\")\n",
    "\n",
    "# Example 2: Generate music from provided lyrics and dummy audio\n",
    "user_provided_lyrics = \"The stars are shining bright tonight, a peaceful, quiet scene.\"\n",
    "print(f\"\\nAttempting to generate music from provided lyrics and dummy audio: '{user_provided_lyrics}'\")\n",
    "generated_music_file_lyrics, generated_lyrics_text_provided = generate_music_from_input_v2(\n",
    "    model_gen_conditioned_v2,\n",
    "    user_text_input=user_provided_lyrics,\n",
    "    user_audio_input_path=dummy_audio_file_path,\n",
    "    input_type='text_lyrics',\n",
    "    generation_steps=200,\n",
    "    sr=sr,\n",
    "    max_padding=max_padding,\n",
    "    n_mfcc=n_mfcc,\n",
    "    n_chroma=n_chroma,\n",
    "    text_conditioning_dim=text_conditioning_dim\n",
    ")\n",
    "\n",
    "if generated_music_file_lyrics:\n",
    "    print(f\"\\nGenerated music file (MP3) from lyrics: {generated_music_file_lyrics}\")\n",
    "    # Note: generated_lyrics_text_provided will be the original provided lyrics here\n",
    "    print(f\"Provided lyrics:\\n{generated_lyrics_text_provided}\")\n",
    "else:\n",
    "    print(\"\\nMusic generation from provided lyrics failed.\")\n",
    "\n",
    "# Example 3: Generate music from dummy audio only (no text input)\n",
    "print(\"\\nAttempting to generate music from dummy audio only:\")\n",
    "generated_music_file_audio_only, _ = generate_music_from_input_v2(\n",
    "    model_gen_conditioned_v2,\n",
    "    user_text_input=None, # No text input\n",
    "    user_audio_input_path=dummy_audio_file_path,\n",
    "    input_type='audio_only', # Indicate audio only\n",
    "    generation_steps=200,\n",
    "    sr=sr,\n",
    "    max_padding=max_padding,\n",
    "    n_mfcc=n_mfcc,\n",
    "    n_chroma=n_chroma,\n",
    "    text_conditioning_dim=text_conditioning_dim\n",
    ")\n",
    "\n",
    "if generated_music_file_audio_only:\n",
    "    print(f\"\\nGenerated music file (MP3) from audio only: {generated_music_file_audio_only}\")\n",
    "else:\n",
    "    print(\"\\nMusic generation from audio only failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6d539cbd",
    "outputId": "3e6ff457-0631-4d40-f673-9e184b0a0271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New conditional music generation model (v2) architecture defined.\n",
      "Downloading the dataset to ensure path is available...\n",
      "Using Colab cache for faster access to the 'musical-instruments-sound-dataset' dataset.\n",
      "Dataset path: /kaggle/input/musical-instruments-sound-dataset\n",
      "Searching for music instrument files in: /kaggle/input/musical-instruments-sound-dataset/Train_submission/Train_submission\n",
      "Found 1965 music instrument files in training submission.\n",
      "Searching for music instrument files in: /kaggle/input/musical-instruments-sound-dataset/Test_submission/Test_submission\n",
      "Found 2029 music instrument files in test submission.\n",
      "\n",
      "Extracting musical features (MFCCs) and audio conditioning features (MFCC+Chroma) from music instrument files...\n",
      "Processing file 100/2029\n",
      "Processing file 200/2029\n",
      "Processing file 300/2029\n",
      "Processing file 400/2029\n",
      "Processing file 500/2029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 600/2029\n",
      "Processing file 700/2029\n",
      "Processing file 800/2029\n",
      "Processing file 900/2029\n",
      "Processing file 1000/2029\n",
      "Processing file 1100/2029\n",
      "Processing file 1200/2029\n",
      "Processing file 1300/2029\n",
      "Processing file 1400/2029\n",
      "Processing file 1500/2029\n",
      "Processing file 1600/2029\n",
      "Processing file 1700/2029\n",
      "Processing file 1800/2029\n",
      "Processing file 1900/2029\n",
      "Processing file 2000/2029\n",
      "\n",
      "Feature extraction and data splitting complete.\n",
      "Shape of training musical features (MFCCs): (1623, 174, 20)\n",
      "Shape of validation musical features (MFCCs): (406, 174, 20)\n",
      "Shape of training audio conditioning features (MFCC+Chroma): (1623, 174, 32)\n",
      "Shape of validation audio conditioning features (MFCC+Chroma): (406, 174, 32)\n",
      "\n",
      "Shape of dummy training text conditioning data: (1623, 128)\n",
      "Shape of dummy validation text conditioning data: (406, 128)\n",
      "\n",
      "Training the conditioned GRU music generation model (v2)...\n",
      "Epoch 1/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 2s/step - loss: 5071.5537 - val_loss: 241.5407\n",
      "Epoch 2/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 2s/step - loss: 528.6285 - val_loss: 90.3351\n",
      "Epoch 3/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 2s/step - loss: 245.2276 - val_loss: 51.9192\n",
      "Epoch 4/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 2s/step - loss: 171.8943 - val_loss: 70.9202\n",
      "Epoch 5/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2s/step - loss: 140.3204 - val_loss: 70.9649\n",
      "Epoch 6/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 2s/step - loss: 123.1015 - val_loss: 67.1558\n",
      "Epoch 7/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 2s/step - loss: 114.2762 - val_loss: 117.0508\n",
      "Epoch 8/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - loss: 114.7398 - val_loss: 29.6587\n",
      "Epoch 9/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - loss: 112.1449 - val_loss: 91.8717\n",
      "Epoch 10/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 2s/step - loss: 110.0717 - val_loss: 29.1946\n",
      "Epoch 11/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 2s/step - loss: 108.1475 - val_loss: 85.3470\n",
      "Epoch 12/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - loss: 104.0145 - val_loss: 27.8017\n",
      "Epoch 13/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 2s/step - loss: 104.6268 - val_loss: 85.0278\n",
      "Epoch 14/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 2s/step - loss: 102.7428 - val_loss: 24.5991\n",
      "Epoch 15/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 2s/step - loss: 101.0396 - val_loss: 91.7502\n",
      "Epoch 16/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 2s/step - loss: 98.8051 - val_loss: 29.7977\n",
      "Epoch 17/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 2s/step - loss: 99.0702 - val_loss: 88.5294\n",
      "Epoch 18/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2s/step - loss: 98.0557 - val_loss: 24.5690\n",
      "Epoch 19/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2s/step - loss: 94.1407 - val_loss: 55.0053\n",
      "Epoch 20/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 2s/step - loss: 86.7660 - val_loss: 34.0421\n",
      "Epoch 21/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - loss: 84.6685 - val_loss: 43.9668\n",
      "Epoch 22/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - loss: 84.0799 - val_loss: 85.2701\n",
      "Epoch 23/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 2s/step - loss: 89.8247 - val_loss: 20.2059\n",
      "Epoch 24/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 2s/step - loss: 90.1321 - val_loss: 86.5770\n",
      "Epoch 25/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - loss: 88.3589 - val_loss: 18.9500\n",
      "Epoch 26/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - loss: 87.4208 - val_loss: 73.8126\n",
      "Epoch 27/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 2s/step - loss: 86.8046 - val_loss: 20.4151\n",
      "Epoch 28/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 2s/step - loss: 84.1809 - val_loss: 35.9818\n",
      "Epoch 29/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - loss: 78.0142 - val_loss: 37.6438\n",
      "Epoch 30/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - loss: 77.0801 - val_loss: 56.9068\n",
      "Epoch 31/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 2s/step - loss: 82.6720 - val_loss: 21.4565\n",
      "Epoch 32/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2s/step - loss: 83.0553 - val_loss: 62.1324\n",
      "Epoch 33/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 2s/step - loss: 82.3745 - val_loss: 25.6947\n",
      "Epoch 34/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - loss: 82.4960 - val_loss: 40.8994\n",
      "Epoch 35/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - loss: 75.4431 - val_loss: 24.0493\n",
      "Epoch 36/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 2s/step - loss: 73.0749 - val_loss: 33.7514\n",
      "Epoch 37/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2s/step - loss: 71.8393 - val_loss: 40.3896\n",
      "Epoch 38/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 2s/step - loss: 72.4421 - val_loss: 26.4962\n",
      "Epoch 39/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - loss: 69.3256 - val_loss: 20.0025\n",
      "Epoch 40/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - loss: 68.9814 - val_loss: 27.7208\n",
      "Epoch 41/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 2s/step - loss: 71.3008 - val_loss: 50.2716\n",
      "Epoch 42/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 2s/step - loss: 70.8630 - val_loss: 47.8462\n",
      "Epoch 43/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 2s/step - loss: 74.7192 - val_loss: 67.9241\n",
      "Epoch 44/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - loss: 78.5327 - val_loss: 14.7653\n",
      "Epoch 45/50\n",
      "\u001b[1m42/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m14s\u001b[0m 2s/step - loss: 80.1361"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-610414205.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;31m# Check if training data is available before training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX_train_music\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         history_gen_conditioned_v2 = model_gen_conditioned_v2.fit(\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mX_train_music\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_text_conditioning_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_audio_cond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Inputs: [musical_input, text_conditioning_input, audio_conditioning_input]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0mX_train_music\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Target output: musical features (MFCCs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, GRU, Input, TimeDistributed, Bidirectional\n",
    "import kagglehub\n",
    "\n",
    "# Define sampling rate (needed for audio processing and saving)\n",
    "sr = 22050 # Common sampling rate, adjust if your audio files use a different rate\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This needs to be consistent with feature extraction\n",
    "n_chroma = 12 # Number of chroma bins\n",
    "total_audio_features_dim = n_mfcc + n_chroma # Total features for audio conditioning\n",
    "\n",
    "# Define the dimensions for the music generation model\n",
    "musical_feature_dim = n_mfcc # The model output is MFCCs\n",
    "musical_sequence_length = max_padding # The model input sequence length\n",
    "\n",
    "# Define the conditioning input dimensions\n",
    "text_conditioning_dim = 128 # Dimension of the text embedding\n",
    "\n",
    "# --- Redefine the Conditional Music Generation Model to accept Sequential Audio Conditioning ---\n",
    "\n",
    "# Input 1: Musical features (MFCCs) - the sequence we are building upon\n",
    "musical_input = Input(shape=(musical_sequence_length, musical_feature_dim), name='musical_input')\n",
    "\n",
    "# Input 2: Text conditioning input (single vector)\n",
    "text_conditioning_input = Input(shape=(text_conditioning_dim,), name='text_conditioning_input')\n",
    "\n",
    "# Input 3: Audio conditioning input (sequence of features)\n",
    "audio_conditioning_input = Input(shape=(musical_sequence_length, total_audio_features_dim), name='audio_conditioning_input')\n",
    "\n",
    "\n",
    "# Process text conditioning (repeat the single vector across the musical sequence length)\n",
    "repeated_text_conditioning = tf.keras.layers.RepeatVector(musical_sequence_length)(text_conditioning_input)\n",
    "\n",
    "# Concatenate musical input with repeated text conditioning and audio conditioning input\n",
    "# This is one way to combine them. Another would be separate processing branches.\n",
    "# Concatenate [musical_input, repeated_text_conditioning, audio_conditioning_input]\n",
    "combined_input = Concatenate(axis=-1)([musical_input, repeated_text_conditioning, audio_conditioning_input])\n",
    "\n",
    "\n",
    "# GRU layers for the sequence generation, now taking the combined input\n",
    "# We can use Bidirectional GRU for potentially better context\n",
    "gru_1 = Bidirectional(GRU(128, activation='relu', return_sequences=True))(combined_input)\n",
    "dropout_1 = Dropout(0.2)(gru_1)\n",
    "\n",
    "gru_2 = Bidirectional(GRU(128, activation='relu', return_sequences=True))(dropout_1)\n",
    "dropout_2 = Dropout(0.2)(gru_2)\n",
    "\n",
    "# TimeDistributed Dense layer to output the generated musical features (MFCCs) for each time step\n",
    "# The output still needs to match the musical_feature_dim\n",
    "output_layer = TimeDistributed(Dense(musical_feature_dim, activation='linear'))(dropout_2)\n",
    "\n",
    "# Define the new conditioned music generation model\n",
    "# The model now has three inputs: musical_input, text_conditioning_input, audio_conditioning_input\n",
    "model_gen_conditioned_v2 = Model(inputs=[musical_input, text_conditioning_input, audio_conditioning_input], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=1.0) # Add gradient clipping\n",
    "model_gen_conditioned_v2.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "print(\"\\nNew conditional music generation model (v2) architecture defined.\")\n",
    "\n",
    "# --- Data Preparation ---\n",
    "# Download the dataset and directly assign the path within this cell\n",
    "print(\"Downloading the dataset to ensure path is available...\")\n",
    "try:\n",
    "    downloaded_dataset_root = kagglehub.dataset_download(\"soumendraprasad/musical-instruments-sound-dataset\")\n",
    "    print(f\"Dataset path: {downloaded_dataset_root}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    downloaded_dataset_root = None # Set to None if download fails\n",
    "\n",
    "\n",
    "if downloaded_dataset_root is None or not os.path.exists(downloaded_dataset_root):\n",
    "    print(\"Failed to get the dataset path. Cannot proceed with data preparation.\")\n",
    "else:\n",
    "    # Define feature extraction parameters (should be consistent with model definition)\n",
    "    n_mfcc = 20\n",
    "    max_padding = 174\n",
    "    n_chroma = 12 # Although not used for musical features output, needed for audio conditioning feature extraction\n",
    "    total_audio_features_dim = n_mfcc + n_chroma\n",
    "    text_conditioning_dim = 128\n",
    "\n",
    "    # Define helper functions within this cell\n",
    "    def extract_mfcc_features_only(file_path, target_sequence_length, n_mfcc=20):\n",
    "        \"\"\"Extract only MFCC features from an audio file.\"\"\"\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "            # Pad or truncate MFCCs to a fixed length\n",
    "            if mfccs.shape[1] < target_sequence_length:\n",
    "                mfccs = np.pad(mfccs, ((0, 0), (0, target_sequence_length - mfccs.shape[1])), mode='constant')\n",
    "            else:\n",
    "                mfccs = mfccs[:, :target_sequence_length]\n",
    "\n",
    "            return mfccs.T # Transpose to have shape (time steps, n_mfcc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "        \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "            # Extract MFCC features\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "            # Extract Chroma features\n",
    "            chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "            # Concatenate features\n",
    "            combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "            # Pad or truncate combined features to a fixed length\n",
    "            if combined_features.shape[1] < target_sequence_length:\n",
    "                combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "            else:\n",
    "                combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "            return combined_features.T # Transpose to have shape (time steps, total_features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def process_user_audio_for_conditioning(audio_file_path, target_sequence_length, n_mfcc, n_chroma):\n",
    "        \"\"\"Processes user audio input and returns features suitable for conditioning.\n",
    "\n",
    "        Args:\n",
    "            audio_file_path: Path to the user's audio file.\n",
    "            target_sequence_length: The expected sequence length for model input (e.g., max_padding).\n",
    "            n_mfcc: The number of MFCCs to extract.\n",
    "            n_chroma: The number of chroma bins to extract.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            A numpy array containing the processed audio features (shape: (1, target_sequence_length, total_feature_dimension)),\n",
    "            or None if processing fails.\n",
    "        \"\"\"\n",
    "        processed_features = extract_features(audio_file_path, target_sequence_length, n_mfcc, n_chroma)\n",
    "\n",
    "        if processed_features is not None:\n",
    "            # Add a batch dimension\n",
    "            processed_features = processed_features[np.newaxis, :, :] # Shape (1, target_sequence_length, total_feature_dimension)\n",
    "            # print(f\"Processed user audio features shape for conditioning: {processed_features.shape}\") # Debug print\n",
    "            return processed_features\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Function to extract combined features (MFCC + Chroma) for audio conditioning data\n",
    "    def extract_combined_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "        \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "            # Extract MFCC features\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "            # Extract Chroma features\n",
    "            chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "            # Concatenate features\n",
    "            combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "            # Pad or truncate combined features to a fixed length\n",
    "            if combined_features.shape[1] < target_sequence_length:\n",
    "                combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "            else:\n",
    "                combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "            return combined_features.T # Transpose to have shape (time steps, total_features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    # Collect music instrument files from the downloaded dataset\n",
    "    music_instrument_files = []\n",
    "\n",
    "    # Assuming the structure from the previous file listing\n",
    "    train_submission_path = os.path.join(downloaded_dataset_root, 'Train_submission', 'Train_submission')\n",
    "    test_submission_path = os.path.join(downloaded_dataset_root, 'Test_submission', 'Test_submission') # Assuming Test_submission is also relevant for data\n",
    "\n",
    "    # Collect files from Train_submission\n",
    "    if os.path.exists(train_submission_path):\n",
    "        print(f\"Searching for music instrument files in: {train_submission_path}\")\n",
    "        for root, dirs, files in os.walk(train_submission_path):\n",
    "            for name in files:\n",
    "                if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                     # Optional: further filter by keywords if needed\n",
    "                    if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                         music_instrument_files.append(os.path.join(root, name))\n",
    "        print(f\"Found {len(music_instrument_files)} music instrument files in training submission.\")\n",
    "    else:\n",
    "        print(f\"Warning: Training submission directory not found at {train_submission_path}\")\n",
    "\n",
    "\n",
    "    # Collect files from Test_submission (if it exists and contains relevant audio)\n",
    "    # You might decide to use test data only for evaluation later, but for now, let's include relevant audio\n",
    "    if os.path.exists(test_submission_path):\n",
    "         print(f\"Searching for music instrument files in: {test_submission_path}\")\n",
    "         for root, dirs, files in os.walk(test_submission_path):\n",
    "             for name in files:\n",
    "                 if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                      # Optional: further filter by keywords if needed\n",
    "                     if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                          music_instrument_files.append(os.path.join(root, name))\n",
    "         print(f\"Found {len(music_instrument_files)} music instrument files in test submission.\")\n",
    "    else:\n",
    "         print(f\"Warning: Test submission directory not found at {test_submission_path}\")\n",
    "\n",
    "\n",
    "    # Use the collected music instrument files for feature extraction\n",
    "    files_to_process_music = music_instrument_files\n",
    "\n",
    "    # Prepare lists to store extracted features for training\n",
    "    X_train_music_features = [] # For musical input (MFCCs only)\n",
    "    X_train_audio_conditioning_features = [] # For audio conditioning input (MFCC + Chroma)\n",
    "\n",
    "\n",
    "    print(\"\\nExtracting musical features (MFCCs) and audio conditioning features (MFCC+Chroma) from music instrument files...\")\n",
    "\n",
    "    # Process a subset of files for faster testing, or all files for full training\n",
    "    # process_limit = 100 # Set a limit for testing\n",
    "    # files_to_process_subset = files_to_process_music[:process_limit]\n",
    "    files_to_process_subset = files_to_process_music # Process all files\n",
    "\n",
    "\n",
    "    for i, file_path in enumerate(files_to_process_subset):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processing file {i + 1}/{len(files_to_process_subset)}\")\n",
    "\n",
    "        # Extract MFCC features for the musical input/output\n",
    "        mfccs_only = extract_mfcc_features_only(file_path, max_padding, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract combined features for the audio conditioning input\n",
    "        combined_features = extract_combined_features(file_path, max_padding, n_mfcc=n_mfcc, n_chroma=n_chroma)\n",
    "\n",
    "\n",
    "        if mfccs_only is not None and combined_features is not None:\n",
    "            X_train_music_features.append(mfccs_only)\n",
    "            X_train_audio_conditioning_features.append(combined_features)\n",
    "\n",
    "\n",
    "    # Convert extracted features to numpy arrays\n",
    "    X_train_music_features = np.array(X_train_music_features)\n",
    "    X_train_audio_conditioning_features = np.array(X_train_audio_conditioning_features)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    # Ensure that the split is consistent for both sets of features\n",
    "    if len(X_train_music_features) > 0:\n",
    "        X_train_music, X_val_music, X_train_audio_cond, X_val_audio_cond = train_test_split(\n",
    "            X_train_music_features, X_train_audio_conditioning_features, test_size=0.2, random_state=42\n",
    "        )\n",
    "    else:\n",
    "        print(\"No features were successfully extracted. Cannot split data.\")\n",
    "        # Initialize empty arrays to prevent errors in subsequent steps\n",
    "        X_train_music = np.array([])\n",
    "        X_val_music = np.array([])\n",
    "        X_train_audio_cond = np.array([])\n",
    "        X_val_audio_cond = np.array([])\n",
    "\n",
    "\n",
    "    print(\"\\nFeature extraction and data splitting complete.\")\n",
    "    print(f\"Shape of training musical features (MFCCs): {X_train_music.shape}\")\n",
    "    print(f\"Shape of validation musical features (MFCCs): {X_val_music.shape}\")\n",
    "    print(f\"Shape of training audio conditioning features (MFCC+Chroma): {X_train_audio_cond.shape}\")\n",
    "    print(f\"Shape of validation audio conditioning features (MFCC+Chroma): {X_val_audio_cond.shape}\")\n",
    "\n",
    "    # Create dummy text conditioning data for training\n",
    "    # This data is not based on actual text input yet, just for model training setup\n",
    "    # It should match the expected shape of the text_conditioning_input layer\n",
    "    dummy_text_conditioning_train = np.random.rand(X_train_music.shape[0], text_conditioning_dim)\n",
    "    dummy_text_conditioning_val = np.random.rand(X_val_music.shape[0], text_conditioning_dim)\n",
    "\n",
    "\n",
    "    print(f\"\\nShape of dummy training text conditioning data: {dummy_text_conditioning_train.shape}\")\n",
    "    print(f\"Shape of dummy validation text conditioning data: {dummy_text_conditioning_val.shape}\")\n",
    "\n",
    "\n",
    "    # --- Train the Model ---\n",
    "    # Assume the necessary variables (model_gen_conditioned_v2, X_train_music, X_val_music, dummy_text_conditioning_train, dummy_text_conditioning_val, X_train_audio_cond, X_val_audio_cond) are defined from the previous step.\n",
    "\n",
    "    # Train the conditioned GRU generation model (v2) with multiple inputs\n",
    "    print(\"\\nTraining the conditioned GRU music generation model (v2)...\")\n",
    "    # Check if training data is available before training\n",
    "    if X_train_music.shape[0] > 0:\n",
    "        history_gen_conditioned_v2 = model_gen_conditioned_v2.fit(\n",
    "            [X_train_music, dummy_text_conditioning_train, X_train_audio_cond], # Inputs: [musical_input, text_conditioning_input, audio_conditioning_input]\n",
    "            X_train_music, # Target output: musical features (MFCCs)\n",
    "            epochs=50, # You might need to adjust epochs based on convergence\n",
    "            batch_size=32,\n",
    "            validation_data=([X_val_music, dummy_text_conditioning_val, X_val_audio_cond], X_val_music) # Validation data\n",
    "        )\n",
    "\n",
    "        print(\"\\nTraining complete.\")\n",
    "    else:\n",
    "        print(\"Training data is empty. Skipping model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "151a46b4",
    "outputId": "734bc5a2-7d10-4e40-c8e3-071bcd8a968a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to generate music from text prompt and dummy audio: 'Generate a calm piano melody with a melancholic feel.'\n",
      "Generating lyrics for prompt: 'Generate a calm piano melody with a melancholic feel.' using DistilGPT-2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyric generation successful.\n",
      "Getting embedding for text: '...' using a placeholder model.\n",
      "Processing user audio input for conditioning.\n",
      "Processed user audio features shape for conditioning: (1, 174, 32)\n",
      "\n",
      "Generating new conditional musical features using v2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 367ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "Shape of the generated musical features: (374, 20)\n",
      "\n",
      "Converting generated musical features to audio...\n",
      "Saving generated conditional audio to generated_music_conditional.wav...\n",
      "Generated conditional audio saved successfully to generated_music_conditional.wav\n",
      "Converting generated_music_conditional.wav to generated_music_conditional.mp3...\n",
      "Conversion to MP3 successful. Output saved to generated_music_conditional.mp3\n",
      "\n",
      "Generated music file (MP3): generated_music_conditional.mp3\n",
      "\n",
      "Attempting to generate music from provided lyrics and dummy audio: 'The stars are shining bright tonight, a peaceful, quiet scene.'\n",
      "Getting embedding for text: 'The stars are shining bright tonight, a peaceful, ...' using a placeholder model.\n",
      "Processing user audio input for conditioning.\n",
      "Processed user audio features shape for conditioning: (1, 174, 32)\n",
      "\n",
      "Generating new conditional musical features using v2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "Shape of the generated musical features: (374, 20)\n",
      "\n",
      "Converting generated musical features to audio...\n",
      "Saving generated conditional audio to generated_music_conditional.wav...\n",
      "Generated conditional audio saved successfully to generated_music_conditional.wav\n",
      "Converting generated_music_conditional.wav to generated_music_conditional.mp3...\n",
      "Conversion to MP3 successful. Output saved to generated_music_conditional.mp3\n",
      "Generated lyrics saved to generated_lyrics.txt\n",
      "\n",
      "Generated music file (MP3) from lyrics: generated_music_conditional.mp3\n",
      "Provided lyrics:\n",
      "The stars are shining bright tonight, a peaceful, quiet scene.\n",
      "\n",
      "Attempting to generate music from dummy audio only:\n",
      "No text input provided. Using dummy text conditioning.\n",
      "Processing user audio input for conditioning.\n",
      "Processed user audio features shape for conditioning: (1, 174, 32)\n",
      "\n",
      "Generating new conditional musical features using v2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "Shape of the generated musical features: (374, 20)\n",
      "\n",
      "Converting generated musical features to audio...\n",
      "Saving generated conditional audio to generated_music_conditional.wav...\n",
      "Generated conditional audio saved successfully to generated_music_conditional.wav\n",
      "Converting generated_music_conditional.wav to generated_music_conditional.mp3...\n",
      "Conversion to MP3 successful. Output saved to generated_music_conditional.mp3\n",
      "\n",
      "Generated music file (MP3) from audio only: generated_music_conditional.mp3\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "from pydub import AudioSegment # Import AudioSegment for MP3 conversion\n",
    "from transformers import pipeline # Import pipeline for using pre-trained models\n",
    "\n",
    "# Define sampling rate (needed for audio processing and saving)\n",
    "sr = 22050 # Common sampling rate, adjust if your audio files use a different rate\n",
    "\n",
    "# Define feature extraction parameters (should be consistent with training)\n",
    "n_mfcc = 20\n",
    "max_padding = 174\n",
    "n_chroma = 12\n",
    "total_audio_features_dim = n_mfcc + n_chroma\n",
    "text_conditioning_dim = 128\n",
    "\n",
    "# Assuming model_gen_conditioned_v2 is trained and defined\n",
    "# Assuming X_train_music is defined and contains the musical features for starting sequences\n",
    "\n",
    "# --- Updated function to generate lyrics using a language model ---\n",
    "# Initialize the text generation pipeline (if not already initialized)\n",
    "if 'lyric_generator' not in globals() or lyric_generator is None:\n",
    "    try:\n",
    "        lyric_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "        print(\"Text generation pipeline (distilgpt2) initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing text generation pipeline: {e}\")\n",
    "        lyric_generator = None\n",
    "\n",
    "\n",
    "def generate_lyrics_with_language_model(prompt):\n",
    "    \"\"\"Generates lyrics based on a prompt using a language model.\"\"\"\n",
    "    if lyric_generator:\n",
    "        print(f\"Generating lyrics for prompt: '{prompt}' using DistilGPT-2.\")\n",
    "        try:\n",
    "            # Generate text with a reasonable length and without generating too many sequences\n",
    "            generated_text = lyric_generator(prompt, max_length=100, num_return_sequences=1, truncation=True)\n",
    "            lyrics = generated_text[0]['generated_text']\n",
    "            # Basic post-processing: remove the prompt from the generated text if it's repeated\n",
    "            if lyrics.startswith(prompt):\n",
    "                lyrics = lyrics[len(prompt):].strip()\n",
    "            print(\"Lyric generation successful.\")\n",
    "            return lyrics\n",
    "        except Exception as e:\n",
    "            print(f\"Error during lyric generation: {e}\")\n",
    "            return f\"Error generating lyrics for prompt: '{prompt}'\" # Return an error message\n",
    "    else:\n",
    "        print(\"Lyric generator not initialized. Cannot generate lyrics.\")\n",
    "        return f\"Cannot generate lyrics for prompt: '{prompt}' (generator not available)\"\n",
    "\n",
    "\n",
    "# Function to get text embeddings (placeholder)\n",
    "def get_text_embedding(text, embedding_dim):\n",
    "    \"\"\"Placeholder function to get a fixed-size embedding for text.\"\"\"\n",
    "    print(f\"Getting embedding for text: '{text[:50]}...' using a placeholder model.\")\n",
    "    return np.random.rand(1, embedding_dim) # Shape (1, embedding_dim)\n",
    "\n",
    "\n",
    "# Function to process text input\n",
    "def process_text_input(user_input, embedding_dim, is_prompt=False):\n",
    "    \"\"\"Processes user text input (lyrics or prompt) and returns a conditioning embedding.\"\"\"\n",
    "    if is_prompt:\n",
    "        lyrics = generate_lyrics_with_language_model(user_input)\n",
    "    else:\n",
    "        lyrics = user_input # User provided lyrics directly\n",
    "\n",
    "    conditioning_embedding = get_text_embedding(lyrics, embedding_dim)\n",
    "\n",
    "    return conditioning_embedding, lyrics\n",
    "\n",
    "\n",
    "# Function to extract combined features (MFCC + Chroma) for audio conditioning data\n",
    "def extract_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "    \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract Chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "        # Pad or truncate combined features to a fixed length\n",
    "        if combined_features.shape[1] < target_sequence_length:\n",
    "            combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "        else:\n",
    "            combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "        return combined_features.T # Transpose to have shape (time steps, total_features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to process user audio for conditioning\n",
    "def process_user_audio_for_conditioning(audio_file_path, target_sequence_length, n_mfcc, n_chroma):\n",
    "    \"\"\"Processes user audio input and returns features suitable for conditioning.\n",
    "\n",
    "    Args:\n",
    "        audio_file_path: Path to the user's audio file.\n",
    "        target_sequence_length: The expected sequence length for model input (e.g., max_padding).\n",
    "        n_mfcc: The number of MFCCs to extract.\n",
    "        n_chroma: The number of chroma bins to extract.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the processed audio features (shape: (1, target_sequence_length, total_feature_dimension)),\n",
    "        or None if processing fails.\n",
    "    \"\"\"\n",
    "    processed_features = extract_features(audio_file_path, target_sequence_length, n_mfcc, n_chroma)\n",
    "\n",
    "    if processed_features is not None:\n",
    "        # Add a batch dimension\n",
    "        processed_features = processed_features[np.newaxis, :, :] # Shape (1, target_sequence_length, total_feature_dimension)\n",
    "        print(f\"Processed user audio features shape for conditioning: {processed_features.shape}\")\n",
    "        return processed_features\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Updated music generation function to use the new model (v2) with combined conditioning ---\n",
    "def generate_music_from_input_v2(model, user_text_input=None, user_audio_input_path=None, input_type='text_prompt', generation_steps=200, sr=22050, max_padding=174, n_mfcc=20, n_chroma=12, text_conditioning_dim=128):\n",
    "    \"\"\"Generates music based on user input (text and/or audio) using the v2 model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras conditional music generation model (v2).\n",
    "        user_text_input: The user's text input (string for lyrics or prompt).\n",
    "        user_audio_input_path: Path to the user's audio file.\n",
    "        input_type: Type of text input ('text_lyrics', 'text_prompt'). 'audio_only' is also possible if no text is provided.\n",
    "        generation_steps: Number of additional time steps to generate.\n",
    "        sr: Sampling rate for audio output.\n",
    "        max_padding: Target sequence length for padding/truncating features.\n",
    "        n_mfcc: Number of MFCCs extracted.\n",
    "        n_chroma: Number of chroma bins extracted.\n",
    "        text_conditioning_dim: Dimension of the text embedding.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the path to the generated audio file (MP3) and the generated lyrics (string),\n",
    "        or (None, None) if an error occurred.\n",
    "    \"\"\"\n",
    "    text_conditioning_input_for_model = None\n",
    "    audio_conditioning_input_for_model = None\n",
    "    generated_lyrics = None\n",
    "\n",
    "    # Process text input if provided\n",
    "    if user_text_input:\n",
    "        is_prompt = (input_type == 'text_prompt')\n",
    "        conditioning_input_for_model, generated_lyrics = process_text_input(user_text_input, text_conditioning_dim, is_prompt=is_prompt)\n",
    "        if conditioning_input_for_model is None:\n",
    "            print(\"Error processing text input for conditioning.\")\n",
    "            return None, None\n",
    "        # For text conditioning, repeat the single vector for each step if the model architecture requires it\n",
    "        # Our current model concatenates the repeated vector in the model definition, so we pass the single vector.\n",
    "        text_conditioning_input_for_model = conditioning_input_for_model # Assign to the correct variable\n",
    "\n",
    "    else:\n",
    "        # If no text input, create a dummy text conditioning input (e.g., zeros or random)\n",
    "        print(\"No text input provided. Using dummy text conditioning.\")\n",
    "        text_conditioning_input_for_model = np.zeros((1, text_conditioning_dim)) # Using zeros as dummy\n",
    "\n",
    "\n",
    "    # Process audio input if provided\n",
    "    if user_audio_input_path and os.path.exists(user_audio_input_path):\n",
    "        print(\"Processing user audio input for conditioning.\")\n",
    "        audio_conditioning_input_for_model = process_user_audio_for_conditioning(user_audio_input_path, max_padding, n_mfcc, n_chroma)\n",
    "        if audio_conditioning_input_for_model is None:\n",
    "            print(\"Error processing user audio input for conditioning.\")\n",
    "            return None, None\n",
    "    else:\n",
    "        # If no audio input, create a dummy audio conditioning input (e.g., zeros)\n",
    "        print(\"No user audio file provided or found. Using dummy audio conditioning.\")\n",
    "        audio_conditioning_input_for_model = np.zeros((1, max_padding, total_audio_features_dim)) # Using zeros as dummy\n",
    "\n",
    "\n",
    "    # Ensure both conditioning inputs are available\n",
    "    if text_conditioning_input_for_model is None or audio_conditioning_input_for_model is None:\n",
    "         print(\"Missing conditioning input(s). Cannot proceed.\")\n",
    "         return None, None\n",
    "\n",
    "\n",
    "    # Generate a starting musical sequence (e.g., take a random sample from the training data)\n",
    "    if 'X_train_music' not in globals() or X_train_music.shape[0] == 0:\n",
    "         print(\"Training data (X_train_music) is not available or empty. Cannot generate a starting sequence.\")\n",
    "         return None, None\n",
    "\n",
    "\n",
    "    start_index = np.random.randint(0, X_train_music.shape[0])\n",
    "    start_sequence = X_train_music[start_index:start_index+1, :, :]\n",
    "\n",
    "\n",
    "    # Generate new musical features using the trained conditional GRU model (v2)\n",
    "    print(\"\\nGenerating new conditional musical features using v2 model...\")\n",
    "    # The model expects three inputs: [musical_input, text_conditioning_input, audio_conditioning_input]\n",
    "    generated_musical_features = generate_conditional_mfccs_v2(model, start_sequence, text_conditioning_input_for_model, audio_conditioning_input_for_model, generation_steps)\n",
    "\n",
    "\n",
    "    print(f\"Shape of the generated musical features: {generated_musical_features.shape}\")\n",
    "\n",
    "    # Convert the generated musical features (MFCCs) back to an audio time-series\n",
    "    print(\"\\nConverting generated musical features to audio...\")\n",
    "    # Assuming the model output is MFCCs\n",
    "    audio_time_series_conditional = librosa.feature.inverse.mfcc_to_audio(generated_musical_features.T, sr=sr)\n",
    "\n",
    "\n",
    "    # Define the output filename (WAV and MP3)\n",
    "    output_filename_wav = 'generated_music_conditional.wav'\n",
    "    output_filename_mp3 = 'generated_music_conditional.mp3'\n",
    "\n",
    "\n",
    "    # Save the generated audio to a WAV file first\n",
    "    print(f\"Saving generated conditional audio to {output_filename_wav}...\")\n",
    "    sf.write(output_filename_wav, audio_time_series_conditional, sr)\n",
    "    print(f\"Generated conditional audio saved successfully to {output_filename_wav}\")\n",
    "\n",
    "\n",
    "    # Convert the WAV file to MP3\n",
    "    print(f\"Converting {output_filename_wav} to {output_filename_mp3}...\")\n",
    "    try:\n",
    "        audio = AudioSegment.from_wav(output_filename_wav)\n",
    "        audio.export(output_filename_mp3, format=\"mp3\")\n",
    "        print(f\"Conversion to MP3 successful. Output saved to {output_filename_mp3}\")\n",
    "        # Optionally, remove the intermediate WAV file\n",
    "        # os.remove(output_filename_wav)\n",
    "        # print(f\"Removed intermediate WAV file: {output_filename_wav}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MP3 conversion: {e}\")\n",
    "        return None, generated_lyrics # Return None for audio path if conversion fails\n",
    "\n",
    "\n",
    "    # Optionally, save the generated lyrics to a text file if lyrics were generated\n",
    "    lyrics_output_filename = 'generated_lyrics.txt'\n",
    "    if generated_lyrics:\n",
    "        try:\n",
    "            with open(lyrics_output_filename, 'w') as f:\n",
    "                f.write(generated_lyrics)\n",
    "            print(f\"Generated lyrics saved to {lyrics_output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving generated lyrics to file: {e}\")\n",
    "\n",
    "\n",
    "    return output_filename_mp3, generated_lyrics # Return the path to the MP3 file and generated lyrics\n",
    "\n",
    "\n",
    "# --- Redefine generate_conditional_mfccs_v2 to work with the new model inputs ---\n",
    "def generate_conditional_mfccs_v2(model, start_sequence, text_conditioning_input, audio_conditioning_input, generation_steps):\n",
    "    \"\"\"Generates new MFCC sequences using the trained conditional model (v2).\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras conditional generation model (v2).\n",
    "        start_sequence: A numpy array representing the initial musical sequence\n",
    "                        (shape: (1, sequence_length, musical_feature_dim)).\n",
    "        text_conditioning_input: The text conditioning input (shape: (1, text_conditioning_dim)).\n",
    "        audio_conditioning_input: The audio conditioning input (shape: (1, sequence_length, total_audio_features_dim)).\n",
    "        generation_steps: The number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated musical MFCC sequence.\n",
    "    \"\"\"\n",
    "    generated_sequence = np.copy(start_sequence)\n",
    "    current_musical_sequence = np.copy(start_sequence)\n",
    "    sequence_length = start_sequence.shape[1] # Get the expected sequence length\n",
    "\n",
    "\n",
    "    # Assuming the model takes a list of inputs: [musical_input, text_conditioning_input, audio_conditioning_input]\n",
    "    model_inputs = [current_musical_sequence, text_conditioning_input, audio_conditioning_input]\n",
    "\n",
    "\n",
    "    for _ in range(generation_steps):\n",
    "        # Predict the next time step of musical features\n",
    "        predicted_mfcc = model.predict(model_inputs) # Model expects [musical_input, text_conditioning_input, audio_conditioning_input]\n",
    "\n",
    "\n",
    "        # Append the predicted MFCC to the generated sequence (only the new time step)\n",
    "        generated_sequence = np.append(generated_sequence, predicted_mfcc[:, -1:, :], axis=1)\n",
    "\n",
    "        # Update the current musical sequence by removing the oldest time step and adding the new one\n",
    "        # Ensure the sequence length remains constant\n",
    "        current_musical_sequence = np.append(current_musical_sequence[:, 1:, :], predicted_mfcc[:, -1:, :], axis=1)\n",
    "\n",
    "        # Note: For sequential audio conditioning, if the audio input influences\n",
    "        # the generation step-by-step, the `audio_conditioning_input` might\n",
    "        # need to be a sequence corresponding to the target output sequence.\n",
    "        # With the current model architecture (concatenating repeated text conditioning\n",
    "        # and the full audio conditioning sequence at the input), the audio_conditioning_input\n",
    "        # remains constant throughout the generation steps for a single generated sequence.\n",
    "        # If a more sophisticated sequential conditioning is needed, the model architecture\n",
    "        # and generation loop would need to be adjusted.\n",
    "\n",
    "\n",
    "    return generated_sequence.squeeze() # Remove the batch dimension\n",
    "\n",
    "# --- Example Usage (requires trained model_gen_conditioned_v2 and data) ---\n",
    "# Assume model_gen_conditioned_v2 is trained and X_train_music is defined.\n",
    "\n",
    "# Example 1: Generate music from a text prompt and dummy audio\n",
    "user_text_prompt = \"Generate a calm piano melody with a melancholic feel.\"\n",
    "dummy_audio_file_path = '/content/dummy_user_audio.wav' # Use the dummy audio file created previously or a real one\n",
    "\n",
    "# Create a dummy audio file if it doesn't exist (for demonstration)\n",
    "if not os.path.exists(dummy_audio_file_path):\n",
    "    print(f\"Creating a dummy audio file at {dummy_audio_file_path} for demonstration.\")\n",
    "    dummy_audio_ts = np.zeros(sr * 5) # 5 seconds of silence\n",
    "    sf.write(dummy_audio_file_path, dummy_audio_ts, sr)\n",
    "\n",
    "\n",
    "print(f\"\\nAttempting to generate music from text prompt and dummy audio: '{user_text_prompt}'\")\n",
    "generated_music_file, generated_lyrics_text = generate_music_from_input_v2(\n",
    "    model_gen_conditioned_v2,\n",
    "    user_text_input=user_text_prompt,\n",
    "    user_audio_input_path=dummy_audio_file_path,\n",
    "    input_type='text_prompt',\n",
    "    generation_steps=200,\n",
    "    sr=sr,\n",
    "    max_padding=max_padding,\n",
    "    n_mfcc=n_mfcc,\n",
    "    n_chroma=n_chroma,\n",
    "    text_conditioning_dim=text_conditioning_dim\n",
    ")\n",
    "\n",
    "if generated_music_file:\n",
    "    print(f\"\\nGenerated music file (MP3): {generated_music_file}\")\n",
    "    if generated_lyrics_text:\n",
    "        print(f\"Generated lyrics:\\n{generated_lyrics_text}\")\n",
    "else:\n",
    "    print(\"\\nMusic generation failed.\")\n",
    "\n",
    "# Example 2: Generate music from provided lyrics and dummy audio\n",
    "user_provided_lyrics = \"The stars are shining bright tonight, a peaceful, quiet scene.\"\n",
    "print(f\"\\nAttempting to generate music from provided lyrics and dummy audio: '{user_provided_lyrics}'\")\n",
    "generated_music_file_lyrics, generated_lyrics_text_provided = generate_music_from_input_v2(\n",
    "    model_gen_conditioned_v2,\n",
    "    user_text_input=user_provided_lyrics,\n",
    "    user_audio_input_path=dummy_audio_file_path,\n",
    "    input_type='text_lyrics',\n",
    "    generation_steps=200,\n",
    "    sr=sr,\n",
    "    max_padding=max_padding,\n",
    "    n_mfcc=n_mfcc,\n",
    "    n_chroma=n_chroma,\n",
    "    text_conditioning_dim=text_conditioning_dim\n",
    ")\n",
    "\n",
    "if generated_music_file_lyrics:\n",
    "    print(f\"\\nGenerated music file (MP3) from lyrics: {generated_music_file_lyrics}\")\n",
    "    # Note: generated_lyrics_text_provided will be the original provided lyrics here\n",
    "    print(f\"Provided lyrics:\\n{generated_lyrics_text_provided}\")\n",
    "else:\n",
    "    print(\"\\nMusic generation from provided lyrics failed.\")\n",
    "\n",
    "# Example 3: Generate music from dummy audio only (no text input)\n",
    "print(\"\\nAttempting to generate music from dummy audio only:\")\n",
    "generated_music_file_audio_only, _ = generate_music_from_input_v2(\n",
    "    model_gen_conditioned_v2,\n",
    "    user_text_input=None, # No text input\n",
    "    user_audio_input_path=dummy_audio_file_path,\n",
    "    input_type='audio_only', # Indicate audio only\n",
    "    generation_steps=200,\n",
    "    sr=sr,\n",
    "    max_padding=max_padding,\n",
    "    n_mfcc=n_mfcc,\n",
    "    n_chroma=n_chroma,\n",
    "    text_conditioning_dim=text_conditioning_dim\n",
    ")\n",
    "\n",
    "if generated_music_file_audio_only:\n",
    "    print(f\"\\nGenerated music file (MP3) from audio only: {generated_music_file_audio_only}\")\n",
    "else:\n",
    "    print(\"\\nMusic generation from audio only failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8a2e33083c2a4373a863ac4f67bd9f43",
      "7431a01cf11f4ac7acd4a29ac761b9a7",
      "2efce94140bf4644847f2128f21119c7",
      "cc2b5be1cc3343f299c29fe9268c1f97",
      "54fbf462f8034346a323698b766b6b4f",
      "07ed71c2e43b48ed9fc3a0074b7682a9",
      "3762b365315b423a9aa8d9b5ebad76b2",
      "54a61ac79d7c46519027a000c440f1a8",
      "8514daf733e14bcfbc2e544c6f141680",
      "b32cf535b81141b49dea5e2a765f9d07",
      "a9c4069479fe4b9f95979a6d54185c9a",
      "45b5bfc931984d5a9b63662019a766a1",
      "69241ec18f194e15899dd94b5c534413",
      "f66eacb832174adc84d856e0694b302a",
      "6f8837d6a17d4252b5f799f4b72f4bc2",
      "115f242b650d460482a0ef30db16dbc5",
      "352c97c25ec34e50b434335f41cb530f",
      "3106b453637246a0a2a5e03980b35c34",
      "25b65831a6644bf881c2cdbb5d5e4ae7",
      "d8aea1b538e4457087a4af0ff718ebcd",
      "feba8a886e68429c8be3b70158eb672f",
      "b00e76a3e85f4c0f93cdb0c7ad883d38",
      "3ba986f4aa4545ecaad5c5c15cadc3e9",
      "d9c4110055204380ba943bf689bacdce",
      "b08f5a59e5294858b418c02b9937e51b",
      "ff0d567e9588436b9d735960e981d817",
      "ed3a2829b658467fadd8ba79c7dabc99",
      "f6e86b67ab3d482ab57708ff82aed7d6",
      "52cec86d234d45a98468e4be84807b3b",
      "2ea24b963b6d4f30ad29aac825a7e8b4",
      "f791b7d306974a0bbdfa84340c5e7136",
      "bd024011d32e491e8b8807bb4b9e79ec",
      "43a9e25b0b4346f7b2299578ed1b4600",
      "503d1abc636f4173b4f5a48275c97665",
      "5bdaf4ff9a954f118300bd186febb514",
      "2722a5f5326e4338b8ba332075b1a8d4",
      "08bfd29cffd24727a98bba0c3c56cbd6",
      "df30717e54234558a2a0eda059d6992a",
      "2a1cd0651618469994bdca6ef1810a05",
      "7298761195ad4d13a7746d27306adfd9",
      "dcba8f13858b4c8ca3096e8c4d5aa7a7",
      "4ab00e55a6684464ae6aca722281f671",
      "444badbaa18e43b7aaa00df0a0aeeb12",
      "78a643c5d6f64744854a5799cc358261",
      "e79a047c93ba4b49bd22633a6cf8470d",
      "7d79231e65b6441884dbe8f95a29d470",
      "b466a69f8d1d41fe838bbb6e626f7574",
      "076dd5631bc94551a75385dbb76cb567",
      "489e7f360541433385c00e8f463b9b43",
      "396bd8fc0b394b87a211a8c3095c9218",
      "0749af30797249b9bb8bf585a594c180",
      "1c96d675e6b44ca19f782ae9ed5582ed",
      "c1fa17450d6e418ba5d0c47412971eaf",
      "febe0ec5029c4618bd4eb8dbfeba7fb3",
      "fc7930985bc54dea8f24e51433646076",
      "e57c4fc7c028442e9b97fea098f6712b",
      "7e8070df0ded4bd7a726d30e079d5670",
      "e75d73cc2ffd411a87b4f13f7f85c92d",
      "53a04b0ba0cc4301a63a88c1415880a4",
      "96584bf8e6f44bb094d0c82787aa0106",
      "9eac856a1e044f60ab54596b0cd9e150",
      "6bee80de41534b57bc933f13221ff219",
      "2bf6f8de359c47f6ac10e8878130694a",
      "6027323aaa21457daaa4014fe574ad9b",
      "946ae799a97d40b980664c439932903f",
      "980b68f3a8124b0cbf34000873fb8296",
      "04134f5b02a34991a79fd7ed958b66ae",
      "7b5218e132bb4c0cbd0d7402488bd519",
      "7b640f6c16244ca08b2017e49e8d0854",
      "c1ddfe77db184693964a968ef7bebbb6",
      "9cbd24faa0a54e028288d51a2d5d22d1",
      "a04c4107b1614d3287bb79546bcfd5c3",
      "ae7c11bc82564baf84021767e9fb9753",
      "38c8fd155c4c4a93a834a94480a63b6c",
      "eb8fb677b50a47b8a1233f1ab47e1588",
      "f71f73dae8284cfb8817639f83afaaa5",
      "93a3d4f9dd534a27a708bca563badaa9"
     ]
    },
    "id": "519a67b2",
    "outputId": "718301eb-3b92-46f5-df78-768df8aeca80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
      "  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
      "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
      "  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
      "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
      "  elif re.match('(flt)p?( \\(default\\))?$', token):\n",
      "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
      "  elif re.match('(dbl)p?( \\(default\\))?$', token):\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2e33083c2a4373a863ac4f67bd9f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b5bfc931984d5a9b63662019a766a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba986f4aa4545ecaad5c5c15cadc3e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503d1abc636f4173b4f5a48275c97665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79a047c93ba4b49bd22633a6cf8470d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57c4fc7c028442e9b97fea098f6712b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04134f5b02a34991a79fd7ed958b66ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generation pipeline (distilgpt2) initialized.\n",
      "\n",
      "New conditional music generation model (v2) architecture defined.\n",
      "Downloading the dataset to ensure path is available...\n",
      "Using Colab cache for faster access to the 'musical-instruments-sound-dataset' dataset.\n",
      "Dataset path: /kaggle/input/musical-instruments-sound-dataset\n",
      "Searching for music instrument files in: /kaggle/input/musical-instruments-sound-dataset/Train_submission/Train_submission\n",
      "Found 1965 music instrument files in training submission.\n",
      "Searching for music instrument files in: /kaggle/input/musical-instruments-sound-dataset/Test_submission/Test_submission\n",
      "Found 2029 music instrument files in test submission.\n",
      "\n",
      "Extracting musical features (MFCCs) and audio conditioning features (MFCC+Chroma) from music instrument files...\n",
      "Processing file 100/2029\n",
      "Processing file 200/2029\n",
      "Processing file 300/2029\n",
      "Processing file 400/2029\n",
      "Processing file 500/2029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 600/2029\n",
      "Processing file 700/2029\n",
      "Processing file 800/2029\n",
      "Processing file 900/2029\n",
      "Processing file 1000/2029\n",
      "Processing file 1100/2029\n",
      "Processing file 1200/2029\n",
      "Processing file 1300/2029\n",
      "Processing file 1400/2029\n",
      "Processing file 1500/2029\n",
      "Processing file 1600/2029\n",
      "Processing file 1700/2029\n",
      "Processing file 1800/2029\n",
      "Processing file 1900/2029\n",
      "Processing file 2000/2029\n",
      "\n",
      "Feature extraction and data splitting complete.\n",
      "Shape of training musical features (MFCCs): (1623, 174, 20)\n",
      "Shape of validation musical features (MFCCs): (406, 174, 20)\n",
      "Shape of training audio conditioning features (MFCC+Chroma): (1623, 174, 32)\n",
      "Shape of validation audio conditioning features (MFCC+Chroma): (406, 174, 32)\n",
      "\n",
      "Shape of dummy training text conditioning data: (1623, 128)\n",
      "Shape of dummy validation text conditioning data: (406, 128)\n",
      "\n",
      "Training the conditioned GRU music generation model (v2)...\n",
      "Epoch 1/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 2s/step - loss: 4788.4751 - val_loss: 231.8510\n",
      "Epoch 2/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 2s/step - loss: 480.7487 - val_loss: 92.1438\n",
      "Epoch 3/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 2s/step - loss: 235.8813 - val_loss: 71.4501\n",
      "Epoch 4/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 2s/step - loss: 169.0332 - val_loss: 49.2653\n",
      "Epoch 5/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 2s/step - loss: 136.9488 - val_loss: 52.9746\n",
      "Epoch 6/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - loss: 123.8055 - val_loss: 69.3968\n",
      "Epoch 7/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - loss: 112.9983 - val_loss: 82.5100\n",
      "Epoch 8/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 2s/step - loss: 105.9822 - val_loss: 50.7316\n",
      "Epoch 9/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - loss: 102.1596 - val_loss: 48.4672\n",
      "Epoch 10/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 2s/step - loss: 99.6248 - val_loss: 65.9017\n",
      "Epoch 11/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - loss: 100.2237 - val_loss: 27.8545\n",
      "Epoch 12/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 2s/step - loss: 107.7122 - val_loss: 101.5776\n",
      "Epoch 13/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 2s/step - loss: 101.2476 - val_loss: 25.4805\n",
      "Epoch 14/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - loss: 101.2180 - val_loss: 94.2926\n",
      "Epoch 15/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 2s/step - loss: 98.8482 - val_loss: 26.0731\n",
      "Epoch 16/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 2s/step - loss: 95.9665 - val_loss: 98.3636\n",
      "Epoch 17/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 2s/step - loss: 95.7601 - val_loss: 29.4546\n",
      "Epoch 18/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 2s/step - loss: 95.7395 - val_loss: 93.5242\n",
      "Epoch 19/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 2s/step - loss: 94.7653 - val_loss: 44.2913\n",
      "Epoch 20/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - loss: 92.5125 - val_loss: 62.8751\n",
      "Epoch 21/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 2s/step - loss: 92.6715 - val_loss: 35.0219\n",
      "Epoch 22/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 2s/step - loss: 92.3954 - val_loss: 59.1824\n",
      "Epoch 23/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - loss: 90.8258 - val_loss: 36.5824\n",
      "Epoch 24/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - loss: 82.8477 - val_loss: 54.9292\n",
      "Epoch 25/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 2s/step - loss: 82.0382 - val_loss: 51.2483\n",
      "Epoch 26/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 2s/step - loss: 80.5900 - val_loss: 61.4927\n",
      "Epoch 27/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 2s/step - loss: 79.7137 - val_loss: 62.2401\n",
      "Epoch 28/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2s/step - loss: 77.8486 - val_loss: 32.9677\n",
      "Epoch 29/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 79.4037 - val_loss: 25.5517\n",
      "Epoch 30/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2s/step - loss: 87.4173 - val_loss: 74.1011\n",
      "Epoch 31/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - loss: 84.5842 - val_loss: 24.6544\n",
      "Epoch 32/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2s/step - loss: 82.9876 - val_loss: 64.3051\n",
      "Epoch 33/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - loss: 78.3142 - val_loss: 40.2411\n",
      "Epoch 34/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2s/step - loss: 78.6892 - val_loss: 24.5091\n",
      "Epoch 35/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2s/step - loss: 83.1804 - val_loss: 64.5090\n",
      "Epoch 36/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 2s/step - loss: 81.5058 - val_loss: 21.9289\n",
      "Epoch 37/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 2s/step - loss: 81.1885 - val_loss: 64.8772\n",
      "Epoch 38/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 2s/step - loss: 81.6819 - val_loss: 24.4266\n",
      "Epoch 39/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2s/step - loss: 80.4797 - val_loss: 61.2398\n",
      "Epoch 40/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - loss: 81.1769 - val_loss: 26.5474\n",
      "Epoch 41/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 2s/step - loss: 80.7051 - val_loss: 72.4757\n",
      "Epoch 42/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - loss: 79.2392 - val_loss: 29.5353\n",
      "Epoch 43/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2s/step - loss: 79.2483 - val_loss: 59.7763\n",
      "Epoch 44/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - loss: 80.3394 - val_loss: 21.5549\n",
      "Epoch 45/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 2s/step - loss: 77.0782 - val_loss: 62.6781\n",
      "Epoch 46/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - loss: 76.7832 - val_loss: 57.9930\n",
      "Epoch 47/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 2s/step - loss: 77.7865 - val_loss: 28.5669\n",
      "Epoch 48/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 2s/step - loss: 75.5512 - val_loss: 23.6529\n",
      "Epoch 49/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 2s/step - loss: 76.8475 - val_loss: 61.5208\n",
      "Epoch 50/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 2s/step - loss: 76.0378 - val_loss: 21.1398\n",
      "\n",
      "Training complete.\n",
      "\n",
      "Attempting to generate music from text prompt: 'Generate a piece of background music.' using audio file at /content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg, targeting 30 seconds.\n",
      "Generating lyrics for prompt: 'Generate a piece of background music.' using DistilGPT-2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyric generation successful.\n",
      "Getting embedding for text: '...' using a placeholder model.\n",
      "Processing user audio input for conditioning.\n",
      "Processed user audio features shape for conditioning: (1, 174, 32)\n",
      "Targeting 30 seconds, which requires approximately 1291 generation steps.\n",
      "\n",
      "Generating musical feature sequence iteratively for 1291 steps...\n",
      "Generated 100/1291 steps.\n",
      "Generated 200/1291 steps.\n",
      "Generated 300/1291 steps.\n",
      "Generated 400/1291 steps.\n",
      "Generated 500/1291 steps.\n",
      "Generated 600/1291 steps.\n",
      "Generated 700/1291 steps.\n",
      "Generated 800/1291 steps.\n",
      "Generated 900/1291 steps.\n",
      "Generated 1000/1291 steps.\n",
      "Generated 1100/1291 steps.\n",
      "Generated 1200/1291 steps.\n",
      "\n",
      "Iterative musical feature generation complete.\n",
      "Shape of the generated musical features: (1291, 20)\n",
      "\n",
      "Generated Musical Features Statistics:\n",
      "Shape: (1291, 20)\n",
      "Mean: -16.45598602294922\n",
      "Min: -601.10595703125\n",
      "Max: 89.18401336669922\n",
      "Standard Deviation: 74.03250885009766\n",
      "Generated musical features saved to generated_musical_features_iterative.npy\n",
      "\n",
      "Converting generated musical features to audio...\n",
      "Saving generated audio to generated_music.wav...\n",
      "Generated audio saved successfully to generated_music.wav\n",
      "Attempting to convert generated_music.wav to generated_music.mp3...\n",
      "Conversion to MP3 successful. Output saved to generated_music.mp3\n",
      "\n",
      "Generated music file: generated_music.mp3\n",
      "\n",
      "Download the generated music file:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='generated_music.mp3' target='_blank'>generated_music.mp3</a><br>"
      ],
      "text/plain": [
       "/content/generated_music.mp3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Playing generated audio file: generated_music.mp3\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "from pydub import AudioSegment # Import AudioSegment for MP3 conversion\n",
    "from transformers import pipeline # Import pipeline for using pre-trained models\n",
    "from IPython.display import display, FileLink, Audio # Import display, FileLink, and Audio\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split for data splitting\n",
    "import kagglehub # Import kagglehub for dataset download\n",
    "import tensorflow as tf # Import tensorflow\n",
    "from tensorflow.keras.models import Model # Import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, GRU, Input, TimeDistributed, Bidirectional # Import necessary layers\n",
    "\n",
    "# Define sampling rate (needed for audio processing and saving)\n",
    "sr = 22050 # Common sampling rate, adjust if your audio files use a different rate\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This needs to be consistent with feature extraction\n",
    "n_chroma = 12 # Number of chroma bins\n",
    "total_audio_features_dim = n_mfcc + n_chroma # Total features for audio conditioning\n",
    "\n",
    "# Define the dimensions for the music generation model\n",
    "musical_feature_dim = n_mfcc # The model output is MFCCs\n",
    "musical_sequence_length = max_padding # The model input sequence length\n",
    "\n",
    "# Define the conditioning input dimensions\n",
    "text_conditioning_dim = 128 # Dimension of the text embedding\n",
    "\n",
    "# --- Helper functions ---\n",
    "\n",
    "# Initialize the text generation pipeline (if not already initialized)\n",
    "lyric_generator = None # Initialize to None\n",
    "try:\n",
    "    lyric_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "    print(\"Text generation pipeline (distilgpt2) initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing text generation pipeline: {e}\")\n",
    "    lyric_generator = None\n",
    "\n",
    "\n",
    "def generate_lyrics_with_language_model(prompt):\n",
    "    \"\"\"Generates lyrics based on a prompt using a language model.\"\"\"\n",
    "    if lyric_generator:\n",
    "        print(f\"Generating lyrics for prompt: '{prompt}' using DistilGPT-2.\")\n",
    "        try:\n",
    "            # Generate text with a reasonable length and without generating too many sequences\n",
    "            generated_text = lyric_generator(prompt, max_length=100, num_return_sequences=1, truncation=True)\n",
    "            lyrics = generated_text[0]['generated_text']\n",
    "            # Basic post-processing: remove the prompt from the generated text if it's repeated\n",
    "            if lyrics.startswith(prompt):\n",
    "                lyrics = lyrics[len(prompt):].strip()\n",
    "            print(\"Lyric generation successful.\")\n",
    "            return lyrics\n",
    "        except Exception as e:\n",
    "            print(f\"Error during lyric generation: {e}\")\n",
    "            return f\"Error generating lyrics for prompt: '{prompt}'\" # Return an error message\n",
    "    else:\n",
    "        print(\"Lyric generator not initialized. Cannot generate lyrics.\")\n",
    "        return f\"Cannot generate lyrics for prompt: '{prompt}' (generator not available)\"\n",
    "\n",
    "\n",
    "# Function to get text embeddings (placeholder)\n",
    "def get_text_embedding(text, embedding_dim):\n",
    "    \"\"\"Placeholder function to get a fixed-size embedding for text.\"\"\"\n",
    "    print(f\"Getting embedding for text: '{text[:50]}...' using a placeholder model.\")\n",
    "    return np.random.rand(1, embedding_dim) # Shape (1, embedding_dim)\n",
    "\n",
    "\n",
    "# Function to process text input\n",
    "def process_text_input(user_input, embedding_dim, is_prompt=False):\n",
    "    \"\"\"Processes user text input (lyrics or prompt) and returns a conditioning embedding.\"\"\"\n",
    "    if is_prompt:\n",
    "        lyrics = generate_lyrics_with_language_model(user_input)\n",
    "    else:\n",
    "        lyrics = user_input # User provided lyrics directly\n",
    "\n",
    "    conditioning_embedding = get_text_embedding(lyrics, embedding_dim)\n",
    "\n",
    "    return conditioning_embedding, lyrics\n",
    "\n",
    "\n",
    "# Function to extract combined features (MFCC + Chroma) from an audio file.\n",
    "# This function is used for both training data preparation and user audio conditioning.\n",
    "def extract_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "    \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract Chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "        # Pad or truncate combined features to a fixed length\n",
    "        if combined_features.shape[1] < target_sequence_length:\n",
    "            combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "        else:\n",
    "            combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "        return combined_features.T # Transpose to have shape (time steps, total_features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process user audio for conditioning - calls extract_features\n",
    "def process_user_audio_for_conditioning(audio_file_path, target_sequence_length, n_mfcc, n_chroma):\n",
    "    \"\"\"Processes user audio input and returns features suitable for conditioning.\"\"\"\n",
    "    processed_features = extract_features(audio_file_path, target_sequence_length, n_mfcc, n_chroma)\n",
    "\n",
    "    if processed_features is not None:\n",
    "        # Add a batch dimension\n",
    "        processed_features = processed_features[np.newaxis, :, :] # Shape (1, target_sequence_length, total_feature_dimension)\n",
    "        print(f\"Processed user audio features shape for conditioning: {processed_features.shape}\")\n",
    "        return processed_features\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to extract MFCC features only (used for model input/output targets)\n",
    "def extract_mfcc_features_only(file_path, target_sequence_length, n_mfcc=20):\n",
    "    \"\"\"Extract only MFCC features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Pad or truncate MFCCs to a fixed length\n",
    "        if mfccs.shape[1] < target_sequence_length:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, target_sequence_length - mfccs.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :target_sequence_length]\n",
    "\n",
    "        return mfccs.T # Transpose to have shape (time steps, n_mfcc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Model Definition ---\n",
    "\n",
    "# Input 1: Musical features (MFCCs) - the sequence we are building upon\n",
    "musical_input = Input(shape=(musical_sequence_length, musical_feature_dim), name='musical_input')\n",
    "\n",
    "# Input 2: Text conditioning input (single vector)\n",
    "text_conditioning_input = Input(shape=(text_conditioning_dim,), name='text_conditioning_input')\n",
    "\n",
    "# Input 3: Audio conditioning input (sequence of features)\n",
    "audio_conditioning_input = Input(shape=(musical_sequence_length, total_audio_features_dim), name='audio_conditioning_input')\n",
    "\n",
    "\n",
    "# Process text conditioning (repeat the single vector across the musical sequence length)\n",
    "repeated_text_conditioning = tf.keras.layers.RepeatVector(musical_sequence_length)(text_conditioning_input)\n",
    "\n",
    "# Concatenate musical input with repeated text conditioning and audio conditioning input\n",
    "# This is one way to combine them. Another would be separate processing branches.\n",
    "# Concatenate [musical_input, repeated_text_conditioning, audio_conditioning_input]\n",
    "combined_input = Concatenate(axis=-1)([musical_input, repeated_text_conditioning, audio_conditioning_input])\n",
    "\n",
    "\n",
    "# GRU layers for the sequence generation, now taking the combined input\n",
    "# We can use Bidirectional GRU for potentially better context\n",
    "gru_1 = Bidirectional(GRU(128, activation='relu', return_sequences=True))(combined_input)\n",
    "dropout_1 = Dropout(0.2)(gru_1)\n",
    "\n",
    "gru_2 = Bidirectional(GRU(128, activation='relu', return_sequences=True))(dropout_1)\n",
    "dropout_2 = Dropout(0.2)(gru_2)\n",
    "\n",
    "# TimeDistributed Dense layer to output the generated musical features (MFCCs) for each time step\n",
    "# The output still needs to match the musical_feature_dim\n",
    "output_layer = TimeDistributed(Dense(musical_feature_dim, activation='linear'))(dropout_2)\n",
    "\n",
    "# Define the new conditioned music generation model\n",
    "# The model now has three inputs: musical_input, text_conditioning_input, audio_conditioning_input\n",
    "model_gen_conditioned_v2 = Model(inputs=[musical_input, text_conditioning_input, audio_conditioning_input], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=1.0) # Add gradient clipping\n",
    "model_gen_conditioned_v2.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "print(\"\\nNew conditional music generation model (v2) architecture defined.\")\n",
    "\n",
    "\n",
    "# --- Data Preparation ---\n",
    "# Download the dataset and directly assign the path within this cell\n",
    "print(\"Downloading the dataset to ensure path is available...\")\n",
    "try:\n",
    "    downloaded_dataset_root = kagglehub.dataset_download(\"soumendraprasad/musical-instruments-sound-dataset\")\n",
    "    print(f\"Dataset path: {downloaded_dataset_root}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    downloaded_dataset_root = None # Set to None if download fails\n",
    "\n",
    "\n",
    "if downloaded_dataset_root is None or not os.path.exists(downloaded_dataset_root):\n",
    "    print(\"Failed to get the dataset path. Cannot proceed with data preparation.\")\n",
    "else:\n",
    "    # Define feature extraction parameters (should be consistent with model definition)\n",
    "    n_mfcc = 20\n",
    "    max_padding = 174\n",
    "    n_chroma = 12 # Although not used for musical features output, needed for audio conditioning feature extraction\n",
    "    total_audio_features_dim = n_mfcc + n_chroma\n",
    "    text_conditioning_dim = 128\n",
    "\n",
    "\n",
    "    # Collect music instrument files from the downloaded dataset\n",
    "    music_instrument_files = []\n",
    "\n",
    "    # Assuming the structure from the previous file listing\n",
    "    train_submission_path = os.path.join(downloaded_dataset_root, 'Train_submission', 'Train_submission')\n",
    "    test_submission_path = os.path.join(downloaded_dataset_root, 'Test_submission', 'Test_submission') # Assuming Test_submission is also relevant for data\n",
    "\n",
    "    # Collect files from Train_submission\n",
    "    if os.path.exists(train_submission_path):\n",
    "        print(f\"Searching for music instrument files in: {train_submission_path}\")\n",
    "        for root, dirs, files in os.walk(train_submission_path):\n",
    "            for name in files:\n",
    "                if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                     # Optional: further filter by keywords if needed\n",
    "                    if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                         music_instrument_files.append(os.path.join(root, name))\n",
    "        print(f\"Found {len(music_instrument_files)} music instrument files in training submission.\")\n",
    "    else:\n",
    "        print(f\"Warning: Training submission directory not found at {train_submission_path}\")\n",
    "\n",
    "\n",
    "    # Collect files from Test_submission (if it exists and contains relevant audio)\n",
    "    # You might decide to use test data only for evaluation later, but for now, let's include relevant audio\n",
    "    if os.path.exists(test_submission_path):\n",
    "         print(f\"Searching for music instrument files in: {test_submission_path}\")\n",
    "         for root, dirs, files in os.walk(test_submission_path):\n",
    "             for name in files:\n",
    "                 if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                      # Optional: further filter by keywords if needed\n",
    "                     if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                          music_instrument_files.append(os.path.join(root, name))\n",
    "         print(f\"Found {len(music_instrument_files)} music instrument files in test submission.\")\n",
    "    else:\n",
    "         print(f\"Warning: Test submission directory not found at {test_submission_path}\")\n",
    "\n",
    "\n",
    "    # Use the collected music instrument files for feature extraction\n",
    "    files_to_process_music = music_instrument_files\n",
    "\n",
    "    # Prepare lists to store extracted features for training\n",
    "    X_train_music_features = [] # For musical input (MFCCs only)\n",
    "    X_train_audio_conditioning_features = [] # For audio conditioning input (MFCC + Chroma)\n",
    "\n",
    "\n",
    "    print(\"\\nExtracting musical features (MFCCs) and audio conditioning features (MFCC+Chroma) from music instrument files...\")\n",
    "\n",
    "    # Process a subset of files for faster testing, or all files for full training\n",
    "    # process_limit = 100 # Set a limit for testing\n",
    "    # files_to_process_subset = files_to_process_music[:process_limit]\n",
    "    files_to_process_subset = files_to_process_music # Process all files\n",
    "\n",
    "\n",
    "    for i, file_path in enumerate(files_to_process_subset):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processing file {i + 1}/{len(files_to_process_subset)}\")\n",
    "\n",
    "        # Extract MFCC features for the musical input/output\n",
    "        mfccs_only = extract_mfcc_features_only(file_path, max_padding, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract combined features for the audio conditioning input\n",
    "        combined_features = extract_features(file_path, max_padding, n_mfcc=n_mfcc, n_chroma=n_chroma)\n",
    "\n",
    "\n",
    "        if mfccs_only is not None and combined_features is not None:\n",
    "            X_train_music_features.append(mfccs_only)\n",
    "            X_train_audio_conditioning_features.append(combined_features)\n",
    "\n",
    "\n",
    "    # Convert extracted features to numpy arrays\n",
    "    X_train_music_features = np.array(X_train_music_features)\n",
    "    X_train_audio_conditioning_features = np.array(X_train_audio_conditioning_features)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    # Ensure that the split is consistent for both sets of features\n",
    "    if len(X_train_music_features) > 0:\n",
    "        X_train_music, X_val_music, X_train_audio_cond, X_val_audio_cond = train_test_split(\n",
    "            X_train_music_features, X_train_audio_conditioning_features, test_size=0.2, random_state=42\n",
    "        )\n",
    "    else:\n",
    "        print(\"No features were successfully extracted. Cannot split data.\")\n",
    "        # Initialize empty arrays to prevent errors in subsequent steps\n",
    "        X_train_music = np.array([])\n",
    "        X_val_music = np.array([])\n",
    "        X_train_audio_cond = np.array([])\n",
    "        X_val_audio_cond = np.array([])\n",
    "\n",
    "\n",
    "    print(\"\\nFeature extraction and data splitting complete.\")\n",
    "    print(f\"Shape of training musical features (MFCCs): {X_train_music.shape}\")\n",
    "    print(f\"Shape of validation musical features (MFCCs): {X_val_music.shape}\")\n",
    "    print(f\"Shape of training audio conditioning features (MFCC+Chroma): {X_train_audio_cond.shape}\")\n",
    "    print(f\"Shape of validation audio conditioning features (MFCC+Chroma): {X_val_audio_cond.shape}\")\n",
    "\n",
    "    # Create dummy text conditioning data for training\n",
    "    # This data is not based on actual text input yet, just for model training setup\n",
    "    # It should match the expected shape of the text_conditioning_input layer\n",
    "    dummy_text_conditioning_train = np.random.rand(X_train_music.shape[0], text_conditioning_dim)\n",
    "    dummy_text_conditioning_val = np.random.rand(X_val_music.shape[0], text_conditioning_dim)\n",
    "\n",
    "\n",
    "    print(f\"\\nShape of dummy training text conditioning data: {dummy_text_conditioning_train.shape}\")\n",
    "    print(f\"Shape of dummy validation text conditioning data: {dummy_text_conditioning_val.shape}\")\n",
    "\n",
    "\n",
    "    # --- Train the Model ---\n",
    "    # Assume the necessary variables (model_gen_conditioned_v2, X_train_music, X_val_music, dummy_text_conditioning_train, dummy_text_conditioning_val, X_train_audio_cond, X_val_audio_cond) are defined from the previous step.\n",
    "\n",
    "    # Train the conditioned GRU generation model (v2) with multiple inputs\n",
    "    print(\"\\nTraining the conditioned GRU music generation model (v2)...\")\n",
    "    # Check if training data is available before training\n",
    "    if X_train_music.shape[0] > 0:\n",
    "        history_gen_conditioned_v2 = model_gen_conditioned_v2.fit(\n",
    "            [X_train_music, dummy_text_conditioning_train, X_train_audio_cond], # Inputs: [musical_input, text_conditioning_input, audio_conditioning_input]\n",
    "            X_train_music, # Target output: musical features (MFCCs)\n",
    "            epochs=50, # You might need to adjust epochs based on convergence\n",
    "            batch_size=32,\n",
    "            validation_data=([X_val_music, dummy_text_conditioning_val, X_val_audio_cond], X_val_music) # Validation data\n",
    "        )\n",
    "\n",
    "        print(\"\\nTraining complete.\")\n",
    "    else:\n",
    "        print(\"Training data is empty. Skipping model training.\")\n",
    "\n",
    "\n",
    "# --- Function to generate musical features iteratively ---\n",
    "def generate_musical_features_iterative(model, start_sequence, text_conditioning_input, audio_conditioning_input, total_generation_steps):\n",
    "    \"\"\"Generates new MFCC sequences step-by-step using the trained conditional model (v2).\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras conditional generation model (v2).\n",
    "        start_sequence: A numpy array representing the initial musical sequence\n",
    "                        (shape: (1, sequence_length, musical_feature_dim)).\n",
    "        text_conditioning_input: The text conditioning input (shape: (1, text_conditioning_dim)).\n",
    "        audio_conditioning_input: The audio conditioning input (sequence of features,\n",
    "                                   shape: (1, sequence_length, total_audio_features_dim)).\n",
    "                                   This is the *full* audio conditioning sequence.\n",
    "        total_generation_steps: The total number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated musical MFCC sequence (shape: (total_generation_steps, musical_feature_dim)),\n",
    "        or None if generation fails.\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating musical feature sequence iteratively for {total_generation_steps} steps...\")\n",
    "\n",
    "    current_musical_sequence = np.copy(start_sequence)\n",
    "    generated_sequence = [] # List to store generated time steps\n",
    "    sequence_length = start_sequence.shape[1] # Expected input sequence length for the model\n",
    "    musical_feature_dim = start_sequence.shape[2] # Dimension of musical features\n",
    "\n",
    "    # Ensure audio_conditioning_input has a batch dimension and correct sequence length\n",
    "    if audio_conditioning_input.ndim != 3 or audio_conditioning_input.shape[1] != sequence_length:\n",
    "         print(\"Error: audio_conditioning_input must have shape (1, sequence_length, total_audio_features_dim).\")\n",
    "         return None\n",
    "\n",
    "    # Ensure text_conditioning_input has a batch dimension and correct dimension\n",
    "    if text_conditioning_input.ndim != 2 or text_conditioning_input.shape[1] != text_conditioning_dim:\n",
    "        print(\"Error: text_conditioning_input must have shape (1, text_conditioning_dim).\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    for i in range(total_generation_steps):\n",
    "        # The model expects [musical_input, text_conditioning_input, audio_conditioning_input]\n",
    "        # We feed the current musical sequence, the text conditioning (repeated internally by the model),\n",
    "        # and the *relevant portion* of the audio conditioning sequence.\n",
    "        # Since the model architecture concatenates the *full* audio conditioning sequence\n",
    "        # with the *full* musical input sequence, we need to decide how to best use\n",
    "        # the sequential audio conditioning in an iterative generation process.\n",
    "\n",
    "        # Option 1: Provide the full audio_conditioning_input at each step (as the model is designed).\n",
    "        # This means the audio conditioning doesn't change based on the current time step being generated.\n",
    "        # This might not leverage the sequential nature of the audio input effectively for time-dependent conditioning.\n",
    "        model_inputs = [current_musical_sequence, text_conditioning_input, audio_conditioning_input]\n",
    "\n",
    "        # Option 2: Provide a sliding window or a fixed segment of the audio_conditioning_input\n",
    "        # relevant to the current time step being generated. This would require modifying\n",
    "        # the model architecture or the input preparation.\n",
    "\n",
    "        # Given the current model architecture (v2) which concatenates the full audio_conditioning_input,\n",
    "        # we will use Option 1 for now, feeding the full audio conditioning sequence at each step.\n",
    "        # To truly leverage sequential audio conditioning, the model architecture would need to be\n",
    "        # designed to process the audio sequence in conjunction with the musical sequence over time.\n",
    "\n",
    "        # Predict the next time step of musical features\n",
    "        try:\n",
    "            # The model predicts a sequence of length `musical_sequence_length`.\n",
    "            # We are interested in the prediction for the *next* time step.\n",
    "            predicted_sequence = model.predict(model_inputs, verbose=0) # Shape (1, musical_sequence_length, musical_feature_dim)\n",
    "\n",
    "            # We need to decide which part of the predicted_sequence corresponds to the \"next\" step.\n",
    "            # In a standard sequence generation setup where the model predicts the *next* item\n",
    "            # based on the input sequence, we'd take the last time step of the prediction.\n",
    "            predicted_next_step = predicted_sequence[:, -1:, :] # Shape (1, 1, musical_feature_dim)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during model prediction at step {i}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "        # Append the predicted time step to the generated sequence\n",
    "        generated_sequence.append(predicted_next_step.squeeze()) # Remove batch and sequence length 1 dimensions\n",
    "\n",
    "        # Update the current musical sequence by removing the oldest time step and adding the new one\n",
    "        # This sliding window approach is standard for iterative sequence generation.\n",
    "        current_musical_sequence = np.append(current_musical_sequence[:, 1:, :], predicted_next_step, axis=1)\n",
    "\n",
    "        # Print progress\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Generated {i + 1}/{total_generation_steps} steps.\")\n",
    "\n",
    "\n",
    "    # Convert the list of generated steps into a numpy array\n",
    "    generated_sequence = np.array(generated_sequence) # Shape (total_generation_steps, musical_feature_dim)\n",
    "\n",
    "    print(\"\\nIterative musical feature generation complete.\")\n",
    "    print(f\"Shape of the generated musical features: {generated_sequence.shape}\")\n",
    "\n",
    "    # Add inspection of generated features\n",
    "    print(\"\\nGenerated Musical Features Statistics:\")\n",
    "    print(f\"Shape: {generated_sequence.shape}\")\n",
    "    print(f\"Mean: {np.mean(generated_sequence)}\")\n",
    "    print(f\"Min: {np.min(generated_sequence)}\")\n",
    "    print(f\"Max: {np.max(generated_sequence)}\")\n",
    "    print(f\"Standard Deviation: {np.std(generated_sequence)}\")\n",
    "\n",
    "    # Save generated features to a file for inspection\n",
    "    features_output_filename = 'generated_musical_features_iterative.npy'\n",
    "    np.save(features_output_filename, generated_sequence)\n",
    "    print(f\"Generated musical features saved to {features_output_filename}\")\n",
    "\n",
    "\n",
    "    return generated_sequence\n",
    "\n",
    "\n",
    "# --- Function to convert musical features to audio and save ---\n",
    "def convert_features_to_audio(musical_features, sr=22050, output_filename_wav='generated_music.wav', output_filename_mp3='generated_music.mp3'):\n",
    "    \"\"\"Converts musical features (MFCCs) to an audio file and saves it.\n",
    "\n",
    "    Args:\n",
    "        musical_features: A numpy array containing the generated musical MFCC sequence (shape: (time_steps, n_mfcc)).\n",
    "        sr: Sampling rate for audio output.\n",
    "        output_filename_wav: Filename for the output WAV file.\n",
    "        output_filename_mp3: Filename for the output MP3 file.\n",
    "\n",
    "    Returns:\n",
    "        The path to the generated audio file (MP3 or WAV), or None if conversion/saving fails.\n",
    "    \"\"\"\n",
    "    if musical_features is None or musical_features.shape[0] == 0:\n",
    "        print(\"No musical features provided for audio conversion.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\nConverting generated musical features to audio...\")\n",
    "    try:\n",
    "        # Assuming the features are MFCCs. Transpose back to (n_mfcc, time_steps) for inverse transform\n",
    "        audio_time_series = librosa.feature.inverse.mfcc_to_audio(musical_features.T, sr=sr)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MFCC to audio conversion: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Save the generated audio to a WAV file first\n",
    "    print(f\"Saving generated audio to {output_filename_wav}...\")\n",
    "    try:\n",
    "        sf.write(output_filename_wav, audio_time_series, sr)\n",
    "        print(f\"Generated audio saved successfully to {output_filename_wav}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving WAV file: {e}\")\n",
    "        return None\n",
    "\n",
    "    generated_audio_file = output_filename_wav # Default to WAV\n",
    "\n",
    "    # Attempt to convert the WAV file to MP3\n",
    "    print(f\"Attempting to convert {output_filename_wav} to {output_filename_mp3}...\")\n",
    "    try:\n",
    "        audio = AudioSegment.from_wav(output_filename_wav)\n",
    "        audio.export(output_filename_mp3, format=\"mp3\")\n",
    "        print(f\"Conversion to MP3 successful. Output saved to {output_filename_mp3}\")\n",
    "        generated_audio_file = output_filename_mp3 # Update to MP3 if successful\n",
    "        # Optionally, remove the intermediate WAV file\n",
    "        # os.remove(output_filename_wav)\n",
    "        # print(f\"Removed intermediate WAV file: {output_filename_wav}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MP3 conversion: {e}. Proceeding with WAV file.\")\n",
    "        # generated_audio_file remains the WAV file path\n",
    "\n",
    "    return generated_audio_file\n",
    "\n",
    "\n",
    "# --- Main music generation function (uses iterative generation) ---\n",
    "def generate_music_from_input_v4(model, user_text_input=None, user_audio_input_path=None, input_type='text_prompt', target_audio_duration_sec=30, sr=22050, max_padding=174, n_mfcc=20, n_chroma=12, text_conditioning_dim=128):\n",
    "    \"\"\"Generates music based on user input (text and/or audio) using the v2 model and iterative generation.\n",
    "\n",
    "    Args:\n",
    "        model: The trained Keras conditional music generation model (v2).\n",
    "        user_text_input: The user's text input (string for lyrics or prompt).\n",
    "        user_audio_input_path: Path to the user's audio file.\n",
    "        input_type: Type of text input ('text_lyrics', 'text_prompt'). 'audio_only' is also possible if no text is provided.\n",
    "        target_audio_duration_sec: The desired duration of the generated audio in seconds.\n",
    "        sr: Sampling rate for audio output.\n",
    "        max_padding: Target sequence length for padding/truncating features (used for initial sequence).\n",
    "        n_mfcc: Number of MFCCs extracted.\n",
    "        n_chroma: Number of chroma bins extracted.\n",
    "        text_conditioning_dim: Dimension of the text embedding.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the path to the generated audio file (MP3 or WAV) and the generated lyrics (string),\n",
    "        or (None, None) if an error occurred.\n",
    "    \"\"\"\n",
    "    text_conditioning_input_for_model = None\n",
    "    audio_conditioning_input_for_model = None\n",
    "    generated_lyrics = None\n",
    "\n",
    "    # Process text input if provided\n",
    "    if user_text_input:\n",
    "        is_prompt = (input_type == 'text_prompt')\n",
    "        conditioning_input_for_model, generated_lyrics = process_text_input(user_text_input, text_conditioning_dim, is_prompt=is_prompt)\n",
    "        if conditioning_input_for_model is None:\n",
    "            print(\"Error processing text input for conditioning.\")\n",
    "            return None, None\n",
    "        text_conditioning_input_for_model = conditioning_input_for_model\n",
    "\n",
    "    else:\n",
    "        print(\"No text input provided. Using dummy text conditioning.\")\n",
    "        text_conditioning_input_for_model = np.zeros((1, text_conditioning_dim))\n",
    "\n",
    "\n",
    "    # Process audio input if provided\n",
    "    if user_audio_input_path and os.path.exists(user_audio_input_path):\n",
    "        print(\"Processing user audio input for conditioning.\")\n",
    "        # For iterative generation, we still need the full audio conditioning sequence\n",
    "        audio_conditioning_input_for_model = process_user_audio_for_conditioning(user_audio_input_path, max_padding, n_mfcc, n_chroma)\n",
    "        if audio_conditioning_input_for_model is None:\n",
    "            print(\"Error processing user audio input for conditioning.\")\n",
    "            return None, None\n",
    "    else:\n",
    "        print(\"No user audio file provided or found. Using dummy audio conditioning.\")\n",
    "        audio_conditioning_input_for_model = np.zeros((1, max_padding, total_audio_features_dim))\n",
    "\n",
    "\n",
    "    # Ensure both conditioning inputs are available\n",
    "    if text_conditioning_input_for_model is None or audio_conditioning_input_for_model is None:\n",
    "         print(\"Missing conditioning input(s). Cannot proceed.\")\n",
    "         return None, None\n",
    "\n",
    "\n",
    "    # Generate a starting musical sequence (e.g., take a random sample from the training data)\n",
    "    # This starting sequence will be the initial input for the iterative generation.\n",
    "    if 'X_train_music' not in globals() or X_train_music.shape[0] == 0:\n",
    "         print(\"Training data (X_train_music) is not available or empty. Cannot generate a starting sequence.\")\n",
    "         return None, None\n",
    "\n",
    "\n",
    "    start_index = np.random.randint(0, X_train_music.shape[0])\n",
    "    start_sequence = X_train_music[start_index:start_index+1, :, :]\n",
    "\n",
    "\n",
    "    # Calculate the total number of time steps to generate\n",
    "    # Each time step in the feature sequence corresponds to hop_length samples (default 512 in librosa mfcc)\n",
    "    # Duration of one feature time step = hop_length / sr\n",
    "    # Total time steps needed = target_audio_duration_sec / (hop_length / sr) = target_audio_duration_sec * sr / hop_length\n",
    "    # Assuming default hop_length of 512\n",
    "    hop_length = 512\n",
    "    total_generation_steps = int(target_audio_duration_sec * sr / hop_length)\n",
    "    print(f\"Targeting {target_audio_duration_sec} seconds, which requires approximately {total_generation_steps} generation steps.\")\n",
    "\n",
    "\n",
    "    # Generate new musical features iteratively using the trained conditional GRU model (v2)\n",
    "    generated_musical_features = generate_musical_features_iterative(\n",
    "        model,\n",
    "        start_sequence,\n",
    "        text_conditioning_input_for_model,\n",
    "        audio_conditioning_input_for_model,\n",
    "        total_generation_steps\n",
    "    )\n",
    "\n",
    "\n",
    "    if generated_musical_features is None:\n",
    "        print(\"Feature generation failed.\")\n",
    "        return None, generated_lyrics # Return None for audio path\n",
    "\n",
    "\n",
    "    # Convert the generated musical features to audio and save\n",
    "    generated_audio_file = convert_features_to_audio(generated_musical_features, sr=sr)\n",
    "\n",
    "\n",
    "    # Optionally, save the generated lyrics to a text file if lyrics were generated\n",
    "    lyrics_output_filename = 'generated_lyrics.txt'\n",
    "    if generated_lyrics:\n",
    "        try:\n",
    "            with open(lyrics_output_filename, 'w') as f:\n",
    "                f.write(generated_lyrics)\n",
    "            print(f\"Generated lyrics saved to {lyrics_output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving generated lyrics to file: {e}\")\n",
    "\n",
    "\n",
    "    return generated_audio_file, generated_lyrics # Return the path to the generated audio file and generated lyrics\n",
    "\n",
    "\n",
    "# --- Example Usage (requires trained model_gen_conditioned_v2 and data) ---\n",
    "# Assume model_gen_conditioned_v2 is trained and X_train_music is defined.\n",
    "\n",
    "# Example: Generate music from a text prompt and user audio, aiming for 30 seconds\n",
    "user_text_prompt = \"Generate a piece of background music.\"\n",
    "# IMPORTANT: Replace '/content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg' with the actual path to your uploaded audio file\n",
    "user_audio_input_path = '/content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg'\n",
    "\n",
    "# Set the target duration\n",
    "target_duration_seconds = 30\n",
    "\n",
    "print(f\"\\nAttempting to generate music from text prompt: '{user_text_prompt}' using audio file at {user_audio_input_path}, targeting {target_duration_seconds} seconds.\")\n",
    "generated_music_file, generated_lyrics_text = generate_music_from_input_v4(\n",
    "    model_gen_conditioned_v2,\n",
    "    user_text_input=user_text_prompt,\n",
    "    user_audio_input_path=user_audio_input_path,\n",
    "    input_type='text_prompt',\n",
    "    target_audio_duration_sec=target_duration_seconds,\n",
    "    sr=sr,\n",
    "    max_padding=max_padding,\n",
    "    n_mfcc=n_mfcc,\n",
    "    n_chroma=n_chroma,\n",
    "    text_conditioning_dim=text_conditioning_dim\n",
    ")\n",
    "\n",
    "if generated_music_file:\n",
    "    print(f\"\\nGenerated music file: {generated_music_file}\")\n",
    "    if generated_lyrics_text:\n",
    "        print(f\"Generated lyrics:\\n{generated_lyrics_text}\")\n",
    "    # Display links to the generated files\n",
    "    print(\"\\nDownload the generated music file:\")\n",
    "    display(FileLink(generated_music_file))\n",
    "    if generated_lyrics_text: # Only display lyrics link if lyrics were generated\n",
    "        print(\"Download the generated lyrics file:\")\n",
    "        display(FileLink('generated_lyrics.txt'))\n",
    "\n",
    "    # Play the generated audio\n",
    "    print(f\"\\nPlaying generated audio file: {generated_music_file}\")\n",
    "    display(Audio(generated_music_file))\n",
    "\n",
    "else:\n",
    "    print(\"\\nMusic generation failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113
    },
    "id": "f2bb4a6e",
    "outputId": "b6b181e8-957e-4493-9a1a-6f24749e741d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Playing generated audio file: generated_music_conditional.mp3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/mpeg;base64,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\" type=\"audio/mpeg\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "# Assuming 'generated_music_file' contains the path to the generated audio file\n",
    "if generated_music_file and os.path.exists(generated_music_file):\n",
    "    print(f\"\\nPlaying generated audio file: {generated_music_file}\")\n",
    "    display(Audio(generated_music_file))\n",
    "else:\n",
    "    print(\"\\nGenerated music file not found or generation failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aaf2cf73",
    "outputId": "1dc33ab9-b44e-4a70-87d6-fb4d3595e0e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to generate music from text prompt and audio: 'Create an energetic electronic dance track.' using audio file at /content/testfile.mpeg\n",
      "Generating lyrics for prompt: 'Create an energetic electronic dance track.' using DistilGPT-2.\n",
      "Lyric generation successful.\n",
      "Getting embedding for text: 'The track was originally written by the French art...' using a placeholder model.\n",
      "Processing user audio input for conditioning.\n",
      "Processed user audio features shape for conditioning: (1, 174, 32)\n",
      "\n",
      "Generating new conditional musical features using v2 model...\n",
      "Lyric generation successful.\n",
      "Getting embedding for text: 'The track was originally written by the French art...' using a placeholder model.\n",
      "Processing user audio input for conditioning.\n",
      "Processed user audio features shape for conditioning: (1, 174, 32)\n",
      "\n",
      "Generating new conditional musical features using v2 model...\n",
      "Shape of the generated musical features: (374, 20)\n",
      "\n",
      "Converting generated musical features to audio...\n",
      "Shape of the generated musical features: (374, 20)\n",
      "\n",
      "Converting generated musical features to audio...\n",
      "Saving generated conditional audio to generated_music_conditional.wav...\n",
      "Generated conditional audio saved successfully to generated_music_conditional.wav\n",
      "Attempting to convert generated_music_conditional.wav to generated_music_conditional.mp3...\n",
      "Saving generated conditional audio to generated_music_conditional.wav...\n",
      "Generated conditional audio saved successfully to generated_music_conditional.wav\n",
      "Attempting to convert generated_music_conditional.wav to generated_music_conditional.mp3...\n",
      "Conversion to MP3 successful. Output saved to generated_music_conditional.mp3\n",
      "Generated lyrics saved to generated_lyrics.txt\n",
      "\n",
      "Generated music file: generated_music_conditional.mp3\n",
      "Generated lyrics:\n",
      "The track was originally written by the French artist Théralène Foul.\n",
      "The track was written by a French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written\n",
      "\n",
      "Download the generated music file:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='generated_music_conditional.mp3' target='_blank'>generated_music_conditional.mp3</a><br>"
      ],
      "text/plain": [
       "/content/generated_music_conditional.mp3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion to MP3 successful. Output saved to generated_music_conditional.mp3\n",
      "Generated lyrics saved to generated_lyrics.txt\n",
      "\n",
      "Generated music file: generated_music_conditional.mp3\n",
      "Generated lyrics:\n",
      "The track was originally written by the French artist Théralène Foul.\n",
      "The track was written by a French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written by the French composer and composer. The track was originally written\n",
      "\n",
      "Download the generated music file:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='generated_music_conditional.mp3' target='_blank'>generated_music_conditional.mp3</a><br>"
      ],
      "text/plain": [
       "/content/generated_music_conditional.mp3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download the generated lyrics file:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='generated_lyrics.txt' target='_blank'>generated_lyrics.txt</a><br>"
      ],
      "text/plain": [
       "/content/generated_lyrics.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Playing generated audio file: generated_music_conditional.mp3\n",
      "Download the generated lyrics file:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='generated_lyrics.txt' target='_blank'>generated_lyrics.txt</a><br>"
      ],
      "text/plain": [
       "/content/generated_lyrics.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Playing generated audio file: generated_music_conditional.mp3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/mpeg;base64,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\" type=\"audio/mpeg\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/mpeg;base64,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\" type=\"audio/mpeg\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate another audio using a different text prompt and the same dummy audio file\n",
    "user_text_prompt_2 = \"Create an energetic electronic dance track.\"\n",
    "user_audio_input_path = '/content/testfile.mpeg' # Using the same dummy audio\n",
    "\n",
    "print(f\"\\nAttempting to generate music from text prompt and audio: '{user_text_prompt_2}' using audio file at {user_audio_input_path}\")\n",
    "generated_music_file_2, generated_lyrics_text_2 = generate_music_from_input_v2(\n",
    "    model_gen_conditioned_v2,\n",
    "    user_text_input=user_text_prompt_2,\n",
    "    user_audio_input_path=user_audio_input_path,\n",
    "    input_type='text_prompt',\n",
    "    generation_steps=200, # You can adjust this\n",
    "    sr=sr,\n",
    "    max_padding=max_padding,\n",
    "    n_mfcc=n_mfcc,\n",
    "    n_chroma=n_chroma,\n",
    "    text_conditioning_dim=text_conditioning_dim\n",
    ")\n",
    "\n",
    "if generated_music_file_2:\n",
    "    print(f\"\\nGenerated music file: {generated_music_file_2}\")\n",
    "    if generated_lyrics_text_2:\n",
    "        print(f\"Generated lyrics:\\n{generated_lyrics_text_2}\")\n",
    "    # Display links to the generated files\n",
    "    print(\"\\nDownload the generated music file:\")\n",
    "    display(FileLink(generated_music_file_2))\n",
    "    if generated_lyrics_text_2: # Only display lyrics link if lyrics were generated\n",
    "        print(\"Download the generated lyrics file:\")\n",
    "        display(FileLink('generated_lyrics.txt')) # Note: This will overwrite the previous lyrics file\n",
    "\n",
    "    # Play the generated audio\n",
    "    print(f\"\\nPlaying generated audio file: {generated_music_file_2}\")\n",
    "    display(Audio(generated_music_file_2))\n",
    "\n",
    "else:\n",
    "    print(\"\\nMusic generation failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "id": "0acd0364",
    "outputId": "945eee96-4c8d-42c8-eac7-4be5b7f9715d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to generate music from text prompt: 'Generate a piece of music.' using audio file at /content/testfile.mpeg\n",
      "Generating lyrics for prompt: 'Generate a piece of music.' using DistilGPT-2.\n",
      "Lyric generation successful.\n",
      "Getting embedding for text: 'The band is on the move to an album....' using a placeholder model.\n",
      "Processing user audio input for conditioning.\n",
      "Processed user audio features shape for conditioning: (1, 174, 32)\n",
      "\n",
      "Generating new conditional musical features using v2 model...\n",
      "Shape of the generated musical features: (374, 20)\n",
      "\n",
      "Converting generated musical features to audio...\n",
      "Saving generated conditional audio to generated_music_conditional.wav...\n",
      "Generated conditional audio saved successfully to generated_music_conditional.wav\n",
      "Attempting to convert generated_music_conditional.wav to generated_music_conditional.mp3...\n",
      "Conversion to MP3 successful. Output saved to generated_music_conditional.mp3\n",
      "Generated lyrics saved to generated_lyrics.txt\n",
      "\n",
      "Generated music file: generated_music_conditional.mp3\n",
      "Generated lyrics:\n",
      "The band is on the move to an album.\n",
      "\n",
      "Download the generated music file:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='generated_music_conditional.mp3' target='_blank'>generated_music_conditional.mp3</a><br>"
      ],
      "text/plain": [
       "/content/generated_music_conditional.mp3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download the generated lyrics file:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='generated_lyrics.txt' target='_blank'>generated_lyrics.txt</a><br>"
      ],
      "text/plain": [
       "/content/generated_lyrics.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Playing generated audio file: generated_music_conditional.mp3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/mpeg;base64,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\" type=\"audio/mpeg\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the user-provided audio file for conditioning\n",
    "user_audio_input_path = '/content/testfile.mpeg'\n",
    "\n",
    "# Use a generic text prompt, or you can replace this with a prompt provided by the user\n",
    "user_text_prompt = \"Generate a piece of music.\"\n",
    "\n",
    "print(f\"\\nAttempting to generate music from text prompt: '{user_text_prompt}' using audio file at {user_audio_input_path}\")\n",
    "generated_music_file, generated_lyrics_text = generate_music_from_input_v2(\n",
    "    model_gen_conditioned_v2,\n",
    "    user_text_input=user_text_prompt,\n",
    "    user_audio_input_path=user_audio_input_path,\n",
    "    input_type='text_prompt',\n",
    "    generation_steps=200, # You can adjust this\n",
    "    sr=sr,\n",
    "    max_padding=max_padding,\n",
    "    n_mfcc=n_mfcc,\n",
    "    n_chroma=n_chroma,\n",
    "    text_conditioning_dim=text_conditioning_dim\n",
    ")\n",
    "\n",
    "if generated_music_file:\n",
    "    print(f\"\\nGenerated music file: {generated_music_file}\")\n",
    "    if generated_lyrics_text:\n",
    "        print(f\"Generated lyrics:\\n{generated_lyrics_text}\")\n",
    "    # Display links to the generated files\n",
    "    print(\"\\nDownload the generated music file:\")\n",
    "    display(FileLink(generated_music_file))\n",
    "    if generated_lyrics_text: # Only display lyrics link if lyrics were generated\n",
    "        print(\"Download the generated lyrics file:\")\n",
    "        display(FileLink('generated_lyrics.txt')) # Note: This will overwrite the previous lyrics file\n",
    "\n",
    "    # Play the generated audio\n",
    "    print(f\"\\nPlaying generated audio file: {generated_music_file}\")\n",
    "    display(Audio(generated_music_file))\n",
    "\n",
    "else:\n",
    "    print(\"\\nMusic generation failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "237b8360",
    "outputId": "738aaacd-c39d-4388-abae-af4ea1f28ecb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to generate music from text prompt: 'Generate a piece of music.' using audio file at /content/testfile.mpeg\n",
      "Generating lyrics for prompt: 'Generate a piece of music.' using DistilGPT-2.\n",
      "Lyric generation successful.\n",
      "Getting embedding for text: 'The following is an excerpt from The Albums of the...' using a placeholder model.\n",
      "Processing user audio input for conditioning.\n",
      "Processed user audio features shape for conditioning: (1, 174, 32)\n",
      "\n",
      "Generating new conditional musical features using v2 model...\n",
      "Shape of the generated musical features: (374, 20)\n",
      "\n",
      "Converting generated musical features to audio...\n",
      "Saving generated conditional audio to generated_music_conditional.wav...\n",
      "Generated conditional audio saved successfully to generated_music_conditional.wav\n",
      "Attempting to convert generated_music_conditional.wav to generated_music_conditional.mp3...\n",
      "Conversion to MP3 successful. Output saved to generated_music_conditional.mp3\n",
      "Generated lyrics saved to generated_lyrics.txt\n",
      "\n",
      "Generated music file: generated_music_conditional.mp3\n",
      "Generated lyrics:\n",
      "The following is an excerpt from The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year:\n",
      "The Albums of the Year\n",
      "\n",
      "Download the generated music file:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='generated_music_conditional.mp3' target='_blank'>generated_music_conditional.mp3</a><br>"
      ],
      "text/plain": [
       "/content/generated_music_conditional.mp3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download the generated lyrics file:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='generated_lyrics.txt' target='_blank'>generated_lyrics.txt</a><br>"
      ],
      "text/plain": [
       "/content/generated_lyrics.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Playing generated audio file: generated_music_conditional.mp3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/mpeg;base64,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\" type=\"audio/mpeg\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the user-provided audio file for conditioning\n",
    "user_audio_input_path = '/content/testfile.mpeg'\n",
    "\n",
    "# Use a generic text prompt for now\n",
    "user_text_prompt = \"Generate a piece of music.\"\n",
    "\n",
    "print(f\"\\nAttempting to generate music from text prompt: '{user_text_prompt}' using audio file at {user_audio_input_path}\")\n",
    "generated_music_file, generated_lyrics_text = generate_music_from_input_v2(\n",
    "    model_gen_conditioned_v2,\n",
    "    user_text_input=user_text_prompt,\n",
    "    user_audio_input_path=user_audio_input_path,\n",
    "    input_type='text_prompt',\n",
    "    generation_steps=200, # You can adjust this\n",
    "    sr=sr,\n",
    "    max_padding=max_padding,\n",
    "    n_mfcc=n_mfcc,\n",
    "    n_chroma=n_chroma,\n",
    "    text_conditioning_dim=text_conditioning_dim\n",
    ")\n",
    "\n",
    "if generated_music_file:\n",
    "    print(f\"\\nGenerated music file: {generated_music_file}\")\n",
    "    if generated_lyrics_text:\n",
    "        print(f\"Generated lyrics:\\n{generated_lyrics_text}\")\n",
    "    # Display links to the generated files\n",
    "    print(\"\\nDownload the generated music file:\")\n",
    "    display(FileLink(generated_music_file))\n",
    "    if generated_lyrics_text: # Only display lyrics link if lyrics were generated\n",
    "        print(\"Download the generated lyrics file:\")\n",
    "        display(FileLink('generated_lyrics.txt')) # Note: This will overwrite the previous lyrics file\n",
    "\n",
    "    # Play the generated audio\n",
    "    print(f\"\\nPlaying generated audio file: {generated_music_file}\")\n",
    "    display(Audio(generated_music_file))\n",
    "\n",
    "else:\n",
    "    print(\"\\nMusic generation failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2e9d97a6",
    "outputId": "f8c94577-7910-4ce1-ea1d-c9f1b06c38da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding with the next step in the plan: Modifying the generation function for iterative output.\n"
     ]
    }
   ],
   "source": [
    "# Use the user-provided audio file for conditioning\n",
    "user_audio_input_path = '/content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg'\n",
    "\n",
    "# Use a generic text prompt for now\n",
    "user_text_prompt = \"Generate a piece of background music.\"\n",
    "\n",
    "# Calculate the number of generation steps needed for at least 30 seconds\n",
    "# sr (sampling rate) = 22050\n",
    "# max_padding (sequence length) = 174\n",
    "# time per step = (max_padding / sr) seconds\n",
    "# To get at least 30 seconds, we need:\n",
    "# total_time = generation_steps * (max_padding / sr)\n",
    "# generation_steps = total_time * (sr / max_padding)\n",
    "# For 30 seconds: generation_steps = 30 * (22050 / 174) approx 30 * 126.78 approx 3803\n",
    "# Let's set generation_steps to a reasonable value that produces at least 30 seconds.\n",
    "# Since our model generates a full sequence of length max_padding at once,\n",
    "# the number of \"steps\" in the generate_music_from_input_v3 function refers to the number of generated time steps.\n",
    "# To get a longer output, we need to increase the `max_padding` when training the model,\n",
    "# or generate multiple sequences and combine them.\n",
    "# Given the current model generates a sequence of `max_padding` (174) time steps at once,\n",
    "# and each time step corresponds to hop_length samples (default 512 in librosa mfcc),\n",
    "# the duration of one generated sequence is (max_padding * hop_length) / sr\n",
    "# (174 * 512) / 22050 approx 4 seconds.\n",
    "# To get 30 seconds, we would theoretically need to generate and combine multiple sequences.\n",
    "# However, the current `generate_musical_features` function generates a *single* sequence of `max_padding` length.\n",
    "# To achieve a longer output with the current model architecture without retraining,\n",
    "# we would need to modify the generation function to be truly iterative,\n",
    "# or generate multiple segments and stitch them together.\n",
    "\n",
    "# For demonstration with the current setup, let's generate a single sequence.\n",
    "# To aim for a longer output in the future, the model architecture or generation process needs modification.\n",
    "# For now, let's keep generation_steps to 200 as in previous examples, but acknowledge the limitation.\n",
    "# If we were using the iterative generation approach (like in generate_conditional_mfccs in older versions),\n",
    "# we could set generation_steps to a higher value.\n",
    "\n",
    "# Let's try a higher generation_steps value in the generate_music_from_input_v3\n",
    "# even though the underlying model call in generate_musical_features still produces a single sequence of max_padding length.\n",
    "# This might not produce a *longer* single audio file with the current setup,\n",
    "# but if the generation function was truly iterative, this is where you'd increase the length.\n",
    "# Let's generate music. The length will be determined by the model's output sequence length (max_padding).\n",
    "# To get a longer output, the approach needs modification (iterative generation or segment stitching).\n",
    "\n",
    "# Let's try generating a few sequences and stitching them. This requires modifying the generation process.\n",
    "# As per the plan, the next step is to modify the `generate_conditional_mfccs_v2` function for iterative generation.\n",
    "\n",
    "# Skipping direct generation for now, as the previous attempts showed the generated audio is short due to the model's fixed output length.\n",
    "# The plan requires modifying the generation function to be iterative to achieve longer outputs.\n",
    "\n",
    "print(\"Proceeding with the next step in the plan: Modifying the generation function for iterative output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "154409a8",
    "outputId": "9d9fca29-bdde-4a78-915d-61c1b1d7c5a7"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "InteractiveShell.run_cell() missing 1 required positional argument: 'raw_cell'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-625372584.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Execute the cell containing the updated generation functions and model definition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'519a67b2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Now generate music using the updated function (generate_music_from_input_v4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0muser_audio_input_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36mrun_cell\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: InteractiveShell.run_cell() missing 1 required positional argument: 'raw_cell'"
     ]
    }
   ],
   "source": [
    "# Execute the cell containing the updated generation functions and model definition\n",
    "get_ipython().run_cell(cell_id='519a67b2')\n",
    "\n",
    "# Now generate music using the updated function (generate_music_from_input_v4)\n",
    "user_audio_input_path = '/content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg'\n",
    "user_text_prompt = \"Generate a piece of background music.\"\n",
    "target_duration_seconds = 30\n",
    "\n",
    "print(f\"\\nAttempting to generate music from text prompt: '{user_text_prompt}' using audio file at {user_audio_input_path}, targeting {target_duration_seconds} seconds with the iterative generation.\")\n",
    "generated_music_file, generated_lyrics_text = generate_music_from_input_v4(\n",
    "    model_gen_conditioned_v2,\n",
    "    user_text_input=user_text_prompt,\n",
    "    user_audio_input_path=user_audio_input_path,\n",
    "    input_type='text_prompt',\n",
    "    target_audio_duration_sec=target_duration_seconds,\n",
    "    sr=sr,\n",
    "    max_padding=max_padding,\n",
    "    n_mfcc=n_mfcc,\n",
    "    n_chroma=n_chroma,\n",
    "    text_conditioning_dim=text_conditioning_dim\n",
    ")\n",
    "\n",
    "if generated_music_file:\n",
    "    print(f\"\\nGenerated music file: {generated_music_file}\")\n",
    "    if generated_lyrics_text:\n",
    "        print(f\"Generated lyrics:\\n{generated_lyrics_text}\")\n",
    "    # Display links to the generated files\n",
    "    print(\"\\nDownload the generated music file:\")\n",
    "    display(FileLink(generated_music_file))\n",
    "    if generated_lyrics_text: # Only display lyrics link if lyrics were generated\n",
    "        print(\"Download the generated lyrics file:\")\n",
    "        display(FileLink('generated_lyrics.txt'))\n",
    "\n",
    "    # Play the generated audio\n",
    "    print(f\"\\nPlaying generated audio file: {generated_music_file}\")\n",
    "    display(Audio(generated_music_file))\n",
    "\n",
    "else:\n",
    "    print(\"\\nMusic generation failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "0feda61b",
    "outputId": "2d79f889-2a3c-47bb-b436-eb11d4bd99d9"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Shell' object has no attribute 'get_cell'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3547654278.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Get the cell content and then run the cell with both id and content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcell_id_to_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'519a67b2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcell_content_to_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_id_to_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_cell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcell_content_to_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcell_id_to_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Shell' object has no attribute 'get_cell'"
     ]
    }
   ],
   "source": [
    "# Execute the cell containing the updated generation functions and model definition\n",
    "# Get the cell content and then run the cell with both id and content\n",
    "cell_id_to_run = '519a67b2'\n",
    "cell_content_to_run = get_ipython().get_cell(cell_id_to_run).get('text', '')\n",
    "get_ipython().run_cell(raw_cell=cell_content_to_run, cell_id=cell_id_to_run)\n",
    "\n",
    "\n",
    "# Now generate music using the updated function (generate_music_from_input_v4)\n",
    "user_audio_input_path = '/content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg'\n",
    "user_text_prompt = \"Generate a piece of background music.\"\n",
    "target_duration_seconds = 30\n",
    "\n",
    "print(f\"\\nAttempting to generate music from text prompt: '{user_text_prompt}' using audio file at {user_audio_input_path}, targeting {target_duration_seconds} seconds with the iterative generation.\")\n",
    "generated_music_file, generated_lyrics_text = generate_music_from_input_v4(\n",
    "    model_gen_conditioned_v2,\n",
    "    user_text_input=user_text_prompt,\n",
    "    user_audio_input_path=user_audio_input_path,\n",
    "    input_type='text_prompt',\n",
    "    target_audio_duration_sec=target_duration_seconds,\n",
    "    sr=sr,\n",
    "    max_padding=max_padding,\n",
    "    n_mfcc=n_mfcc,\n",
    "    n_chroma=n_chroma,\n",
    "    text_conditioning_dim=text_conditioning_dim\n",
    ")\n",
    "\n",
    "if generated_music_file:\n",
    "    print(f\"\\nGenerated music file: {generated_music_file}\")\n",
    "    if generated_lyrics_text:\n",
    "        print(f\"Generated lyrics:\\n{generated_lyrics_text}\")\n",
    "    # Display links to the generated files\n",
    "    print(\"\\nDownload the generated music file:\")\n",
    "    display(FileLink(generated_music_file))\n",
    "    if generated_lyrics_text: # Only display lyrics link if lyrics were generated\n",
    "        print(\"Download the generated lyrics file:\")\n",
    "        display(FileLink('generated_lyrics.txt'))\n",
    "\n",
    "    # Play the generated audio\n",
    "    print(f\"\\nPlaying generated audio file: {generated_music_file}\")\n",
    "    display(Audio(generated_music_file))\n",
    "\n",
    "else:\n",
    "    print(\"\\nMusic generation failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1b11bff",
    "outputId": "7ce5f337-c9f8-43f2-f5fa-1a95d918b819"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generation pipeline (distilgpt2) initialized.\n",
      "Using device: cpu\n",
      "\n",
      "PyTorch conditional music generation model architecture defined.\n",
      "MusicGenerator(\n",
      "  (gru1): GRU(180, 128, batch_first=True, bidirectional=True)\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (gru2): GRU(256, 128, batch_first=True, bidirectional=True)\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  (linear): Linear(in_features=256, out_features=20, bias=True)\n",
      ")\n",
      "Downloading the dataset to ensure path is available...\n",
      "Using Colab cache for faster access to the 'musical-instruments-sound-dataset' dataset.\n",
      "Dataset path: /kaggle/input/musical-instruments-sound-dataset\n",
      "Searching for music instrument files in: /kaggle/input/musical-instruments-sound-dataset/Train_submission/Train_submission\n",
      "Found 1965 music instrument files in training submission.\n",
      "Searching for music instrument files in: /kaggle/input/musical-instruments-sound-dataset/Test_submission/Test_submission\n",
      "Found 2029 music instrument files in test submission.\n",
      "\n",
      "Extracting musical features (MFCCs) and audio conditioning features (MFCC+Chroma) from music instrument files...\n",
      "Processing file 100/2029\n",
      "Processing file 200/2029\n",
      "Processing file 300/2029\n",
      "Processing file 400/2029\n",
      "Processing file 500/2029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 600/2029\n",
      "Processing file 700/2029\n",
      "Processing file 800/2029\n",
      "Processing file 900/2029\n",
      "Processing file 1000/2029\n",
      "Processing file 1100/2029\n",
      "Processing file 1200/2029\n",
      "Processing file 1300/2029\n",
      "Processing file 1400/2029\n",
      "Processing file 1500/2029\n",
      "Processing file 1600/2029\n",
      "Processing file 1700/2029\n",
      "Processing file 1800/2029\n",
      "Processing file 1900/2029\n",
      "Processing file 2000/2029\n",
      "\n",
      "Feature extraction and data splitting complete.\n",
      "Shape of training musical features (MFCCs): (1623, 174, 20)\n",
      "Shape of validation musical features (MFCCs): (406, 174, 20)\n",
      "Shape of training audio conditioning features (MFCC+Chroma): (1623, 174, 32)\n",
      "Shape of validation audio conditioning features (MFCC+Chroma): (406, 174, 32)\n",
      "\n",
      "Shape of dummy training text conditioning data: (1623, 128)\n",
      "Shape of dummy validation text conditioning data: (406, 128)\n",
      "\n",
      "Data converted to PyTorch tensors and DataLoaders created for training.\n",
      "\n",
      "Starting PyTorch model training for 50 epochs...\n",
      "Epoch [1/50], Step [10/51], Loss: 13044.3564\n",
      "Epoch [1/50], Step [20/51], Loss: 13682.9248\n",
      "Epoch [1/50], Step [30/51], Loss: 11858.1621\n",
      "Epoch [1/50], Step [40/51], Loss: 12427.2090\n",
      "Epoch [1/50], Step [50/51], Loss: 12124.8291\n",
      "Epoch [1/50] finished. Average Training Loss: 13900.3151\n",
      "Validation Loss: 13461.7753\n",
      "Epoch [2/50], Step [10/51], Loss: 14580.5117\n",
      "Epoch [2/50], Step [20/51], Loss: 12606.6191\n",
      "Epoch [2/50], Step [30/51], Loss: 14277.3711\n",
      "Epoch [2/50], Step [40/51], Loss: 12080.1084\n",
      "Epoch [2/50], Step [50/51], Loss: 15535.3027\n",
      "Epoch [2/50] finished. Average Training Loss: 13087.6206\n",
      "Validation Loss: 12753.4180\n",
      "Epoch [3/50], Step [10/51], Loss: 13958.4697\n",
      "Epoch [3/50], Step [20/51], Loss: 12920.2686\n",
      "Epoch [3/50], Step [30/51], Loss: 14029.7129\n",
      "Epoch [3/50], Step [40/51], Loss: 12876.0283\n",
      "Epoch [3/50], Step [50/51], Loss: 12583.9229\n",
      "Epoch [3/50] finished. Average Training Loss: 12419.5610\n",
      "Validation Loss: 12105.3404\n",
      "Epoch [4/50], Step [10/51], Loss: 10406.3320\n",
      "Epoch [4/50], Step [20/51], Loss: 11904.3223\n",
      "Epoch [4/50], Step [30/51], Loss: 11951.7344\n",
      "Epoch [4/50], Step [40/51], Loss: 9977.5586\n",
      "Epoch [4/50], Step [50/51], Loss: 12592.4453\n",
      "Epoch [4/50] finished. Average Training Loss: 11798.7700\n",
      "Validation Loss: 11502.9577\n",
      "Epoch [5/50], Step [10/51], Loss: 11196.6084\n",
      "Epoch [5/50], Step [20/51], Loss: 11942.4492\n",
      "Epoch [5/50], Step [30/51], Loss: 13211.3193\n",
      "Epoch [5/50], Step [40/51], Loss: 10723.5303\n",
      "Epoch [5/50], Step [50/51], Loss: 10765.4131\n",
      "Epoch [5/50] finished. Average Training Loss: 11215.2503\n",
      "Validation Loss: 10940.7950\n",
      "Epoch [6/50], Step [10/51], Loss: 9671.0332\n",
      "Epoch [6/50], Step [20/51], Loss: 11225.0342\n",
      "Epoch [6/50], Step [30/51], Loss: 8350.2930\n",
      "Epoch [6/50], Step [40/51], Loss: 8252.2812\n",
      "Epoch [6/50], Step [50/51], Loss: 10935.4609\n",
      "Epoch [6/50] finished. Average Training Loss: 10669.9773\n",
      "Validation Loss: 10412.5711\n",
      "Epoch [7/50], Step [10/51], Loss: 10314.3701\n",
      "Epoch [7/50], Step [20/51], Loss: 8733.7100\n",
      "Epoch [7/50], Step [30/51], Loss: 9992.0537\n",
      "Epoch [7/50], Step [40/51], Loss: 9529.5430\n",
      "Epoch [7/50], Step [50/51], Loss: 9766.1641\n",
      "Epoch [7/50] finished. Average Training Loss: 10169.7745\n",
      "Validation Loss: 9912.9604\n",
      "Epoch [8/50], Step [10/51], Loss: 9615.9209\n",
      "Epoch [8/50], Step [20/51], Loss: 8904.9219\n",
      "Epoch [8/50], Step [30/51], Loss: 8301.6016\n",
      "Epoch [8/50], Step [40/51], Loss: 11203.7266\n",
      "Epoch [8/50], Step [50/51], Loss: 7748.9121\n",
      "Epoch [8/50] finished. Average Training Loss: 9667.8355\n",
      "Validation Loss: 9438.7410\n",
      "Epoch [9/50], Step [10/51], Loss: 9419.5000\n",
      "Epoch [9/50], Step [20/51], Loss: 9925.6582\n",
      "Epoch [9/50], Step [30/51], Loss: 7417.0688\n",
      "Epoch [9/50], Step [40/51], Loss: 9662.4727\n",
      "Epoch [9/50], Step [50/51], Loss: 7466.3867\n",
      "Epoch [9/50] finished. Average Training Loss: 9203.3250\n",
      "Validation Loss: 8991.7905\n",
      "Epoch [10/50], Step [10/51], Loss: 9088.3584\n",
      "Epoch [10/50], Step [20/51], Loss: 10548.8594\n",
      "Epoch [10/50], Step [30/51], Loss: 7754.9233\n",
      "Epoch [10/50], Step [40/51], Loss: 9681.1602\n",
      "Epoch [10/50], Step [50/51], Loss: 9215.6973\n",
      "Epoch [10/50] finished. Average Training Loss: 8761.1998\n",
      "Validation Loss: 8565.7656\n",
      "Epoch [11/50], Step [10/51], Loss: 7900.4883\n",
      "Epoch [11/50], Step [20/51], Loss: 8548.4590\n",
      "Epoch [11/50], Step [30/51], Loss: 7838.6533\n",
      "Epoch [11/50], Step [40/51], Loss: 8949.2021\n",
      "Epoch [11/50], Step [50/51], Loss: 7412.0864\n",
      "Epoch [11/50] finished. Average Training Loss: 8359.2937\n",
      "Validation Loss: 8157.9142\n",
      "Epoch [12/50], Step [10/51], Loss: 9070.0137\n",
      "Epoch [12/50], Step [20/51], Loss: 7763.1426\n",
      "Epoch [12/50], Step [30/51], Loss: 9541.1641\n",
      "Epoch [12/50], Step [40/51], Loss: 7391.1650\n",
      "Epoch [12/50], Step [50/51], Loss: 7817.1719\n",
      "Epoch [12/50] finished. Average Training Loss: 7950.6669\n",
      "Validation Loss: 7771.2773\n",
      "Epoch [13/50], Step [10/51], Loss: 8610.6367\n",
      "Epoch [13/50], Step [20/51], Loss: 7641.2573\n",
      "Epoch [13/50], Step [30/51], Loss: 7198.3838\n",
      "Epoch [13/50], Step [40/51], Loss: 7269.3120\n",
      "Epoch [13/50], Step [50/51], Loss: 6859.1982\n",
      "Epoch [13/50] finished. Average Training Loss: 7567.9816\n",
      "Validation Loss: 7403.4408\n",
      "Epoch [14/50], Step [10/51], Loss: 6933.9956\n",
      "Epoch [14/50], Step [20/51], Loss: 7133.7495\n",
      "Epoch [14/50], Step [30/51], Loss: 6886.8472\n",
      "Epoch [14/50], Step [40/51], Loss: 7487.7124\n",
      "Epoch [14/50], Step [50/51], Loss: 7209.4585\n",
      "Epoch [14/50] finished. Average Training Loss: 7196.4757\n",
      "Validation Loss: 7051.8156\n",
      "Epoch [15/50], Step [10/51], Loss: 7420.7617\n",
      "Epoch [15/50], Step [20/51], Loss: 6916.9448\n",
      "Epoch [15/50], Step [30/51], Loss: 7101.3828\n",
      "Epoch [15/50], Step [40/51], Loss: 7511.4556\n",
      "Epoch [15/50], Step [50/51], Loss: 6975.3613\n",
      "Epoch [15/50] finished. Average Training Loss: 6861.1448\n",
      "Validation Loss: 6719.0730\n",
      "Epoch [16/50], Step [10/51], Loss: 6036.5444\n",
      "Epoch [16/50], Step [20/51], Loss: 6295.8862\n",
      "Epoch [16/50], Step [30/51], Loss: 6061.3271\n",
      "Epoch [16/50], Step [40/51], Loss: 6952.3535\n",
      "Epoch [16/50], Step [50/51], Loss: 4923.3477\n",
      "Epoch [16/50] finished. Average Training Loss: 6528.2215\n",
      "Validation Loss: 6398.6920\n",
      "Epoch [17/50], Step [10/51], Loss: 5642.0254\n",
      "Epoch [17/50], Step [20/51], Loss: 5945.3872\n",
      "Epoch [17/50], Step [30/51], Loss: 6465.7007\n",
      "Epoch [17/50], Step [40/51], Loss: 7033.7500\n",
      "Epoch [17/50], Step [50/51], Loss: 5816.7124\n",
      "Epoch [17/50] finished. Average Training Loss: 6222.6526\n",
      "Validation Loss: 6097.5927\n",
      "Epoch [18/50], Step [10/51], Loss: 5200.0088\n",
      "Epoch [18/50], Step [20/51], Loss: 5243.9697\n",
      "Epoch [18/50], Step [30/51], Loss: 6205.8804\n",
      "Epoch [18/50], Step [40/51], Loss: 6592.4253\n",
      "Epoch [18/50], Step [50/51], Loss: 5734.3208\n",
      "Epoch [18/50] finished. Average Training Loss: 5928.8731\n",
      "Validation Loss: 5809.4174\n",
      "Epoch [19/50], Step [10/51], Loss: 4702.7192\n",
      "Epoch [19/50], Step [20/51], Loss: 4932.8701\n",
      "Epoch [19/50], Step [30/51], Loss: 6199.1709\n",
      "Epoch [19/50], Step [40/51], Loss: 5186.0293\n",
      "Epoch [19/50], Step [50/51], Loss: 5022.6597\n",
      "Epoch [19/50] finished. Average Training Loss: 5643.6486\n",
      "Validation Loss: 5535.2293\n",
      "Epoch [20/50], Step [10/51], Loss: 5391.1621\n",
      "Epoch [20/50], Step [20/51], Loss: 4998.9443\n",
      "Epoch [20/50], Step [30/51], Loss: 4864.6772\n",
      "Epoch [20/50], Step [40/51], Loss: 4414.7520\n",
      "Epoch [20/50], Step [50/51], Loss: 6122.6567\n",
      "Epoch [20/50] finished. Average Training Loss: 5379.1795\n",
      "Validation Loss: 5276.8279\n",
      "Epoch [21/50], Step [10/51], Loss: 5859.1016\n",
      "Epoch [21/50], Step [20/51], Loss: 4709.8677\n",
      "Epoch [21/50], Step [30/51], Loss: 5362.0190\n",
      "Epoch [21/50], Step [40/51], Loss: 5531.5884\n",
      "Epoch [21/50], Step [50/51], Loss: 5561.5645\n",
      "Epoch [21/50] finished. Average Training Loss: 5125.2677\n",
      "Validation Loss: 5029.6725\n",
      "Epoch [22/50], Step [10/51], Loss: 4688.1499\n",
      "Epoch [22/50], Step [20/51], Loss: 5755.5654\n",
      "Epoch [22/50], Step [30/51], Loss: 5606.4082\n",
      "Epoch [22/50], Step [40/51], Loss: 4029.5847\n",
      "Epoch [22/50], Step [50/51], Loss: 5234.4331\n",
      "Epoch [22/50] finished. Average Training Loss: 4876.5988\n",
      "Validation Loss: 4795.3260\n",
      "Epoch [23/50], Step [10/51], Loss: 5510.2412\n",
      "Epoch [23/50], Step [20/51], Loss: 3735.9751\n",
      "Epoch [23/50], Step [30/51], Loss: 5562.6504\n",
      "Epoch [23/50], Step [40/51], Loss: 5949.1582\n",
      "Epoch [23/50], Step [50/51], Loss: 3738.8669\n",
      "Epoch [23/50] finished. Average Training Loss: 4657.7846\n",
      "Validation Loss: 4573.5967\n",
      "Epoch [24/50], Step [10/51], Loss: 3958.1353\n",
      "Epoch [24/50], Step [20/51], Loss: 4712.4956\n",
      "Epoch [24/50], Step [30/51], Loss: 4428.5127\n",
      "Epoch [24/50], Step [40/51], Loss: 3268.5120\n",
      "Epoch [24/50], Step [50/51], Loss: 4296.2427\n",
      "Epoch [24/50] finished. Average Training Loss: 4437.1297\n",
      "Validation Loss: 4363.7980\n",
      "Epoch [25/50], Step [10/51], Loss: 4322.0332\n",
      "Epoch [25/50], Step [20/51], Loss: 4373.0669\n",
      "Epoch [25/50], Step [30/51], Loss: 4061.1455\n",
      "Epoch [25/50], Step [40/51], Loss: 5500.2988\n",
      "Epoch [25/50], Step [50/51], Loss: 3823.5046\n",
      "Epoch [25/50] finished. Average Training Loss: 4227.5944\n",
      "Validation Loss: 4163.6550\n",
      "Epoch [26/50], Step [10/51], Loss: 4576.4077\n",
      "Epoch [26/50], Step [20/51], Loss: 3001.3911\n",
      "Epoch [26/50], Step [30/51], Loss: 4204.8984\n",
      "Epoch [26/50], Step [40/51], Loss: 3434.6157\n",
      "Epoch [26/50], Step [50/51], Loss: 6779.2236\n",
      "Epoch [26/50] finished. Average Training Loss: 4036.5207\n",
      "Validation Loss: 3976.7391\n",
      "Epoch [27/50], Step [10/51], Loss: 3716.7891\n",
      "Epoch [27/50], Step [20/51], Loss: 3303.5564\n",
      "Epoch [27/50], Step [30/51], Loss: 4193.8042\n",
      "Epoch [27/50], Step [40/51], Loss: 4138.2026\n",
      "Epoch [27/50], Step [50/51], Loss: 4137.1675\n",
      "Epoch [27/50] finished. Average Training Loss: 3856.7287\n",
      "Validation Loss: 3798.5128\n",
      "Epoch [28/50], Step [10/51], Loss: 3625.9573\n",
      "Epoch [28/50], Step [20/51], Loss: 3199.1458\n",
      "Epoch [28/50], Step [30/51], Loss: 2752.3135\n",
      "Epoch [28/50], Step [40/51], Loss: 4939.3252\n",
      "Epoch [28/50], Step [50/51], Loss: 4192.2402\n",
      "Epoch [28/50] finished. Average Training Loss: 3676.8787\n",
      "Validation Loss: 3630.0804\n",
      "Epoch [29/50], Step [10/51], Loss: 3715.2932\n",
      "Epoch [29/50], Step [20/51], Loss: 2881.6284\n",
      "Epoch [29/50], Step [30/51], Loss: 3752.0681\n",
      "Epoch [29/50], Step [40/51], Loss: 3991.8411\n",
      "Epoch [29/50], Step [50/51], Loss: 3355.0720\n",
      "Epoch [29/50] finished. Average Training Loss: 3518.1702\n",
      "Validation Loss: 3469.4730\n",
      "Epoch [30/50], Step [10/51], Loss: 3975.5029\n",
      "Epoch [30/50], Step [20/51], Loss: 2566.8813\n",
      "Epoch [30/50], Step [30/51], Loss: 3181.9495\n",
      "Epoch [30/50], Step [40/51], Loss: 2795.7170\n",
      "Epoch [30/50], Step [50/51], Loss: 4571.3267\n",
      "Epoch [30/50] finished. Average Training Loss: 3347.0754\n",
      "Validation Loss: 3318.3480\n",
      "Epoch [31/50], Step [10/51], Loss: 2416.2256\n",
      "Epoch [31/50], Step [20/51], Loss: 3165.3367\n",
      "Epoch [31/50], Step [30/51], Loss: 3250.4502\n",
      "Epoch [31/50], Step [40/51], Loss: 2968.1909\n",
      "Epoch [31/50], Step [50/51], Loss: 2753.9141\n",
      "Epoch [31/50] finished. Average Training Loss: 3205.1945\n",
      "Validation Loss: 3174.5636\n",
      "Epoch [32/50], Step [10/51], Loss: 2307.9453\n",
      "Epoch [32/50], Step [20/51], Loss: 2005.1119\n",
      "Epoch [32/50], Step [30/51], Loss: 3553.8008\n",
      "Epoch [32/50], Step [40/51], Loss: 4481.6567\n",
      "Epoch [32/50], Step [50/51], Loss: 2573.7700\n",
      "Epoch [32/50] finished. Average Training Loss: 3061.3310\n",
      "Validation Loss: 3037.3201\n",
      "Epoch [33/50], Step [10/51], Loss: 2618.1462\n",
      "Epoch [33/50], Step [20/51], Loss: 2859.9282\n",
      "Epoch [33/50], Step [30/51], Loss: 3632.3616\n",
      "Epoch [33/50], Step [40/51], Loss: 2798.2324\n",
      "Epoch [33/50], Step [50/51], Loss: 2810.2759\n",
      "Epoch [33/50] finished. Average Training Loss: 2934.0368\n",
      "Validation Loss: 2909.1687\n",
      "Epoch [34/50], Step [10/51], Loss: 2103.5012\n",
      "Epoch [34/50], Step [20/51], Loss: 2896.6040\n",
      "Epoch [34/50], Step [30/51], Loss: 3876.7761\n",
      "Epoch [34/50], Step [40/51], Loss: 2960.2891\n",
      "Epoch [34/50], Step [50/51], Loss: 2743.3853\n",
      "Epoch [34/50] finished. Average Training Loss: 2801.4423\n",
      "Validation Loss: 2782.8489\n",
      "Epoch [35/50], Step [10/51], Loss: 2634.4810\n",
      "Epoch [35/50], Step [20/51], Loss: 3346.1460\n",
      "Epoch [35/50], Step [30/51], Loss: 3001.9778\n",
      "Epoch [35/50], Step [40/51], Loss: 2847.4902\n",
      "Epoch [35/50], Step [50/51], Loss: 2809.5864\n",
      "Epoch [35/50] finished. Average Training Loss: 2681.5497\n",
      "Validation Loss: 2663.6730\n",
      "Epoch [36/50], Step [10/51], Loss: 3770.6155\n",
      "Epoch [36/50], Step [20/51], Loss: 2493.9834\n",
      "Epoch [36/50], Step [30/51], Loss: 3029.3572\n",
      "Epoch [36/50], Step [40/51], Loss: 3010.1086\n",
      "Epoch [36/50], Step [50/51], Loss: 1674.4158\n",
      "Epoch [36/50] finished. Average Training Loss: 2567.9139\n",
      "Validation Loss: 2547.9227\n",
      "Epoch [37/50], Step [10/51], Loss: 1758.2638\n",
      "Epoch [37/50], Step [20/51], Loss: 2246.2485\n",
      "Epoch [37/50], Step [30/51], Loss: 2846.4084\n",
      "Epoch [37/50], Step [40/51], Loss: 2389.0286\n",
      "Epoch [37/50], Step [50/51], Loss: 3149.1772\n",
      "Epoch [37/50] finished. Average Training Loss: 2449.9154\n",
      "Validation Loss: 2437.3089\n",
      "Epoch [38/50], Step [10/51], Loss: 2875.2961\n",
      "Epoch [38/50], Step [20/51], Loss: 2415.8643\n",
      "Epoch [38/50], Step [30/51], Loss: 2260.6772\n",
      "Epoch [38/50], Step [40/51], Loss: 2787.8269\n",
      "Epoch [38/50], Step [50/51], Loss: 2136.7026\n",
      "Epoch [38/50] finished. Average Training Loss: 2341.9660\n",
      "Validation Loss: 2329.4731\n",
      "Epoch [39/50], Step [10/51], Loss: 3405.0833\n",
      "Epoch [39/50], Step [20/51], Loss: 2626.7305\n",
      "Epoch [39/50], Step [30/51], Loss: 2093.5762\n",
      "Epoch [39/50], Step [40/51], Loss: 2262.2932\n",
      "Epoch [39/50], Step [50/51], Loss: 2829.1504\n",
      "Epoch [39/50] finished. Average Training Loss: 2239.0190\n",
      "Validation Loss: 2226.5726\n",
      "Epoch [40/50], Step [10/51], Loss: 3781.5452\n",
      "Epoch [40/50], Step [20/51], Loss: 2106.9268\n",
      "Epoch [40/50], Step [30/51], Loss: 2039.9766\n",
      "Epoch [40/50], Step [40/51], Loss: 2713.6677\n",
      "Epoch [40/50], Step [50/51], Loss: 1902.7567\n",
      "Epoch [40/50] finished. Average Training Loss: 2142.9118\n",
      "Validation Loss: 2128.5007\n",
      "Epoch [41/50], Step [10/51], Loss: 2888.1284\n",
      "Epoch [41/50], Step [20/51], Loss: 2627.9688\n",
      "Epoch [41/50], Step [30/51], Loss: 1739.8818\n",
      "Epoch [41/50], Step [40/51], Loss: 2162.5872\n",
      "Epoch [41/50], Step [50/51], Loss: 1858.6399\n",
      "Epoch [41/50] finished. Average Training Loss: 2041.0998\n",
      "Validation Loss: 2030.2393\n",
      "Epoch [42/50], Step [10/51], Loss: 2378.7256\n",
      "Epoch [42/50], Step [20/51], Loss: 1968.0736\n",
      "Epoch [42/50], Step [30/51], Loss: 2548.3340\n",
      "Epoch [42/50], Step [40/51], Loss: 1889.5206\n",
      "Epoch [42/50], Step [50/51], Loss: 2015.1837\n",
      "Epoch [42/50] finished. Average Training Loss: 1945.7258\n",
      "Validation Loss: 1937.4269\n",
      "Epoch [43/50], Step [10/51], Loss: 1781.7185\n",
      "Epoch [43/50], Step [20/51], Loss: 1975.8948\n",
      "Epoch [43/50], Step [30/51], Loss: 1812.7269\n",
      "Epoch [43/50], Step [40/51], Loss: 1607.2792\n",
      "Epoch [43/50], Step [50/51], Loss: 2868.0142\n",
      "Epoch [43/50] finished. Average Training Loss: 1856.6208\n",
      "Validation Loss: 1849.0980\n",
      "Epoch [44/50], Step [10/51], Loss: 1804.1698\n",
      "Epoch [44/50], Step [20/51], Loss: 1516.4556\n",
      "Epoch [44/50], Step [30/51], Loss: 1156.3297\n",
      "Epoch [44/50], Step [40/51], Loss: 2037.0115\n",
      "Epoch [44/50], Step [50/51], Loss: 1566.3025\n",
      "Epoch [44/50] finished. Average Training Loss: 1767.7913\n",
      "Validation Loss: 1763.5449\n",
      "Epoch [45/50], Step [10/51], Loss: 2237.4937\n",
      "Epoch [45/50], Step [20/51], Loss: 1833.2543\n",
      "Epoch [45/50], Step [30/51], Loss: 1346.8224\n",
      "Epoch [45/50], Step [40/51], Loss: 1417.5107\n",
      "Epoch [45/50], Step [50/51], Loss: 1485.9727\n",
      "Epoch [45/50] finished. Average Training Loss: 1687.8350\n",
      "Validation Loss: 1682.1621\n",
      "Epoch [46/50], Step [10/51], Loss: 1723.0957\n",
      "Epoch [46/50], Step [20/51], Loss: 1299.1969\n",
      "Epoch [46/50], Step [30/51], Loss: 1560.6431\n",
      "Epoch [46/50], Step [40/51], Loss: 2053.4443\n",
      "Epoch [46/50], Step [50/51], Loss: 1340.4578\n",
      "Epoch [46/50] finished. Average Training Loss: 1607.9589\n",
      "Validation Loss: 1603.8011\n",
      "Epoch [47/50], Step [10/51], Loss: 1442.8566\n",
      "Epoch [47/50], Step [20/51], Loss: 1716.3440\n",
      "Epoch [47/50], Step [30/51], Loss: 1364.5786\n",
      "Epoch [47/50], Step [40/51], Loss: 1442.4269\n",
      "Epoch [47/50], Step [50/51], Loss: 1661.5439\n",
      "Epoch [47/50] finished. Average Training Loss: 1532.4088\n",
      "Validation Loss: 1528.7084\n",
      "Epoch [48/50], Step [10/51], Loss: 1361.6661\n",
      "Epoch [48/50], Step [20/51], Loss: 1006.2292\n",
      "Epoch [48/50], Step [30/51], Loss: 1507.5881\n",
      "Epoch [48/50], Step [40/51], Loss: 1370.5715\n",
      "Epoch [48/50], Step [50/51], Loss: 1054.8689\n",
      "Epoch [48/50] finished. Average Training Loss: 1461.3671\n",
      "Validation Loss: 1458.5990\n",
      "Epoch [49/50], Step [10/51], Loss: 1632.1016\n",
      "Epoch [49/50], Step [20/51], Loss: 1940.4636\n",
      "Epoch [49/50], Step [30/51], Loss: 1132.4580\n",
      "Epoch [49/50], Step [40/51], Loss: 1968.4333\n",
      "Epoch [49/50], Step [50/51], Loss: 945.3263\n",
      "Epoch [49/50] finished. Average Training Loss: 1389.5119\n",
      "Validation Loss: 1389.1513\n",
      "Epoch [50/50], Step [10/51], Loss: 1384.0363\n",
      "Epoch [50/50], Step [20/51], Loss: 1122.9252\n",
      "Epoch [50/50], Step [30/51], Loss: 1009.8309\n",
      "Epoch [50/50], Step [40/51], Loss: 1479.5359\n",
      "Epoch [50/50], Step [50/51], Loss: 991.5752\n",
      "Epoch [50/50] finished. Average Training Loss: 1327.7760\n",
      "Validation Loss: 1324.6088\n",
      "\n",
      "PyTorch model training complete.\n",
      "Error: User audio file not found at /content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg. Please upload the file.\n",
      "\n",
      "Skipping music generation as user audio file was not found.\n"
     ]
    }
   ],
   "source": [
    "# Regenerate the content of cell '519a67b2' to ensure functions and model are defined\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "from pydub import AudioSegment # Import AudioSegment for MP3 conversion\n",
    "from transformers import pipeline # Import pipeline for using pre-trained models\n",
    "from IPython.display import display, FileLink, Audio # Import display, FileLink, and Audio\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split for data splitting\n",
    "import kagglehub # Import kagglehub for dataset download\n",
    "import tensorflow as tf # Import tensorflow\n",
    "from tensorflow.keras.models import Model # Import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, GRU, Input, TimeDistributed, Bidirectional # Import necessary layers\n",
    "import torch # Import PyTorch\n",
    "import torch.nn as nn # Import PyTorch neural network module\n",
    "from music21 import stream, note, duration, tempo # Import music21 elements\n",
    "\n",
    "\n",
    "# Define sampling rate (needed for audio processing and saving)\n",
    "sr = 22050 # Common sampling rate, adjust if your audio files use a different rate\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This needs to be consistent with feature extraction\n",
    "n_chroma = 12 # Number of chroma bins\n",
    "total_audio_features_dim = n_mfcc + n_chroma # Total features for audio conditioning\n",
    "\n",
    "# Define the dimensions for the music generation model\n",
    "musical_feature_dim = n_mfcc # The model output is MFCCs\n",
    "musical_sequence_length = max_padding # The model input sequence length\n",
    "\n",
    "# Define the conditioning input dimensions\n",
    "text_conditioning_dim = 128 # Dimension of the text embedding\n",
    "\n",
    "# --- Helper functions ---\n",
    "\n",
    "# Initialize the text generation pipeline (if not already initialized)\n",
    "lyric_generator = None # Initialize to None\n",
    "try:\n",
    "    lyric_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "    print(\"Text generation pipeline (distilgpt2) initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing text generation pipeline: {e}\")\n",
    "    lyric_generator = None\n",
    "\n",
    "\n",
    "def generate_lyrics_with_language_model(prompt):\n",
    "    \"\"\"Generates lyrics based on a prompt using a language model.\"\"\"\n",
    "    if lyric_generator:\n",
    "        print(f\"Generating lyrics for prompt: '{prompt}' using DistilGPT-2.\")\n",
    "        try:\n",
    "            # Generate text with a reasonable length and without generating too many sequences\n",
    "            generated_text = lyric_generator(prompt, max_length=100, num_return_sequences=1, truncation=True)\n",
    "            lyrics = generated_text[0]['generated_text']\n",
    "            # Basic post-processing: remove the prompt from the generated text if it's repeated\n",
    "            if lyrics.startswith(prompt):\n",
    "                lyrics = lyrics[len(prompt):].strip()\n",
    "            print(\"Lyric generation successful.\")\n",
    "            return lyrics\n",
    "        except Exception as e:\n",
    "            print(f\"Error during lyric generation: {e}\")\n",
    "            return f\"Error generating lyrics for prompt: '{prompt}'\" # Return an error message\n",
    "    else:\n",
    "        print(\"Lyric generator not initialized. Cannot generate lyrics.\")\n",
    "        return f\"Cannot generate lyrics for prompt: '{prompt}' (generator not available)\"\n",
    "\n",
    "\n",
    "# Function to get text embeddings (placeholder)\n",
    "def get_text_embedding(text, embedding_dim):\n",
    "    \"\"\"Placeholder function to get a fixed-size embedding for text.\"\"\"\n",
    "    print(f\"Getting embedding for text: '{text[:50]}...' using a placeholder model.\")\n",
    "    # Return as numpy array for now, will convert to tensor later in process_text_input\n",
    "    return np.random.rand(1, embedding_dim) # Shape (1, embedding_dim)\n",
    "\n",
    "\n",
    "# Function to process text input\n",
    "def process_text_input(user_input, embedding_dim, is_prompt=False):\n",
    "    \"\"\"Processes user text input (lyrics or prompt) and returns a conditioning embedding.\"\"\"\n",
    "    if is_prompt:\n",
    "        lyrics = generate_lyrics_with_language_model(user_input)\n",
    "    else:\n",
    "        lyrics = user_input # User provided lyrics directly\n",
    "\n",
    "    conditioning_embedding_np = get_text_embedding(lyrics, embedding_dim)\n",
    "\n",
    "    # Convert to PyTorch tensor and move to device\n",
    "    conditioning_embedding_tensor = torch.FloatTensor(conditioning_embedding_np).to(device)\n",
    "\n",
    "\n",
    "    return conditioning_embedding_tensor, lyrics # Return as PyTorch tensor\n",
    "\n",
    "\n",
    "# Function to extract combined features (MFCC + Chroma) from an audio file.\n",
    "# This function is used for both training data preparation and user audio conditioning.\n",
    "def extract_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "    \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract Chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "        # Pad or truncate combined features to a fixed length\n",
    "        if combined_features.shape[1] < target_sequence_length:\n",
    "            combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "        else:\n",
    "            combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "        return combined_features.T # Return as numpy array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process user audio for conditioning - calls extract_features\n",
    "def process_user_audio_for_conditioning(audio_file_path, target_sequence_length, n_mfcc, n_chroma):\n",
    "    \"\"\"Processes user audio input and returns features suitable for conditioning.\"\"\"\n",
    "    processed_features_np = extract_features(audio_file_path, target_sequence_length, n_mfcc, n_chroma)\n",
    "\n",
    "    if processed_features_np is not None:\n",
    "        # Convert to PyTorch tensor, add a batch dimension, and move to device\n",
    "        processed_features_tensor = torch.FloatTensor(processed_features_np).unsqueeze(0).to(device) # Shape (1, target_sequence_length, total_feature_dimension)\n",
    "        print(f\"Processed user audio features shape for conditioning: {processed_features_tensor.shape}\")\n",
    "        return processed_features_tensor\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to extract MFCC features only (used for model input/output targets)\n",
    "def extract_mfcc_features_only(file_path, target_sequence_length, n_mfcc=20):\n",
    "    \"\"\"Extract only MFCC features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Pad or truncate MFCCs to a fixed length\n",
    "        if mfccs.shape[1] < target_sequence_length:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, target_sequence_length - mfccs.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :target_sequence_length]\n",
    "\n",
    "        return mfccs.T # Return as numpy array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Model Definition (PyTorch) ---\n",
    "\n",
    "# Define the PyTorch model\n",
    "class MusicGenerator(nn.Module):\n",
    "    def __init__(self, musical_feature_dim, total_audio_features_dim, text_conditioning_dim, hidden_dim=128):\n",
    "        super(MusicGenerator, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.musical_feature_dim = musical_feature_dim\n",
    "        self.total_audio_features_dim = total_audio_features_dim\n",
    "        self.text_conditioning_dim = text_conditioning_dim\n",
    "\n",
    "        # GRU layers\n",
    "        # Input to GRU will be concatenation of musical features, text conditioning, and audio conditioning\n",
    "        # Dimension of combined input: musical_feature_dim + text_conditioning_dim + total_audio_features_dim\n",
    "        self.gru1 = nn.GRU(musical_feature_dim + text_conditioning_dim + total_audio_features_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.gru2 = nn.GRU(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True) # *2 for bidirectional output\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        # Output layer: predicts musical features for each time step\n",
    "        # Since the GRU returns sequences, we use a TimeDistributed-like approach with a linear layer\n",
    "        self.linear = nn.Linear(hidden_dim * 2, musical_feature_dim) # *2 for bidirectional output\n",
    "\n",
    "    def forward(self, musical_input, text_conditioning_input, audio_conditioning_input):\n",
    "        # musical_input shape: (batch_size, sequence_length, musical_feature_dim)\n",
    "        # text_conditioning_input shape: (batch_size, text_conditioning_dim)\n",
    "        # audio_conditioning_input shape: (batch_size, sequence_length, total_audio_features_dim)\n",
    "\n",
    "        # Repeat text conditioning across the sequence length\n",
    "        # text_conditioning_input shape: (batch_size, 1, text_conditioning_dim)\n",
    "        text_conditioning_repeated = text_conditioning_input.unsqueeze(1).repeat(1, musical_input.size(1), 1)\n",
    "        # text_conditioning_repeated shape: (batch_size, sequence_length, text_conditioning_dim)\n",
    "\n",
    "\n",
    "        # Concatenate inputs\n",
    "        combined_input = torch.cat((musical_input, text_conditioning_repeated, audio_conditioning_input), dim=-1)\n",
    "        # combined_input shape: (batch_size, sequence_length, musical_feature_dim + text_conditioning_dim + total_audio_features_dim)\n",
    "\n",
    "        # Pass through GRU layers\n",
    "        gru_out1, _ = self.gru1(combined_input)\n",
    "        dropout_out1 = self.dropout1(gru_out1)\n",
    "\n",
    "        gru_out2, _ = self.gru2(dropout_out1)\n",
    "        dropout_out2 = self.dropout2(gru_out2)\n",
    "\n",
    "        # Apply linear layer to each time step's output\n",
    "        output = self.linear(dropout_out2)\n",
    "        # output shape: (batch_size, sequence_length, musical_feature_dim)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Check if a GPU is available and use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model_gen_conditioned_pytorch = MusicGenerator(musical_feature_dim, total_audio_features_dim, text_conditioning_dim)\n",
    "\n",
    "# Move the model to the selected device\n",
    "model_gen_conditioned_pytorch.to(device)\n",
    "\n",
    "print(\"\\nPyTorch conditional music generation model architecture defined.\")\n",
    "print(model_gen_conditioned_pytorch)\n",
    "\n",
    "\n",
    "# --- Data Preparation ---\n",
    "# Download the dataset and directly assign the path within this cell\n",
    "print(\"Downloading the dataset to ensure path is available...\")\n",
    "try:\n",
    "    downloaded_dataset_root = kagglehub.dataset_download(\"soumendraprasad/musical-instruments-sound-dataset\")\n",
    "    print(f\"Dataset path: {downloaded_dataset_root}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    downloaded_dataset_root = None # Set to None if download fails\n",
    "\n",
    "\n",
    "if downloaded_dataset_root is None or not os.path.exists(downloaded_dataset_root):\n",
    "    print(\"Failed to get the dataset path. Cannot proceed with data preparation.\")\n",
    "else:\n",
    "    # Define feature extraction parameters (should be consistent with model definition)\n",
    "    n_mfcc = 20\n",
    "    max_padding = 174\n",
    "    n_chroma = 12 # Although not used for musical features output, needed for audio conditioning feature extraction\n",
    "    total_audio_features_dim = n_mfcc + n_chroma\n",
    "    text_conditioning_dim = 128\n",
    "\n",
    "\n",
    "    # Collect music instrument files from the downloaded dataset\n",
    "    music_instrument_files = []\n",
    "\n",
    "    # Assuming the structure from the previous file listing\n",
    "    train_submission_path = os.path.join(downloaded_dataset_root, 'Train_submission', 'Train_submission')\n",
    "    test_submission_path = os.path.join(downloaded_dataset_root, 'Test_submission', 'Test_submission') # Assuming Test_submission is also relevant for data\n",
    "\n",
    "    # Collect files from Train_submission\n",
    "    if os.path.exists(train_submission_path):\n",
    "        print(f\"Searching for music instrument files in: {train_submission_path}\")\n",
    "        for root, dirs, files in os.walk(train_submission_path):\n",
    "            for name in files:\n",
    "                if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                     # Optional: further filter by keywords if needed\n",
    "                    if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                         music_instrument_files.append(os.path.join(root, name))\n",
    "        print(f\"Found {len(music_instrument_files)} music instrument files in training submission.\")\n",
    "    else:\n",
    "        print(f\"Warning: Training submission directory not found at {train_submission_path}\")\n",
    "\n",
    "\n",
    "    # Collect files from Test_submission (if it exists and contains relevant audio)\n",
    "    # You might decide to use test data only for evaluation later, but for now, let's include relevant audio\n",
    "    if os.path.exists(test_submission_path):\n",
    "         print(f\"Searching for music instrument files in: {test_submission_path}\")\n",
    "         for root, dirs, files in os.walk(test_submission_path):\n",
    "             for name in files:\n",
    "                 if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                      # Optional: further filter by keywords if needed\n",
    "                     if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                          music_instrument_files.append(os.path.join(root, name))\n",
    "         print(f\"Found {len(music_instrument_files)} music instrument files in test submission.\")\n",
    "    else:\n",
    "         print(f\"Warning: Test submission directory not found at {test_submission_path}\")\n",
    "\n",
    "\n",
    "    # Use the collected music instrument files for feature extraction\n",
    "    files_to_process_music = music_instrument_files\n",
    "\n",
    "    # Prepare lists to store extracted features for training\n",
    "    X_train_music_features = [] # For musical input (MFCCs only)\n",
    "    X_train_audio_conditioning_features = [] # For audio conditioning input (MFCC + Chroma)\n",
    "\n",
    "\n",
    "    print(\"\\nExtracting musical features (MFCCs) and audio conditioning features (MFCC+Chroma) from music instrument files...\")\n",
    "\n",
    "    # Process a subset of files for faster testing, or all files for full training\n",
    "    # process_limit = 100 # Set a limit for testing\n",
    "    # files_to_process_subset = files_to_process_music[:process_limit]\n",
    "    files_to_process_subset = files_to_process_music # Process all files\n",
    "\n",
    "\n",
    "    for i, file_path in enumerate(files_to_process_subset):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processing file {i + 1}/{len(files_to_process_subset)}\")\n",
    "\n",
    "        # Extract MFCC features for the musical input/output\n",
    "        mfccs_only = extract_mfcc_features_only(file_path, max_padding, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract combined features for the audio conditioning input\n",
    "        combined_features = extract_features(file_path, max_padding, n_mfcc=n_mfcc, n_chroma=n_chroma)\n",
    "\n",
    "\n",
    "        if mfccs_only is not None and combined_features is not None:\n",
    "            X_train_music_features.append(mfccs_only)\n",
    "            X_train_audio_conditioning_features.append(combined_features)\n",
    "\n",
    "\n",
    "    # Convert extracted features to numpy arrays (keeping as numpy for TensorFlow training setup initially)\n",
    "    X_train_music_features = np.array(X_train_music_features)\n",
    "    X_train_audio_conditioning_features = np.array(X_train_audio_conditioning_features)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    # Ensure that the split is consistent for both sets of features\n",
    "    if len(X_train_music_features) > 0:\n",
    "        X_train_music, X_val_music, X_train_audio_cond, X_val_audio_cond = train_test_split(\n",
    "            X_train_music_features, X_train_audio_conditioning_features, test_size=0.2, random_state=42\n",
    "        )\n",
    "    else:\n",
    "        print(\"No features were successfully extracted. Cannot split data.\")\n",
    "        # Initialize empty arrays to prevent errors in subsequent steps\n",
    "        X_train_music = np.array([])\n",
    "        X_val_music = np.array([])\n",
    "        X_train_audio_cond = np.array([])\n",
    "        X_val_audio_cond = np.array([])\n",
    "\n",
    "\n",
    "    print(\"\\nFeature extraction and data splitting complete.\")\n",
    "    print(f\"Shape of training musical features (MFCCs): {X_train_music.shape}\")\n",
    "    print(f\"Shape of validation musical features (MFCCs): {X_val_music.shape}\")\n",
    "    print(f\"Shape of training audio conditioning features (MFCC+Chroma): {X_train_audio_cond.shape}\")\n",
    "    print(f\"Shape of validation audio conditioning features (MFCC+Chroma): {X_val_audio_cond.shape}\")\n",
    "\n",
    "    # Create dummy text conditioning data for training\n",
    "    # This data is not based on actual text input yet, just for model training setup\n",
    "    # It should match the expected shape of the text_conditioning_input layer\n",
    "    dummy_text_conditioning_train = np.random.rand(X_train_music.shape[0], text_conditioning_dim)\n",
    "    dummy_text_conditioning_val = np.random.rand(X_val_music.shape[0], text_conditioning_dim)\n",
    "\n",
    "\n",
    "    print(f\"\\nShape of dummy training text conditioning data: {dummy_text_conditioning_train.shape}\")\n",
    "    print(f\"Shape of dummy validation text conditioning data: {dummy_text_conditioning_val.shape}\")\n",
    "\n",
    "\n",
    "    # --- Train the Model (PyTorch) ---\n",
    "    # Check if training data is available before training\n",
    "    if 'X_train_music' in globals() and X_train_music.shape[0] > 0:\n",
    "        # Convert NumPy data to PyTorch tensors and move to device\n",
    "        X_train_music_tensor = torch.FloatTensor(X_train_music).to(device)\n",
    "        X_val_music_tensor = torch.FloatTensor(X_val_music).to(device)\n",
    "        X_train_audio_cond_tensor = torch.FloatTensor(X_train_audio_cond).to(device)\n",
    "        X_val_audio_cond_tensor = torch.FloatTensor(X_val_audio_cond).to(device)\n",
    "        dummy_text_conditioning_train_tensor = torch.FloatTensor(dummy_text_conditioning_train).to(device)\n",
    "        dummy_text_conditioning_val_tensor = torch.FloatTensor(dummy_text_conditioning_val).to(device)\n",
    "\n",
    "        # Create PyTorch DataLoaders\n",
    "        from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_music_tensor, dummy_text_conditioning_train_tensor, X_train_audio_cond_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        val_dataset = TensorDataset(X_val_music_tensor, dummy_text_conditioning_val_tensor, X_val_audio_cond_tensor)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32) # No need to shuffle validation data\n",
    "\n",
    "        print(\"\\nData converted to PyTorch tensors and DataLoaders created for training.\")\n",
    "\n",
    "        # Define Loss Function and Optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer_pytorch = torch.optim.Adam(model_gen_conditioned_pytorch.parameters(), lr=0.001) # You can adjust the learning rate\n",
    "\n",
    "\n",
    "        num_epochs = 50 # You can adjust the number of epochs\n",
    "        print(f\"\\nStarting PyTorch model training for {num_epochs} epochs...\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model_gen_conditioned_pytorch.train() # Set the model to training mode\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for i, (musical_inputs, text_cond_inputs, audio_cond_inputs) in enumerate(train_loader):\n",
    "                # Zero the parameter gradients\n",
    "                optimizer_pytorch.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model_gen_conditioned_pytorch(musical_inputs, text_cond_inputs, audio_cond_inputs)\n",
    "\n",
    "                # Calculate loss\n",
    "                # The target for musical_inputs is the same as the input in this autoencoder-like setup\n",
    "                loss = criterion(outputs, musical_inputs)\n",
    "\n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer_pytorch.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Print statistics\n",
    "                if (i + 1) % 10 == 0: # Print every 10 batches\n",
    "                    print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}] finished. Average Training Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "            # Validation step\n",
    "            model_gen_conditioned_pytorch.eval() # Set the model to evaluation mode\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad(): # Disable gradient calculation for validation\n",
    "                for musical_inputs_val, text_cond_inputs_val, audio_cond_inputs_val in val_loader:\n",
    "                    outputs_val = model_gen_conditioned_pytorch(musical_inputs_val, text_cond_inputs_val, audio_cond_inputs_val)\n",
    "                    loss_val = criterion(outputs_val, musical_inputs_val)\n",
    "                    val_loss += loss_val.item()\n",
    "\n",
    "            print(f'Validation Loss: {val_loss / len(val_loader):.4f}')\n",
    "\n",
    "        print(\"\\nPyTorch model training complete.\")\n",
    "    else:\n",
    "        print(\"Training data is empty. Skipping PyTorch model training.\")\n",
    "\n",
    "\n",
    "# --- Function to generate musical features iteratively (PyTorch) ---\n",
    "def generate_musical_features_iterative_pytorch(model, start_sequence, text_conditioning_input, audio_conditioning_input, total_generation_steps):\n",
    "    \"\"\"Generates new MFCC sequences step-by-step using the trained PyTorch conditional model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained PyTorch conditional generation model.\n",
    "        start_sequence: A PyTorch tensor representing the initial musical sequence\n",
    "                        (shape: (1, sequence_length, musical_feature_dim)).\n",
    "        text_conditioning_input: The text conditioning input (shape: (1, text_conditioning_dim)).\n",
    "        audio_conditioning_input: The audio conditioning input (sequence of features,\n",
    "                                   shape: (1, sequence_length, total_audio_features_dim)).\n",
    "                                   This is the *full* audio conditioning sequence.\n",
    "        total_generation_steps: The total number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated musical MFCC sequence (shape: (total_generation_steps, musical_feature_dim)),\n",
    "        or None if generation fails.\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating musical feature sequence iteratively for {total_generation_steps} steps using PyTorch model...\")\n",
    "\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    generated_sequence = [] # List to store generated time steps\n",
    "    current_musical_sequence = start_sequence.clone().to(device) # Start with the initial sequence on the correct device\n",
    "    sequence_length = start_sequence.shape[1] # Expected input sequence length for the model\n",
    "    musical_feature_dim = start_sequence.shape[2] # Dimension of musical features\n",
    "\n",
    "    # Ensure conditioning inputs are on the correct device\n",
    "    text_conditioning_input = text_conditioning_input.to(device)\n",
    "    audio_conditioning_input = audio_conditioning_input.to(device)\n",
    "\n",
    "\n",
    "    # Ensure audio_conditioning_input has a batch dimension and correct sequence length\n",
    "    if audio_conditioning_input.ndim != 3 or audio_conditioning_input.shape[1] != sequence_length:\n",
    "         print(\"Error: audio_conditioning_input must have shape (1, sequence_length, total_audio_features_dim).\")\n",
    "         return None\n",
    "\n",
    "    # Ensure text_conditioning_input has a batch dimension and correct dimension\n",
    "    if text_conditioning_input.ndim != 2 or text_conditioning_input.shape[1] != text_conditioning_dim:\n",
    "        print(\"Error: text_conditioning_input must have shape (1, text_conditioning_dim).\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation during generation\n",
    "        for i in range(total_generation_steps):\n",
    "            # The model expects [musical_input, text_conditioning_input, audio_conditioning_input]\n",
    "            model_inputs = [current_musical_sequence, text_conditioning_input, audio_conditioning_input]\n",
    "\n",
    "            # Predict the next time step of musical features\n",
    "            try:\n",
    "                # The model predicts a sequence of length `musical_sequence_length`.\n",
    "                # We are interested in the prediction for the *next* time step.\n",
    "                predicted_sequence = model(*model_inputs) # Pass inputs unpacking the list\n",
    "                # predicted_sequence shape: (1, musical_sequence_length, musical_feature_dim)\n",
    "\n",
    "                # Take the last time step of the prediction\n",
    "                predicted_next_step = predicted_sequence[:, -1:, :] # Shape (1, 1, musical_feature_dim)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during model prediction at step {i}: {e}\")\n",
    "                return None\n",
    "\n",
    "\n",
    "            # Append the predicted time step to the generated sequence (as numpy array)\n",
    "            generated_sequence.append(predicted_next_step.squeeze().cpu().numpy()) # Move to CPU and convert to numpy\n",
    "\n",
    "            # Update the current musical sequence by removing the oldest time step and adding the new one\n",
    "            # This sliding window approach is standard for iterative sequence generation.\n",
    "            # Ensure the sequence length remains constant\n",
    "            current_musical_sequence = torch.cat((current_musical_sequence[:, 1:, :], predicted_next_step), dim=1)\n",
    "\n",
    "            # Print progress\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Generated {i + 1}/{total_generation_steps} steps.\")\n",
    "\n",
    "\n",
    "    # Convert the list of generated steps into a numpy array\n",
    "    generated_sequence_np = np.array(generated_sequence) # Shape (total_generation_steps, musical_feature_dim)\n",
    "\n",
    "    print(\"\\nIterative musical feature generation complete.\")\n",
    "    print(f\"Shape of the generated musical features: {generated_sequence_np.shape}\")\n",
    "\n",
    "    # Add inspection of generated features\n",
    "    print(\"\\nGenerated Musical Features Statistics:\")\n",
    "    print(f\"Shape: {generated_sequence_np.shape}\")\n",
    "    print(f\"Mean: {np.mean(generated_sequence_np)}\")\n",
    "    print(f\"Min: {np.min(generated_sequence_np)}\")\n",
    "    print(f\"Max: {np.max(generated_sequence_np)}\")\n",
    "    print(f\"Standard Deviation: {np.std(generated_sequence_np)}\")\n",
    "\n",
    "    # Save generated features to a file for inspection\n",
    "    features_output_filename = 'generated_musical_features_iterative_pytorch.npy'\n",
    "    np.save(features_output_filename, generated_sequence_np)\n",
    "    print(f\"Generated musical features saved to {features_output_filename}\")\n",
    "\n",
    "\n",
    "    return generated_sequence_np\n",
    "\n",
    "\n",
    "# --- Function to convert musical features to audio and save ---\n",
    "def convert_features_to_audio(musical_features, sr=22050, output_filename_wav='generated_music.wav', output_filename_mp3='generated_music.mp3'):\n",
    "    \"\"\"Converts musical features (MFCCs) to an audio file and saves it.\n",
    "\n",
    "    Args:\n",
    "        musical_features: A numpy array containing the generated musical MFCC sequence (shape: (time_steps, n_mfcc)).\n",
    "        sr: Sampling rate for audio output.\n",
    "        output_filename_wav: Filename for the output WAV file.\n",
    "        output_filename_mp3: Filename for the output MP3 file.\n",
    "\n",
    "    Returns:\n",
    "        The path to the generated audio file (MP3 or WAV), or None if conversion/saving fails.\n",
    "    \"\"\"\n",
    "    if musical_features is None or musical_features.shape[0] == 0:\n",
    "        print(\"No musical features provided for audio conversion.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\nConverting generated musical features to audio...\")\n",
    "    try:\n",
    "        # Assuming the features are MFCCs. Transpose back to (n_mfcc, time_steps) for inverse transform\n",
    "        audio_time_series = librosa.feature.inverse.mfcc_to_audio(musical_features.T, sr=sr)\n",
    "\n",
    "        # Add inspection of generated audio time series\n",
    "        print(\"\\nGenerated Audio Time Series Statistics:\")\n",
    "        print(f\"Shape: {audio_time_series.shape}\")\n",
    "        print(f\"Mean: {np.mean(audio_time_series)}\")\n",
    "        print(f\"Min: {np.min(audio_time_series)}\")\n",
    "        print(f\"Max: {np.max(audio_time_series)}\")\n",
    "        print(f\"Standard Deviation: {np.std(audio_time_series)}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MFCC to audio conversion: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Save the generated audio to a WAV file first\n",
    "    print(f\"Saving generated audio to {output_filename_wav}...\")\n",
    "    try:\n",
    "        sf.write(output_filename_wav, audio_time_series, sr)\n",
    "        print(f\"Generated audio saved successfully to {output_filename_wav}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving WAV file: {e}\")\n",
    "        return None\n",
    "\n",
    "    generated_audio_file = output_filename_wav # Default to WAV\n",
    "\n",
    "    # Attempt to convert the WAV file to MP3\n",
    "    print(f\"Attempting to convert {output_filename_wav} to {output_filename_mp3}...\")\n",
    "    try:\n",
    "        audio = AudioSegment.from_wav(output_filename_wav)\n",
    "        audio.export(output_filename_mp3, format=\"mp3\")\n",
    "        print(f\"Conversion to MP3 successful. Output saved to {output_filename_mp3}\")\n",
    "        generated_audio_file = output_filename_mp3 # Update to MP3 if successful\n",
    "        # Optionally, remove the intermediate WAV file\n",
    "        # os.remove(output_filename_wav)\n",
    "        # print(f\"Removed intermediate WAV file: {output_filename_wav}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MP3 conversion: {e}. Proceeding with WAV file.\")\n",
    "        # generated_audio_file remains the WAV file path\n",
    "\n",
    "    return generated_audio_file\n",
    "\n",
    "\n",
    "# --- Function to convert generated features to symbolic music (music21) (Step 4 in Plan) ---\n",
    "def convert_features_to_music21(musical_features_np, sr=22050, hop_length=512):\n",
    "    \"\"\"Converts generated musical features (MFCCs) to a music21 stream.\n",
    "\n",
    "    This is a placeholder function. Converting MFCCs directly to\n",
    "    meaningful symbolic music (notes, chords, rhythm) is a complex task\n",
    "    and requires more advanced techniques (e.g., a separate model trained\n",
    "    to map features to musical events).\n",
    "\n",
    "    Args:\n",
    "        musical_features_np: A numpy array containing the generated musical MFCC sequence (shape: (time_steps, n_mfcc)).\n",
    "        sr: Sampling rate.\n",
    "        hop_length: Hop length used during feature extraction.\n",
    "\n",
    "    Returns:\n",
    "        A music21 stream, or None if conversion is not implemented or fails.\n",
    "    \"\"\"\n",
    "    print(\"\\nAttempting to convert musical features to music21 stream...\")\n",
    "    print(\"Note: Direct conversion of MFCCs to symbolic music is complex and requires further development.\")\n",
    "    # Placeholder implementation: Create a simple stream for demonstration\n",
    "    from music21 import stream, note, duration, tempo, pitch\n",
    "\n",
    "    s = stream.Stream()\n",
    "\n",
    "    # Add a tempo indication (optional, but good practice)\n",
    "    # Estimate tempo from features or use a default\n",
    "    # For simplicity, using a default tempo\n",
    "    s.append(tempo.MetronomeMark(number=120))\n",
    "\n",
    "    # Simple approach: Map a feature value (e.g., the first MFCC coefficient) to pitch\n",
    "    # and use a fixed duration based on the hop length. This is a very basic mapping.\n",
    "    time_per_step = hop_length / sr # Duration of each feature vector in seconds\n",
    "\n",
    "    # Define a mapping range for MFCC values to MIDI pitches (adjust as needed)\n",
    "    # Find min/max of the first MFCC coefficient in the generated sequence\n",
    "    if musical_features_np.shape[0] > 0:\n",
    "        min_mfcc1 = np.min(musical_features_np[:, 0])\n",
    "        max_mfcc1 = np.max(musical_features_np[:, 0])\n",
    "        midi_pitch_range = 60 # Map to a range of 60 MIDI pitches (e.g., from 30 to 90)\n",
    "        min_midi_pitch = 30\n",
    "\n",
    "        for i, features in enumerate(musical_features_np):\n",
    "            # Use the first MFCC coefficient for pitch mapping\n",
    "            mfcc1 = features[0]\n",
    "\n",
    "            # Normalize the MFCC value to the range [0, 1]\n",
    "            if (max_mfcc1 - min_mfcc1) > 0:\n",
    "                normalized_mfcc1 = (mfcc1 - min_mfcc1) / (max_mfcc1 - min_mfcc1)\n",
    "            else:\n",
    "                normalized_mfcc1 = 0.5 # Default to middle if range is zero\n",
    "\n",
    "            # Map the normalized value to a MIDI pitch\n",
    "            midi_pitch_val = int(min_midi_pitch + normalized_mfcc1 * midi_pitch_range)\n",
    "            midi_pitch_val = max(0, min(127, midi_pitch_val)) # Clamp to valid MIDI range\n",
    "\n",
    "            try:\n",
    "                 # Create a note\n",
    "                 n = note.Note(midi_pitch_val)\n",
    "                 # Set the duration based on the time per feature step\n",
    "                 # Using a fixed duration for simplicity, could be refined\n",
    "                 n.duration = duration.Duration(time_per_step)\n",
    "\n",
    "                 s.append(n)\n",
    "            except Exception as note_error:\n",
    "                 print(f\"Error creating music21 note at step {i}: {note_error}\")\n",
    "                 # Optionally, append a rest or skip this step\n",
    "                 s.append(note.Rest(duration=duration.Duration(time_per_step)))\n",
    "\n",
    "\n",
    "            # Avoid creating an excessively large stream for demonstration\n",
    "            if i > 200: # Limit the number of elements for a quicker conversion/display\n",
    "                 break\n",
    "    else:\n",
    "         print(\"No generated features to convert to music21.\")\n",
    "         return None\n",
    "\n",
    "\n",
    "    print(f\"Created a music21 stream with {len(s.elements)} elements.\") # Use .elements to count top-level elements\n",
    "\n",
    "    return s\n",
    "\n",
    "# --- Function to save and display music21 stream (Step 5 in Plan) ---\n",
    "def save_and_display_music21_stream(music21_stream, output_filename_midi='generated_music.mid'):\n",
    "    \"\"\"Saves a music21 stream as a MIDI file and displays its text representation.\"\"\"\n",
    "    if music21_stream is None:\n",
    "        print(\"No music21 stream provided to save or display.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nSaving music21 stream to {output_filename_midi}...\")\n",
    "    try:\n",
    "        music21_stream.write('midi', fp=output_filename_midi)\n",
    "        print(f\"Music21 stream saved successfully to {output_filename_midi}\")\n",
    "\n",
    "        # Display the stream as text\n",
    "        print(\"\\nGenerated Music21 Stream (text representation):\")\n",
    "        # Use .show('text') which is the standard way to get text representation\n",
    "        print(music21_stream.show('text'))\n",
    "\n",
    "\n",
    "        return output_filename_midi # Return the path to the saved MIDI file\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving or displaying music21 stream: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Main music generation function (uses iterative generation with PyTorch model) ---\n",
    "def generate_music_from_input_v4(model, user_text_input=None, user_audio_input_path=None, input_type='text_prompt', target_audio_duration_sec=30, sr=22050, max_padding=174, n_mfcc=20, n_chroma=12, text_conditioning_dim=128):\n",
    "    \"\"\"Generates music based on user input (text and/or audio) using the PyTorch model and iterative generation.\n",
    "\n",
    "    Args:\n",
    "        model: The trained PyTorch conditional music generation model.\n",
    "        user_text_input: The user's text input (string for lyrics or prompt).\n",
    "        user_audio_input_path: Path to the user's audio file.\n",
    "        input_type: Type of text input ('text_lyrics', 'text_prompt'). 'audio_only' is also possible if no text is provided.\n",
    "        target_audio_duration_sec: The desired duration of the generated audio in seconds.\n",
    "        sr: Sampling rate for audio output.\n",
    "        max_padding: Target sequence length for padding/truncating features (used for initial sequence).\n",
    "        n_mfcc: Number of MFCCs extracted.\n",
    "        n_chroma: Number of chroma bins extracted.\n",
    "        text_conditioning_dim: Dimension of the text embedding.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - The path to the generated audio file (MP3 or WAV),\n",
    "        - The generated lyrics (string),\n",
    "        - The generated musical features (numpy array),\n",
    "        or (None, None, None) if an error occurred.\n",
    "    \"\"\"\n",
    "    text_conditioning_input_for_model = None\n",
    "    audio_conditioning_input_for_model = None\n",
    "    generated_lyrics = None\n",
    "\n",
    "    # Process text input if provided\n",
    "    if user_text_input:\n",
    "        is_prompt = (input_type == 'text_prompt')\n",
    "        conditioning_input_for_model, generated_lyrics = process_text_input(user_text_input, text_conditioning_dim, is_prompt=is_prompt)\n",
    "        if conditioning_input_for_model is None:\n",
    "            print(\"Error processing text input for conditioning.\")\n",
    "            return None, None, None\n",
    "        text_conditioning_input_for_model = conditioning_input_for_model\n",
    "\n",
    "    else:\n",
    "        print(\"No text input provided. Using dummy text conditioning.\")\n",
    "        # Create dummy text conditioning input as a PyTorch tensor and move to device\n",
    "        text_conditioning_input_for_model = torch.zeros((1, text_conditioning_dim)).to(device)\n",
    "\n",
    "\n",
    "    # Process audio input if provided\n",
    "    if user_audio_input_path and os.path.exists(user_audio_input_path):\n",
    "        print(\"Processing user audio input for conditioning.\")\n",
    "        # For iterative generation, we still need the full audio conditioning sequence\n",
    "        audio_conditioning_input_for_model = process_user_audio_for_conditioning(user_audio_input_path, max_padding, n_mfcc, n_chroma)\n",
    "        if audio_conditioning_input_for_model is None:\n",
    "            print(\"Error processing user audio input for conditioning.\")\n",
    "            return None, None, None\n",
    "    else:\n",
    "        print(\"No user audio file provided or found. Using dummy audio conditioning.\")\n",
    "        # Create dummy audio conditioning input as a PyTorch tensor and move to device\n",
    "        audio_conditioning_input_for_model = torch.zeros((1, max_padding, total_audio_features_dim)).to(device)\n",
    "\n",
    "\n",
    "    # Ensure both conditioning inputs are available\n",
    "    if text_conditioning_input_for_model is None or audio_conditioning_input_for_model is None:\n",
    "         print(\"Missing conditioning input(s). Cannot proceed.\")\n",
    "         return None, None, None\n",
    "\n",
    "\n",
    "    # Generate a starting musical sequence (e.g., take a random sample from the training data)\n",
    "    # This starting sequence will be the initial input for the iterative generation.\n",
    "    # Ensure X_train_music is available from data loading\n",
    "    if 'X_train_music' not in globals() or X_train_music.shape[0] == 0:\n",
    "         print(\"Training data (X_train_music) is not available or empty. Cannot generate a starting sequence.\")\n",
    "         return None, None, None\n",
    "\n",
    "    # Select a random starting sequence and convert to PyTorch tensor, add batch dim, move to device\n",
    "    start_index = np.random.randint(0, X_train_music.shape[0])\n",
    "    start_sequence_np = X_train_music[start_index:start_index+1, :, :]\n",
    "    start_sequence_tensor = torch.FloatTensor(start_sequence_np).to(device)\n",
    "\n",
    "\n",
    "    # Calculate the total number of time steps to generate\n",
    "    # Each time step in the feature sequence corresponds to hop_length samples (default 512 in librosa mfcc)\n",
    "    # Duration of one feature time step = hop_length / sr\n",
    "    # Total time steps needed = target_audio_duration_sec / (hop_length / sr) = target_audio_duration_sec * sr / hop_length\n",
    "    # Assuming default hop_length of 512\n",
    "    hop_length = 512\n",
    "    total_generation_steps = int(target_audio_duration_sec * sr / hop_length)\n",
    "    print(f\"Targeting {target_audio_duration_sec} seconds, which requires approximately {total_generation_steps} generation steps.\")\n",
    "\n",
    "\n",
    "    # Generate new musical features iteratively using the trained PyTorch conditional model\n",
    "    generated_musical_features_np = generate_musical_features_iterative_pytorch(\n",
    "        model,\n",
    "        start_sequence_tensor,\n",
    "        text_conditioning_input_for_model,\n",
    "        audio_conditioning_input_for_model,\n",
    "        total_generation_steps\n",
    "    )\n",
    "\n",
    "\n",
    "    if generated_musical_features_np is None:\n",
    "        print(\"Feature generation failed.\")\n",
    "        return None, generated_lyrics, None # Return None for audio path and features\n",
    "\n",
    "\n",
    "    # Convert the generated musical features to audio and save\n",
    "    generated_audio_file = convert_features_to_audio(generated_musical_features_np, sr=sr)\n",
    "\n",
    "\n",
    "    # Optionally, save the generated lyrics to a text file if lyrics were generated\n",
    "    lyrics_output_filename = 'generated_lyrics.txt'\n",
    "    if generated_lyrics:\n",
    "        try:\n",
    "            with open(lyrics_output_filename, 'w') as f:\n",
    "                f.write(generated_lyrics)\n",
    "            print(f\"Generated lyrics saved to {lyrics_output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving generated lyrics to file: {e}\")\n",
    "\n",
    "\n",
    "    return generated_audio_file, generated_lyrics, generated_musical_features_np # Return features\n",
    "\n",
    "\n",
    "# --- Example Usage (requires trained PyTorch model and data) ---\n",
    "# Assume model_gen_conditioned_pytorch is trained and X_train_music is defined.\n",
    "\n",
    "# Example: Generate music from a text prompt and user audio, aiming for 30 seconds\n",
    "user_text_prompt = \"Generate a piece of background music.\"\n",
    "# IMPORTANT: Replace '/content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg' with the actual path to your uploaded audio file\n",
    "# Ensure the audio file exists before running\n",
    "user_audio_input_path = '/content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg'\n",
    "if not os.path.exists(user_audio_input_path):\n",
    "    print(f\"Error: User audio file not found at {user_audio_input_path}. Please upload the file.\")\n",
    "    user_audio_input_path = None # Set to None if file doesn't exist\n",
    "\n",
    "target_duration_seconds = 30\n",
    "\n",
    "if user_audio_input_path is not None and 'model_gen_conditioned_pytorch' in globals() and 'X_train_music' in globals() and X_train_music.shape[0] > 0:\n",
    "    print(f\"\\nAttempting to generate music from text prompt: '{user_text_prompt}' using audio file at {user_audio_input_path}, targeting {target_duration_seconds} seconds.\")\n",
    "    generated_music_file, generated_lyrics_text, generated_features_np = generate_music_from_input_v4(\n",
    "        model_gen_conditioned_pytorch, # Use the PyTorch model\n",
    "        user_text_input=user_text_prompt,\n",
    "        user_audio_input_path=user_audio_input_path,\n",
    "        input_type='text_prompt',\n",
    "        target_audio_duration_sec=target_duration_seconds,\n",
    "        sr=sr,\n",
    "        max_padding=max_padding,\n",
    "        n_mfcc=n_mfcc,\n",
    "        n_chroma=n_chroma,\n",
    "        text_conditioning_dim=text_conditioning_dim\n",
    "    )\n",
    "\n",
    "\n",
    "    if generated_music_file:\n",
    "        print(f\"\\nGenerated music file: {generated_music_file}\")\n",
    "        if generated_lyrics_text:\n",
    "            print(f\"Generated lyrics:\\n{generated_lyrics_text}\")\n",
    "        # Display links to the generated files\n",
    "        print(\"\\nDownload the generated music file:\")\n",
    "        display(FileLink(generated_music_file))\n",
    "        if generated_lyrics_text: # Only display lyrics link if lyrics were generated\n",
    "            print(\"Download the generated lyrics file:\")\n",
    "            display(FileLink('generated_lyrics.txt'))\n",
    "\n",
    "        # Play the generated audio\n",
    "        print(f\"\\nPlaying generated audio file: {generated_music_file}\")\n",
    "        display(Audio(generated_music_file))\n",
    "\n",
    "        # Step 4: Convert generated features to symbolic music (music21)\n",
    "        if generated_features_np is not None:\n",
    "            music21_stream = convert_features_to_music21(generated_features_np, sr=sr, hop_length=512) # Pass hop_length\n",
    "\n",
    "            # Step 5: Save and display symbolic music (music21)\n",
    "            if music21_stream:\n",
    "                generated_midi_file = save_and_display_music21_stream(music21_stream)\n",
    "                if generated_midi_file:\n",
    "                    print(\"\\nDownload the generated MIDI file:\")\n",
    "                    display(FileLink(generated_midi_file))\n",
    "        else:\n",
    "            print(\"\\nNo generated features available for music21 conversion.\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"\\nMusic generation failed.\")\n",
    "else:\n",
    "    if user_audio_input_path is None:\n",
    "         print(\"\\nSkipping music generation as user audio file was not found.\")\n",
    "    else:\n",
    "         print(\"\\nSkipping music generation. PyTorch model or training data not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "id": "9f2557e8",
    "outputId": "a721e030-0991-43a9-dd95-22921e1f8072"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to generate music from text prompt: 'Generate a piece of background music.' using audio file at /content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg, targeting 30 seconds with the iterative generation.\n",
      "Generating lyrics for prompt: 'Generate a piece of background music.' using DistilGPT-2.\n",
      "Lyric generation successful.\n",
      "Getting embedding for text: 'You can choose from a list of the tracks you want ...' using a placeholder model.\n",
      "Processing user audio input for conditioning.\n",
      "Processed user audio features shape for conditioning: (1, 174, 32)\n",
      "Targeting 30 seconds, which requires approximately 1291 generation steps.\n",
      "\n",
      "Generating musical feature sequence iteratively for 1291 steps...\n",
      "Generated 100/1291 steps.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2641437299.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nAttempting to generate music from text prompt: '{user_text_prompt}' using audio file at {user_audio_input_path}, targeting {target_duration_seconds} seconds with the iterative generation.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m generated_music_file, generated_lyrics_text = generate_music_from_input_v4(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel_gen_conditioned_v2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0muser_text_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_text_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-620620618.py\u001b[0m in \u001b[0;36mgenerate_music_from_input_v4\u001b[0;34m(model, user_text_input, user_audio_input_path, input_type, target_audio_duration_sec, sr, max_padding, n_mfcc, n_chroma, text_conditioning_dim)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Generate new musical features iteratively using the trained conditional GRU model (v2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     generated_musical_features = generate_musical_features_iterative(\n\u001b[0m\u001b[1;32m    576\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mstart_sequence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-620620618.py\u001b[0m in \u001b[0;36mgenerate_musical_features_iterative\u001b[0;34m(model, start_sequence, text_conditioning_input, audio_conditioning_input, total_generation_steps)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# The model predicts a sequence of length `musical_sequence_length`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;31m# We are interested in the prediction for the *next* time step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mpredicted_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Shape (1, musical_sequence_length, musical_feature_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0;31m# We need to decide which part of the predicted_sequence corresponds to the \"next\" step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py\u001b[0m in \u001b[0;36m_enumerate_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_epoch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_execution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 709\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    746\u001b[0m             self._flat_output_types)\n\u001b[1;32m    747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_set_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfulltype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3476\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3478\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3479\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3480\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now generate music using the updated function (generate_music_from_input_v4)\n",
    "user_audio_input_path = '/content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg'\n",
    "user_text_prompt = \"Generate a piece of background music.\"\n",
    "target_duration_seconds = 30\n",
    "\n",
    "print(f\"\\nAttempting to generate music from text prompt: '{user_text_prompt}' using audio file at {user_audio_input_path}, targeting {target_duration_seconds} seconds with the iterative generation.\")\n",
    "generated_music_file, generated_lyrics_text = generate_music_from_input_v4(\n",
    "    model_gen_conditioned_v2,\n",
    "    user_text_input=user_text_prompt,\n",
    "    user_audio_input_path=user_audio_input_path,\n",
    "    input_type='text_prompt',\n",
    "    target_audio_duration_sec=target_duration_seconds,\n",
    "    sr=sr,\n",
    "    max_padding=max_padding,\n",
    "    n_mfcc=n_mfcc,\n",
    "    n_chroma=n_chroma,\n",
    "    text_conditioning_dim=text_conditioning_dim\n",
    ")\n",
    "\n",
    "if generated_music_file:\n",
    "    print(f\"\\nGenerated music file: {generated_music_file}\")\n",
    "    if generated_lyrics_text:\n",
    "        print(f\"Generated lyrics:\\n{generated_lyrics_text}\")\n",
    "    # Display links to the generated files\n",
    "    print(\"\\nDownload the generated music file:\")\n",
    "    display(FileLink(generated_music_file))\n",
    "    if generated_lyrics_text: # Only display lyrics link if lyrics were generated\n",
    "        print(\"Download the generated lyrics file:\")\n",
    "        display(FileLink('generated_lyrics.txt'))\n",
    "\n",
    "    # Play the generated audio\n",
    "    print(f\"\\nPlaying generated audio file: {generated_music_file}\")\n",
    "    display(Audio(generated_music_file))\n",
    "\n",
    "else:\n",
    "    print(\"\\nMusic generation failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb62f09e",
    "outputId": "027f1396-3581-4c8d-e63c-9bb65d43bb6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: music21 in /usr/local/lib/python3.12/dist-packages (9.3.0)\n",
      "Requirement already satisfied: chardet in /usr/local/lib/python3.12/dist-packages (from music21) (5.2.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from music21) (1.5.2)\n",
      "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.12/dist-packages (from music21) (4.1.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from music21) (3.10.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from music21) (10.8.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from music21) (2.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from music21) (2.32.4)\n",
      "Requirement already satisfied: webcolors>=1.5 in /usr/local/lib/python3.12/dist-packages (from music21) (24.11.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (2.9.0.post0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->music21) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->music21) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->music21) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->music21) (2025.8.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->music21) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdf1f3f6",
    "outputId": "fc219d25-c984-4326-9811-caa8b9b839c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a simple musical stream and saved it as simple_melody.mid\n",
      "\n",
      "Musical Stream (text representation):\n",
      "{0.0} <music21.note.Note C>\n",
      "{1.0} <music21.note.Note D>\n",
      "{2.0} <music21.note.Note E>\n",
      "{3.0} <music21.chord.Chord C4 E4 G4>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from music21 import note, chord, stream, midi\n",
    "\n",
    "# Create individual notes\n",
    "c4 = note.Note('C4')\n",
    "d4 = note.Note('D4')\n",
    "e4 = note.Note('E4')\n",
    "\n",
    "# Create a chord\n",
    "c_major = chord.Chord(['C4', 'E4', 'G4'])\n",
    "\n",
    "# Create a stream (a container for musical elements)\n",
    "my_stream = stream.Stream()\n",
    "\n",
    "# Add elements to the stream\n",
    "my_stream.append(c4)\n",
    "my_stream.append(d4)\n",
    "my_stream.append(e4)\n",
    "my_stream.append(c_major)\n",
    "\n",
    "# You can also add rests, dynamics, etc.\n",
    "# from music21 import duration\n",
    "# eighth_rest = note.Rest(duration.Duration('eighth'))\n",
    "# my_stream.append(eighth_rest)\n",
    "\n",
    "# Display the stream (this will usually open a music notation viewer if available)\n",
    "# my_stream.show()\n",
    "\n",
    "# To save the stream as a MIDI file:\n",
    "midi_output_filename = 'simple_melody.mid'\n",
    "my_stream.write('midi', fp=midi_output_filename)\n",
    "\n",
    "print(f\"Created a simple musical stream and saved it as {midi_output_filename}\")\n",
    "\n",
    "# You can also display the stream as text (useful in environments without a music viewer)\n",
    "print(\"\\nMusical Stream (text representation):\")\n",
    "print(my_stream.show('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56ae8a71",
    "outputId": "f996a3ec-c216-4569-91fc-b2a94c947e1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded MIDI file: simple_melody.mid\n",
      "Number of notes in the loaded stream: 4\n",
      "\n",
      "Loaded Musical Stream (text representation):\n",
      "{0.0} <music21.metadata.Metadata object at 0x7f6e76690500>\n",
      "{0.0} <music21.stream.Part 0x7f6e76693e90>\n",
      "    {0.0} <music21.stream.Measure 1 offset=0.0>\n",
      "        {0.0} <music21.instrument.Instrument ''>\n",
      "        {0.0} <music21.clef.TrebleClef>\n",
      "        {0.0} <music21.tempo.MetronomeMark animato Quarter=120>\n",
      "        {0.0} <music21.meter.TimeSignature 4/4>\n",
      "        {0.0} <music21.note.Note C>\n",
      "        {1.0} <music21.note.Note D>\n",
      "        {2.0} <music21.note.Note E>\n",
      "        {3.0} <music21.chord.Chord C4 E4 G4>\n",
      "        {4.0} <music21.bar.Barline type=final>\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/music21/stream/base.py:3689: Music21DeprecationWarning: .flat is deprecated.  Call .flatten() instead\n",
      "  return self.iter().getElementsByClass(classFilterList)\n"
     ]
    }
   ],
   "source": [
    "from music21 import converter, stream\n",
    "\n",
    "# Define the path to the MIDI file you want to load\n",
    "# You can upload a MIDI file or use one that might be available in your environment\n",
    "midi_file_path = 'simple_melody.mid' # Using the file we just created for demonstration\n",
    "\n",
    "# Load the MIDI file into a music21 stream\n",
    "try:\n",
    "    loaded_stream = converter.parse(midi_file_path)\n",
    "    print(f\"Successfully loaded MIDI file: {midi_file_path}\")\n",
    "\n",
    "    # You can now work with the loaded_stream\n",
    "    # For example, count the number of notes:\n",
    "    note_count = len(loaded_stream.flat.notes)\n",
    "    print(f\"Number of notes in the loaded stream: {note_count}\")\n",
    "\n",
    "    # Display the loaded stream as text\n",
    "    print(\"\\nLoaded Musical Stream (text representation):\")\n",
    "    print(loaded_stream.show('text'))\n",
    "\n",
    "    # You could also iterate through the elements\n",
    "    # for element in loaded_stream.flat:\n",
    "    #     print(element)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading MIDI file {midi_file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "118be072",
    "outputId": "683da5b6-0739-4e21-f0ba-22080ce5aac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35d83faa",
    "outputId": "cd28989e-5485-4058-b30f-da46fdd8c22b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyTorch conditional music generation model architecture defined.\n",
      "MusicGenerator(\n",
      "  (gru1): GRU(180, 128, batch_first=True, bidirectional=True)\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (gru2): GRU(256, 128, batch_first=True, bidirectional=True)\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  (linear): Linear(in_features=256, out_features=20, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the dimensions (should match the feature extraction and conditioning)\n",
    "musical_feature_dim = 20  # n_mfcc\n",
    "total_audio_features_dim = 32 # n_mfcc + n_chroma\n",
    "text_conditioning_dim = 128\n",
    "musical_sequence_length = 174 # max_padding\n",
    "\n",
    "# Define the PyTorch model\n",
    "class MusicGenerator(nn.Module):\n",
    "    def __init__(self, musical_feature_dim, total_audio_features_dim, text_conditioning_dim, hidden_dim=128):\n",
    "        super(MusicGenerator, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.musical_feature_dim = musical_feature_dim\n",
    "        self.total_audio_features_dim = total_audio_features_dim\n",
    "        self.text_conditioning_dim = text_conditioning_dim\n",
    "\n",
    "        # GRU layers\n",
    "        # Input to GRU will be concatenation of musical features, text conditioning, and audio conditioning\n",
    "        # Dimension of combined input: musical_feature_dim + text_conditioning_dim + total_audio_features_dim\n",
    "        self.gru1 = nn.GRU(musical_feature_dim + text_conditioning_dim + total_audio_features_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.gru2 = nn.GRU(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True) # *2 for bidirectional output\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        # Output layer: predicts musical features for each time step\n",
    "        # Since the GRU returns sequences, we use a TimeDistributed-like approach with a linear layer\n",
    "        self.linear = nn.Linear(hidden_dim * 2, musical_feature_dim) # *2 for bidirectional output\n",
    "\n",
    "    def forward(self, musical_input, text_conditioning_input, audio_conditioning_input):\n",
    "        # musical_input shape: (batch_size, sequence_length, musical_feature_dim)\n",
    "        # text_conditioning_input shape: (batch_size, text_conditioning_dim)\n",
    "        # audio_conditioning_input shape: (batch_size, sequence_length, total_audio_features_dim)\n",
    "\n",
    "        # Repeat text conditioning across the sequence length\n",
    "        # text_conditioning_input shape: (batch_size, 1, text_conditioning_dim)\n",
    "        text_conditioning_repeated = text_conditioning_input.unsqueeze(1).repeat(1, musical_input.size(1), 1)\n",
    "        # text_conditioning_repeated shape: (batch_size, sequence_length, text_conditioning_dim)\n",
    "\n",
    "\n",
    "        # Concatenate inputs\n",
    "        combined_input = torch.cat((musical_input, text_conditioning_repeated, audio_conditioning_input), dim=-1)\n",
    "        # combined_input shape: (batch_size, sequence_length, musical_feature_dim + text_conditioning_dim + total_audio_features_dim)\n",
    "\n",
    "        # Pass through GRU layers\n",
    "        gru_out1, _ = self.gru1(combined_input)\n",
    "        dropout_out1 = self.dropout1(gru_out1)\n",
    "\n",
    "        gru_out2, _ = self.gru2(dropout_out1)\n",
    "        dropout_out2 = self.dropout2(gru_out2)\n",
    "\n",
    "        # Apply linear layer to each time step's output\n",
    "        output = self.linear(dropout_out2)\n",
    "        # output shape: (batch_size, sequence_length, musical_feature_dim)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model_gen_conditioned_pytorch = MusicGenerator(musical_feature_dim, total_audio_features_dim, text_conditioning_dim)\n",
    "\n",
    "print(\"\\nPyTorch conditional music generation model architecture defined.\")\n",
    "print(model_gen_conditioned_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab07fd99"
   },
   "source": [
    "### Training the PyTorch Model\n",
    "\n",
    "Now that the PyTorch model is defined and the data loading/feature extraction functions return PyTorch tensors, we can set up the training loop.\n",
    "\n",
    "**1. Prepare Data:**\n",
    "We already have our training and validation data as NumPy arrays (`X_train_music`, `X_val_music`, `X_train_audio_cond`, `X_val_audio_cond`, `dummy_text_conditioning_train`, `dummy_text_conditioning_val`). We need to convert these to PyTorch tensors and create PyTorch DataLoaders for efficient batching during training.\n",
    "\n",
    "**2. Define Loss Function and Optimizer:**\n",
    "For this type of regression task (predicting musical features), a common loss function is Mean Squared Error (MSE). We'll use an optimizer like Adam.\n",
    "\n",
    "**3. Set up the Training Loop:**\n",
    "The training loop will involve iterating over epochs, then over batches of data. For each batch, we'll perform the following steps:\n",
    "- Move data to the appropriate device (CPU or GPU).\n",
    "- Perform a forward pass through the model.\n",
    "- Calculate the loss between the model's predictions and the target musical features (`X_train_music`).\n",
    "- Perform a backward pass to calculate gradients.\n",
    "- Update the model's weights using the optimizer.\n",
    "\n",
    "We'll also include evaluation on the validation data after each epoch to monitor progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50989ea6",
    "outputId": "12572c14-7e25-4d8f-a0d6-b7bdaa640b0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Data converted to PyTorch tensors and DataLoaders created.\n",
      "\n",
      "Starting PyTorch model training for 50 epochs...\n",
      "Epoch [1/50], Step [10/51], Loss: 16723.7168\n",
      "Epoch [1/50], Step [20/51], Loss: 13892.7539\n",
      "Epoch [1/50], Step [30/51], Loss: 15458.0273\n",
      "Epoch [1/50], Step [40/51], Loss: 14097.8838\n",
      "Epoch [1/50], Step [50/51], Loss: 13929.9854\n",
      "Epoch [1/50] finished. Average Training Loss: 13849.4510\n",
      "Validation Loss: 13434.3042\n",
      "Epoch [2/50], Step [10/51], Loss: 12097.5469\n",
      "Epoch [2/50], Step [20/51], Loss: 13407.2607\n",
      "Epoch [2/50], Step [30/51], Loss: 11517.9785\n",
      "Epoch [2/50], Step [40/51], Loss: 13469.3623\n",
      "Epoch [2/50], Step [50/51], Loss: 13351.4346\n",
      "Epoch [2/50] finished. Average Training Loss: 13076.8580\n",
      "Validation Loss: 12724.2708\n",
      "Epoch [3/50], Step [10/51], Loss: 14744.4414\n",
      "Epoch [3/50], Step [20/51], Loss: 12920.9814\n",
      "Epoch [3/50], Step [30/51], Loss: 10972.5322\n",
      "Epoch [3/50], Step [40/51], Loss: 11052.7275\n",
      "Epoch [3/50], Step [50/51], Loss: 12205.0137\n",
      "Epoch [3/50] finished. Average Training Loss: 12386.8383\n",
      "Validation Loss: 12074.5205\n",
      "Epoch [4/50], Step [10/51], Loss: 12134.7617\n",
      "Epoch [4/50], Step [20/51], Loss: 10080.1436\n",
      "Epoch [4/50], Step [30/51], Loss: 11931.9180\n",
      "Epoch [4/50], Step [40/51], Loss: 12073.3887\n",
      "Epoch [4/50], Step [50/51], Loss: 11551.4492\n",
      "Epoch [4/50] finished. Average Training Loss: 11761.9534\n",
      "Validation Loss: 11473.9483\n",
      "Epoch [5/50], Step [10/51], Loss: 13472.1924\n",
      "Epoch [5/50], Step [20/51], Loss: 10546.9082\n",
      "Epoch [5/50], Step [30/51], Loss: 9954.1953\n",
      "Epoch [5/50], Step [40/51], Loss: 11706.7520\n",
      "Epoch [5/50], Step [50/51], Loss: 8819.3330\n",
      "Epoch [5/50] finished. Average Training Loss: 11175.0669\n",
      "Validation Loss: 10910.1011\n",
      "Epoch [6/50], Step [10/51], Loss: 9591.7021\n",
      "Epoch [6/50], Step [20/51], Loss: 7960.6182\n",
      "Epoch [6/50], Step [30/51], Loss: 11167.0498\n",
      "Epoch [6/50], Step [40/51], Loss: 11979.4766\n",
      "Epoch [6/50], Step [50/51], Loss: 10464.8652\n",
      "Epoch [6/50] finished. Average Training Loss: 10628.0018\n",
      "Validation Loss: 10384.3051\n",
      "Epoch [7/50], Step [10/51], Loss: 11640.7617\n",
      "Epoch [7/50], Step [20/51], Loss: 10890.3135\n",
      "Epoch [7/50], Step [30/51], Loss: 9987.5693\n",
      "Epoch [7/50], Step [40/51], Loss: 9221.9648\n",
      "Epoch [7/50], Step [50/51], Loss: 9495.1738\n",
      "Epoch [7/50] finished. Average Training Loss: 10127.8948\n",
      "Validation Loss: 9885.5186\n",
      "Epoch [8/50], Step [10/51], Loss: 11970.3564\n",
      "Epoch [8/50], Step [20/51], Loss: 10065.6377\n",
      "Epoch [8/50], Step [30/51], Loss: 7976.1235\n",
      "Epoch [8/50], Step [40/51], Loss: 8110.9502\n",
      "Epoch [8/50], Step [50/51], Loss: 9179.3838\n",
      "Epoch [8/50] finished. Average Training Loss: 9636.8155\n",
      "Validation Loss: 9413.9475\n",
      "Epoch [9/50], Step [10/51], Loss: 10239.3721\n",
      "Epoch [9/50], Step [20/51], Loss: 9806.1885\n",
      "Epoch [9/50], Step [30/51], Loss: 9421.4199\n",
      "Epoch [9/50], Step [40/51], Loss: 9255.1338\n",
      "Epoch [9/50], Step [50/51], Loss: 9803.1514\n",
      "Epoch [9/50] finished. Average Training Loss: 9170.3416\n",
      "Validation Loss: 8966.5535\n",
      "Epoch [10/50], Step [10/51], Loss: 7867.1748\n",
      "Epoch [10/50], Step [20/51], Loss: 7006.4849\n",
      "Epoch [10/50], Step [30/51], Loss: 9638.1426\n",
      "Epoch [10/50], Step [40/51], Loss: 9508.1680\n",
      "Epoch [10/50], Step [50/51], Loss: 10659.5723\n",
      "Epoch [10/50] finished. Average Training Loss: 8743.7504\n",
      "Validation Loss: 8540.4433\n",
      "Epoch [11/50], Step [10/51], Loss: 7603.1230\n",
      "Epoch [11/50], Step [20/51], Loss: 9232.9473\n",
      "Epoch [11/50], Step [30/51], Loss: 8055.5391\n",
      "Epoch [11/50], Step [40/51], Loss: 8018.8726\n",
      "Epoch [11/50], Step [50/51], Loss: 7210.4585\n",
      "Epoch [11/50] finished. Average Training Loss: 8313.4359\n",
      "Validation Loss: 8134.6165\n",
      "Epoch [12/50], Step [10/51], Loss: 7442.0264\n",
      "Epoch [12/50], Step [20/51], Loss: 5920.8696\n",
      "Epoch [12/50], Step [30/51], Loss: 7024.9863\n",
      "Epoch [12/50], Step [40/51], Loss: 6998.9956\n",
      "Epoch [12/50], Step [50/51], Loss: 8809.9297\n",
      "Epoch [12/50] finished. Average Training Loss: 7922.4221\n",
      "Validation Loss: 7749.6630\n",
      "Epoch [13/50], Step [10/51], Loss: 8497.9951\n",
      "Epoch [13/50], Step [20/51], Loss: 8519.0674\n",
      "Epoch [13/50], Step [30/51], Loss: 6663.1782\n",
      "Epoch [13/50], Step [40/51], Loss: 7665.2080\n",
      "Epoch [13/50], Step [50/51], Loss: 5719.6523\n",
      "Epoch [13/50] finished. Average Training Loss: 7549.5134\n",
      "Validation Loss: 7380.3614\n",
      "Epoch [14/50], Step [10/51], Loss: 7957.9702\n",
      "Epoch [14/50], Step [20/51], Loss: 6985.8604\n",
      "Epoch [14/50], Step [30/51], Loss: 5973.8594\n",
      "Epoch [14/50], Step [40/51], Loss: 6784.3389\n",
      "Epoch [14/50], Step [50/51], Loss: 5656.3960\n",
      "Epoch [14/50] finished. Average Training Loss: 7182.2420\n",
      "Validation Loss: 7029.1813\n",
      "Epoch [15/50], Step [10/51], Loss: 7161.5317\n",
      "Epoch [15/50], Step [20/51], Loss: 6265.1562\n",
      "Epoch [15/50], Step [30/51], Loss: 6171.8018\n",
      "Epoch [15/50], Step [40/51], Loss: 6998.6958\n",
      "Epoch [15/50], Step [50/51], Loss: 6603.9810\n",
      "Epoch [15/50] finished. Average Training Loss: 6839.9946\n",
      "Validation Loss: 6696.7019\n",
      "Epoch [16/50], Step [10/51], Loss: 6055.1763\n",
      "Epoch [16/50], Step [20/51], Loss: 5266.3301\n",
      "Epoch [16/50], Step [30/51], Loss: 6627.1162\n",
      "Epoch [16/50], Step [40/51], Loss: 6889.9761\n",
      "Epoch [16/50], Step [50/51], Loss: 5180.4795\n",
      "Epoch [16/50] finished. Average Training Loss: 6514.5733\n",
      "Validation Loss: 6378.5619\n",
      "Epoch [17/50], Step [10/51], Loss: 6542.7051\n",
      "Epoch [17/50], Step [20/51], Loss: 5959.9668\n",
      "Epoch [17/50], Step [30/51], Loss: 6936.0747\n",
      "Epoch [17/50], Step [40/51], Loss: 5795.8374\n",
      "Epoch [17/50], Step [50/51], Loss: 6327.4585\n",
      "Epoch [17/50] finished. Average Training Loss: 6203.0860\n",
      "Validation Loss: 6077.2310\n",
      "Epoch [18/50], Step [10/51], Loss: 6292.1597\n",
      "Epoch [18/50], Step [20/51], Loss: 5197.8682\n",
      "Epoch [18/50], Step [30/51], Loss: 5070.8550\n",
      "Epoch [18/50], Step [40/51], Loss: 8373.7568\n",
      "Epoch [18/50], Step [50/51], Loss: 6318.9941\n",
      "Epoch [18/50] finished. Average Training Loss: 5899.9943\n",
      "Validation Loss: 5790.2274\n",
      "Epoch [19/50], Step [10/51], Loss: 6151.0420\n",
      "Epoch [19/50], Step [20/51], Loss: 5937.1953\n",
      "Epoch [19/50], Step [30/51], Loss: 5679.4038\n",
      "Epoch [19/50], Step [40/51], Loss: 4526.3071\n",
      "Epoch [19/50], Step [50/51], Loss: 5110.8105\n",
      "Epoch [19/50] finished. Average Training Loss: 5626.3243\n",
      "Validation Loss: 5516.4851\n",
      "Epoch [20/50], Step [10/51], Loss: 5233.2163\n",
      "Epoch [20/50], Step [20/51], Loss: 6432.4868\n",
      "Epoch [20/50], Step [30/51], Loss: 5830.7544\n",
      "Epoch [20/50], Step [40/51], Loss: 5226.3599\n",
      "Epoch [20/50], Step [50/51], Loss: 4125.2720\n",
      "Epoch [20/50] finished. Average Training Loss: 5366.4386\n",
      "Validation Loss: 5257.4712\n",
      "Epoch [21/50], Step [10/51], Loss: 5596.7759\n",
      "Epoch [21/50], Step [20/51], Loss: 5714.0757\n",
      "Epoch [21/50], Step [30/51], Loss: 7169.3311\n",
      "Epoch [21/50], Step [40/51], Loss: 5326.2046\n",
      "Epoch [21/50], Step [50/51], Loss: 5327.1299\n",
      "Epoch [21/50] finished. Average Training Loss: 5108.7921\n",
      "Validation Loss: 5012.2788\n",
      "Epoch [22/50], Step [10/51], Loss: 5161.2749\n",
      "Epoch [22/50], Step [20/51], Loss: 5952.2134\n",
      "Epoch [22/50], Step [30/51], Loss: 3903.4956\n",
      "Epoch [22/50], Step [40/51], Loss: 5186.4180\n",
      "Epoch [22/50], Step [50/51], Loss: 4369.9766\n",
      "Epoch [22/50] finished. Average Training Loss: 4869.8287\n",
      "Validation Loss: 4777.5722\n",
      "Epoch [23/50], Step [10/51], Loss: 5258.2793\n",
      "Epoch [23/50], Step [20/51], Loss: 5817.2812\n",
      "Epoch [23/50], Step [30/51], Loss: 4474.3315\n",
      "Epoch [23/50], Step [40/51], Loss: 4123.0576\n",
      "Epoch [23/50], Step [50/51], Loss: 4109.2949\n",
      "Epoch [23/50] finished. Average Training Loss: 4639.6510\n",
      "Validation Loss: 4557.0994\n",
      "Epoch [24/50], Step [10/51], Loss: 4136.6855\n",
      "Epoch [24/50], Step [20/51], Loss: 4105.9141\n",
      "Epoch [24/50], Step [30/51], Loss: 4285.2607\n",
      "Epoch [24/50], Step [40/51], Loss: 5573.1694\n",
      "Epoch [24/50], Step [50/51], Loss: 5329.8452\n",
      "Epoch [24/50] finished. Average Training Loss: 4412.7163\n",
      "Validation Loss: 4348.9501\n",
      "Epoch [25/50], Step [10/51], Loss: 4467.7715\n",
      "Epoch [25/50], Step [20/51], Loss: 4220.4395\n",
      "Epoch [25/50], Step [30/51], Loss: 3955.2683\n",
      "Epoch [25/50], Step [40/51], Loss: 4208.7959\n",
      "Epoch [25/50], Step [50/51], Loss: 4708.9868\n",
      "Epoch [25/50] finished. Average Training Loss: 4214.4936\n",
      "Validation Loss: 4150.4045\n",
      "Epoch [26/50], Step [10/51], Loss: 4275.2295\n",
      "Epoch [26/50], Step [20/51], Loss: 4955.4810\n",
      "Epoch [26/50], Step [30/51], Loss: 5126.5034\n",
      "Epoch [26/50], Step [40/51], Loss: 4002.9495\n",
      "Epoch [26/50], Step [50/51], Loss: 5097.6714\n",
      "Epoch [26/50] finished. Average Training Loss: 4022.6136\n",
      "Validation Loss: 3961.7731\n",
      "Epoch [27/50], Step [10/51], Loss: 3979.2791\n",
      "Epoch [27/50], Step [20/51], Loss: 3236.4934\n",
      "Epoch [27/50], Step [30/51], Loss: 4857.0688\n",
      "Epoch [27/50], Step [40/51], Loss: 3388.3818\n",
      "Epoch [27/50], Step [50/51], Loss: 3822.1677\n",
      "Epoch [27/50] finished. Average Training Loss: 3839.1175\n",
      "Validation Loss: 3784.5702\n",
      "Epoch [28/50], Step [10/51], Loss: 3177.3022\n",
      "Epoch [28/50], Step [20/51], Loss: 3694.7307\n",
      "Epoch [28/50], Step [30/51], Loss: 3057.7539\n",
      "Epoch [28/50], Step [40/51], Loss: 2803.6792\n",
      "Epoch [28/50], Step [50/51], Loss: 3102.7517\n",
      "Epoch [28/50] finished. Average Training Loss: 3668.2307\n",
      "Validation Loss: 3615.8511\n",
      "Epoch [29/50], Step [10/51], Loss: 3445.1570\n",
      "Epoch [29/50], Step [20/51], Loss: 3354.8716\n",
      "Epoch [29/50], Step [30/51], Loss: 3684.4839\n",
      "Epoch [29/50], Step [40/51], Loss: 3259.7646\n",
      "Epoch [29/50], Step [50/51], Loss: 2733.5083\n",
      "Epoch [29/50] finished. Average Training Loss: 3498.3373\n",
      "Validation Loss: 3456.2516\n",
      "Epoch [30/50], Step [10/51], Loss: 4769.7090\n",
      "Epoch [30/50], Step [20/51], Loss: 3319.1558\n",
      "Epoch [30/50], Step [30/51], Loss: 3375.7258\n",
      "Epoch [30/50], Step [40/51], Loss: 2678.9590\n",
      "Epoch [30/50], Step [50/51], Loss: 3404.9048\n",
      "Epoch [30/50] finished. Average Training Loss: 3342.7953\n",
      "Validation Loss: 3307.1843\n",
      "Epoch [31/50], Step [10/51], Loss: 3762.7815\n",
      "Epoch [31/50], Step [20/51], Loss: 3561.0935\n",
      "Epoch [31/50], Step [30/51], Loss: 2403.6206\n",
      "Epoch [31/50], Step [40/51], Loss: 3214.4387\n",
      "Epoch [31/50], Step [50/51], Loss: 3472.9216\n",
      "Epoch [31/50] finished. Average Training Loss: 3194.5627\n",
      "Validation Loss: 3161.9640\n",
      "Epoch [32/50], Step [10/51], Loss: 4641.6313\n",
      "Epoch [32/50], Step [20/51], Loss: 2891.2178\n",
      "Epoch [32/50], Step [30/51], Loss: 2815.9983\n",
      "Epoch [32/50], Step [40/51], Loss: 2698.5444\n",
      "Epoch [32/50], Step [50/51], Loss: 5175.0605\n",
      "Epoch [32/50] finished. Average Training Loss: 3058.2919\n",
      "Validation Loss: 3026.4684\n",
      "Epoch [33/50], Step [10/51], Loss: 2601.0813\n",
      "Epoch [33/50], Step [20/51], Loss: 2658.6897\n",
      "Epoch [33/50], Step [30/51], Loss: 2435.3933\n",
      "Epoch [33/50], Step [40/51], Loss: 2489.1904\n",
      "Epoch [33/50], Step [50/51], Loss: 4326.2041\n",
      "Epoch [33/50] finished. Average Training Loss: 2922.0078\n",
      "Validation Loss: 2896.7280\n",
      "Epoch [34/50], Step [10/51], Loss: 4347.7988\n",
      "Epoch [34/50], Step [20/51], Loss: 2803.8877\n",
      "Epoch [34/50], Step [30/51], Loss: 2596.2500\n",
      "Epoch [34/50], Step [40/51], Loss: 2528.2341\n",
      "Epoch [34/50], Step [50/51], Loss: 2070.7251\n",
      "Epoch [34/50] finished. Average Training Loss: 2793.7399\n",
      "Validation Loss: 2770.0169\n",
      "Epoch [35/50], Step [10/51], Loss: 2670.9023\n",
      "Epoch [35/50], Step [20/51], Loss: 3372.0864\n",
      "Epoch [35/50], Step [30/51], Loss: 2755.6816\n",
      "Epoch [35/50], Step [40/51], Loss: 1991.2822\n",
      "Epoch [35/50], Step [50/51], Loss: 3101.9250\n",
      "Epoch [35/50] finished. Average Training Loss: 2665.8935\n",
      "Validation Loss: 2653.6927\n",
      "Epoch [36/50], Step [10/51], Loss: 2232.5752\n",
      "Epoch [36/50], Step [20/51], Loss: 2722.2632\n",
      "Epoch [36/50], Step [30/51], Loss: 2809.7771\n",
      "Epoch [36/50], Step [40/51], Loss: 3445.9653\n",
      "Epoch [36/50], Step [50/51], Loss: 2520.6624\n",
      "Epoch [36/50] finished. Average Training Loss: 2554.1451\n",
      "Validation Loss: 2535.2911\n",
      "Epoch [37/50], Step [10/51], Loss: 1773.9983\n",
      "Epoch [37/50], Step [20/51], Loss: 1972.1438\n",
      "Epoch [37/50], Step [30/51], Loss: 2423.7966\n",
      "Epoch [37/50], Step [40/51], Loss: 2442.8159\n",
      "Epoch [37/50], Step [50/51], Loss: 2059.6873\n",
      "Epoch [37/50] finished. Average Training Loss: 2438.2509\n",
      "Validation Loss: 2424.3207\n",
      "Epoch [38/50], Step [10/51], Loss: 2207.3792\n",
      "Epoch [38/50], Step [20/51], Loss: 2276.6763\n",
      "Epoch [38/50], Step [30/51], Loss: 1861.5425\n",
      "Epoch [38/50], Step [40/51], Loss: 2157.3987\n",
      "Epoch [38/50], Step [50/51], Loss: 2273.2341\n",
      "Epoch [38/50] finished. Average Training Loss: 2327.7346\n",
      "Validation Loss: 2316.4592\n",
      "Epoch [39/50], Step [10/51], Loss: 1860.5769\n",
      "Epoch [39/50], Step [20/51], Loss: 2000.7003\n",
      "Epoch [39/50], Step [30/51], Loss: 2499.7737\n",
      "Epoch [39/50], Step [40/51], Loss: 2142.6301\n",
      "Epoch [39/50], Step [50/51], Loss: 2535.9670\n",
      "Epoch [39/50] finished. Average Training Loss: 2224.2640\n",
      "Validation Loss: 2213.5100\n",
      "Epoch [40/50], Step [10/51], Loss: 1915.7734\n",
      "Epoch [40/50], Step [20/51], Loss: 1938.2494\n",
      "Epoch [40/50], Step [30/51], Loss: 1920.6296\n",
      "Epoch [40/50], Step [40/51], Loss: 1938.0325\n",
      "Epoch [40/50], Step [50/51], Loss: 1423.0739\n",
      "Epoch [40/50] finished. Average Training Loss: 2124.9577\n",
      "Validation Loss: 2113.7976\n",
      "Epoch [41/50], Step [10/51], Loss: 1593.3071\n",
      "Epoch [41/50], Step [20/51], Loss: 1685.0212\n",
      "Epoch [41/50], Step [30/51], Loss: 1796.8749\n",
      "Epoch [41/50], Step [40/51], Loss: 2003.5607\n",
      "Epoch [41/50], Step [50/51], Loss: 1871.7815\n",
      "Epoch [41/50] finished. Average Training Loss: 2026.1855\n",
      "Validation Loss: 2017.1531\n",
      "Epoch [42/50], Step [10/51], Loss: 2012.1842\n",
      "Epoch [42/50], Step [20/51], Loss: 2420.0735\n",
      "Epoch [42/50], Step [30/51], Loss: 1636.2673\n",
      "Epoch [42/50], Step [40/51], Loss: 1510.7124\n",
      "Epoch [42/50], Step [50/51], Loss: 1567.6918\n",
      "Epoch [42/50] finished. Average Training Loss: 1936.5338\n",
      "Validation Loss: 1925.6759\n",
      "Epoch [43/50], Step [10/51], Loss: 1700.3687\n",
      "Epoch [43/50], Step [20/51], Loss: 1353.3185\n",
      "Epoch [43/50], Step [30/51], Loss: 1735.7875\n",
      "Epoch [43/50], Step [40/51], Loss: 1541.6693\n",
      "Epoch [43/50], Step [50/51], Loss: 1748.6469\n",
      "Epoch [43/50] finished. Average Training Loss: 1841.3881\n",
      "Validation Loss: 1837.5842\n",
      "Epoch [44/50], Step [10/51], Loss: 2444.3269\n",
      "Epoch [44/50], Step [20/51], Loss: 1287.1125\n",
      "Epoch [44/50], Step [30/51], Loss: 2509.5378\n",
      "Epoch [44/50], Step [40/51], Loss: 1515.9695\n",
      "Epoch [44/50], Step [50/51], Loss: 1489.5482\n",
      "Epoch [44/50] finished. Average Training Loss: 1756.3478\n",
      "Validation Loss: 1752.2213\n",
      "Epoch [45/50], Step [10/51], Loss: 1686.5568\n",
      "Epoch [45/50], Step [20/51], Loss: 2015.4764\n",
      "Epoch [45/50], Step [30/51], Loss: 1974.8833\n",
      "Epoch [45/50], Step [40/51], Loss: 1808.3196\n",
      "Epoch [45/50], Step [50/51], Loss: 1511.0225\n",
      "Epoch [45/50] finished. Average Training Loss: 1678.6436\n",
      "Validation Loss: 1670.6603\n",
      "Epoch [46/50], Step [10/51], Loss: 1356.4524\n",
      "Epoch [46/50], Step [20/51], Loss: 1066.4879\n",
      "Epoch [46/50], Step [30/51], Loss: 1294.7578\n",
      "Epoch [46/50], Step [40/51], Loss: 1562.7522\n",
      "Epoch [46/50], Step [50/51], Loss: 2078.9163\n",
      "Epoch [46/50] finished. Average Training Loss: 1595.8474\n",
      "Validation Loss: 1592.6470\n",
      "Epoch [47/50], Step [10/51], Loss: 1253.0347\n",
      "Epoch [47/50], Step [20/51], Loss: 1256.0576\n",
      "Epoch [47/50], Step [30/51], Loss: 1353.5858\n",
      "Epoch [47/50], Step [40/51], Loss: 2531.8860\n",
      "Epoch [47/50], Step [50/51], Loss: 1902.4880\n",
      "Epoch [47/50] finished. Average Training Loss: 1523.2083\n",
      "Validation Loss: 1519.3715\n",
      "Epoch [48/50], Step [10/51], Loss: 1559.3120\n",
      "Epoch [48/50], Step [20/51], Loss: 1531.3329\n",
      "Epoch [48/50], Step [30/51], Loss: 1032.6588\n",
      "Epoch [48/50], Step [40/51], Loss: 995.4843\n",
      "Epoch [48/50], Step [50/51], Loss: 1336.2518\n",
      "Epoch [48/50] finished. Average Training Loss: 1448.0889\n",
      "Validation Loss: 1447.3333\n",
      "Epoch [49/50], Step [10/51], Loss: 1857.9933\n",
      "Epoch [49/50], Step [20/51], Loss: 1908.7925\n",
      "Epoch [49/50], Step [30/51], Loss: 935.2488\n",
      "Epoch [49/50], Step [40/51], Loss: 1989.3224\n",
      "Epoch [49/50], Step [50/51], Loss: 1001.2059\n",
      "Epoch [49/50] finished. Average Training Loss: 1385.8866\n",
      "Validation Loss: 1380.7927\n",
      "Epoch [50/50], Step [10/51], Loss: 1364.3165\n",
      "Epoch [50/50], Step [20/51], Loss: 904.1301\n",
      "Epoch [50/50], Step [30/51], Loss: 942.9403\n",
      "Epoch [50/50], Step [40/51], Loss: 1121.2625\n",
      "Epoch [50/50], Step [50/51], Loss: 1785.9390\n",
      "Epoch [50/50] finished. Average Training Loss: 1317.6074\n",
      "Validation Loss: 1316.1881\n",
      "\n",
      "PyTorch model training complete.\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available and use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "model_gen_conditioned_pytorch.to(device)\n",
    "\n",
    "# Convert NumPy data to PyTorch tensors and move to device\n",
    "# Ensure the data exists (from the previous data preparation step)\n",
    "if 'X_train_music' in globals() and X_train_music.shape[0] > 0:\n",
    "    X_train_music_tensor = torch.FloatTensor(X_train_music).to(device)\n",
    "    X_val_music_tensor = torch.FloatTensor(X_val_music).to(device)\n",
    "    X_train_audio_cond_tensor = torch.FloatTensor(X_train_audio_cond).to(device)\n",
    "    X_val_audio_cond_tensor = torch.FloatTensor(X_val_audio_cond).to(device)\n",
    "    dummy_text_conditioning_train_tensor = torch.FloatTensor(dummy_text_conditioning_train).to(device)\n",
    "    dummy_text_conditioning_val_tensor = torch.FloatTensor(dummy_text_conditioning_val).to(device)\n",
    "\n",
    "    # Create PyTorch DataLoaders\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_music_tensor, dummy_text_conditioning_train_tensor, X_train_audio_cond_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    val_dataset = TensorDataset(X_val_music_tensor, dummy_text_conditioning_val_tensor, X_val_audio_cond_tensor)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32) # No need to shuffle validation data\n",
    "\n",
    "    print(\"\\nData converted to PyTorch tensors and DataLoaders created.\")\n",
    "\n",
    "else:\n",
    "    print(\"Training data is not available or empty. Skipping PyTorch training setup.\")\n",
    "    train_loader = None\n",
    "    val_loader = None\n",
    "\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_pytorch = torch.optim.Adam(model_gen_conditioned_pytorch.parameters(), lr=0.001) # You can adjust the learning rate\n",
    "\n",
    "# Training loop\n",
    "if train_loader is not None:\n",
    "    num_epochs = 50 # You can adjust the number of epochs\n",
    "    print(f\"\\nStarting PyTorch model training for {num_epochs} epochs...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model_gen_conditioned_pytorch.train() # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (musical_inputs, text_cond_inputs, audio_cond_inputs) in enumerate(train_loader):\n",
    "            # Zero the parameter gradients\n",
    "            optimizer_pytorch.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model_gen_conditioned_pytorch(musical_inputs, text_cond_inputs, audio_cond_inputs)\n",
    "\n",
    "            # Calculate loss\n",
    "            # The target for musical_inputs is the same as the input in this autoencoder-like setup\n",
    "            loss = criterion(outputs, musical_inputs)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer_pytorch.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Print statistics\n",
    "            if (i + 1) % 10 == 0: # Print every 10 batches\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] finished. Average Training Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "        # Validation step\n",
    "        model_gen_conditioned_pytorch.eval() # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad(): # Disable gradient calculation for validation\n",
    "            for musical_inputs_val, text_cond_inputs_val, audio_cond_inputs_val in val_loader:\n",
    "                outputs_val = model_gen_conditioned_pytorch(musical_inputs_val, text_cond_inputs_val, audio_cond_inputs_val)\n",
    "                loss_val = criterion(outputs_val, musical_inputs_val)\n",
    "                val_loss += loss_val.item()\n",
    "\n",
    "        print(f'Validation Loss: {val_loss / len(val_loader):.4f}')\n",
    "\n",
    "    print(\"\\nPyTorch model training complete.\")\n",
    "else:\n",
    "    print(\"\\nSkipping PyTorch model training due to missing data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5f0f3c51"
   },
   "outputs": [],
   "source": [
    "# Save the trained PyTorch model (optional)\n",
    "# torch.save(model_gen_conditioned_pytorch.state_dict(), 'music_generator_pytorch.pth')\n",
    "# print(\"\\nPyTorch model state dict saved to music_generator_pytorch.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "132417d3",
    "outputId": "25c80d79-3d6e-4cc9-a18c-7405d496de21"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 717) (ipython-input-2316692263.py, line 717)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2316692263.py\"\u001b[0;36m, line \u001b[0;32m717\u001b[0m\n\u001b[0;31m    user_text_input: The user's text input (string for lyrics or prompt).\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 717)\n"
     ]
    }
   ],
   "source": [
    "# Regenerate the content of cell 'a1b11bff' to ensure functions and model are defined\n",
    "# This is a safeguard to ensure all necessary code is present before execution\n",
    "# In a real scenario, if the code was already correctly updated,\n",
    "# you would just run the cell. However, regenerating ensures\n",
    "# the latest version with all planned changes is used.\n",
    "# Note: This approach assumes the user wants to re-execute the entire cell,\n",
    "# including data loading, training, and generation.\n",
    "\n",
    "# Get the content of cell 'a1b11bff' (assuming it contains the latest code)\n",
    "# In a real environment, you might need a different method to get cell content by ID.\n",
    "# For this interaction, we'll assume the content is available or regenerate it.\n",
    "\n",
    "# Attempt to get cell content by ID (if the environment supports it)\n",
    "try:\n",
    "    cell_id_to_run = 'a1b11bff'\n",
    "    cell_content_to_run = get_ipython().get_cell(cell_id_to_run).get('text', '')\n",
    "    print(f\"Retrieved content for cell '{cell_id_to_run}'. Executing...\")\n",
    "    get_ipython().run_cell(raw_cell=cell_content_to_run, cell_id=cell_id_to_run)\n",
    "except AttributeError:\n",
    "    print(\"Could not retrieve cell content by ID. Regenerating and executing the cell content directly.\")\n",
    "    # If getting cell content by ID fails, regenerate the code and execute\n",
    "    # This is a fallback to ensure the code runs, but might not reflect\n",
    "    # real-time cell modifications if the user made manual changes not seen by the model.\n",
    "\n",
    "    # The content of cell 'a1b11bff' as regenerated based on the plan\n",
    "    # (This should match the code in the last successful `modify_cells` command for this cell)\n",
    "    # Correcting potential syntax issues in the triple-quoted string by escaping backslashes and ensuring proper formatting\n",
    "    regenerated_cell_content = \"\"\"\n",
    "# Regenerate the content of cell '519a67b2' to ensure functions and model are defined\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "from pydub import AudioSegment # Import AudioSegment for MP3 conversion\n",
    "from transformers import pipeline # Import pipeline for using pre-trained models\n",
    "from IPython.display import display, FileLink, Audio # Import display, FileLink, and Audio\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split for data splitting\n",
    "import kagglehub # Import kagglehub for dataset download\n",
    "import tensorflow as tf # Import tensorflow\n",
    "from tensorflow.keras.models import Model # Import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, GRU, Input, TimeDistributed, Bidirectional # Import necessary layers\n",
    "import torch # Import PyTorch\n",
    "import torch.nn as nn # Import PyTorch neural network module\n",
    "from music21 import stream, note, duration, tempo # Import music21 elements\n",
    "\n",
    "\n",
    "# Define sampling rate (needed for audio processing and saving)\n",
    "sr = 22050 # Common sampling rate, adjust if your audio files use a different rate\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This needs to be consistent with feature extraction\n",
    "n_chroma = 12 # Number of chroma bins\n",
    "total_audio_features_dim = n_mfcc + n_chroma # Total features for audio conditioning\n",
    "\n",
    "# Define the dimensions for the music generation model\n",
    "musical_feature_dim = n_mfcc # The model output is MFCCs\n",
    "musical_sequence_length = max_padding # The model input sequence length\n",
    "\n",
    "# Define the conditioning input dimensions\n",
    "text_conditioning_dim = 128 # Dimension of the text embedding\n",
    "\n",
    "# --- Helper functions ---\n",
    "\n",
    "# Initialize the text generation pipeline (if not already initialized)\n",
    "lyric_generator = None # Initialize to None\n",
    "try:\n",
    "    lyric_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "    print(\"Text generation pipeline (distilgpt2) initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing text generation pipeline: {e}\")\n",
    "    lyric_generator = None\n",
    "\n",
    "\n",
    "def generate_lyrics_with_language_model(prompt):\n",
    "    \\\"\\\"\\\"Generates lyrics based on a prompt using a language model.\\\"\\\"\\\"\n",
    "    if lyric_generator:\n",
    "        print(f\"Generating lyrics for prompt: '{prompt}' using DistilGPT-2.\")\n",
    "        try:\n",
    "            # Generate text with a reasonable length and without generating too many sequences\n",
    "            generated_text = lyric_generator(prompt, max_length=100, num_return_sequences=1, truncation=True)\n",
    "            lyrics = generated_text[0]['generated_text']\n",
    "            # Basic post-processing: remove the prompt from the generated text if it's repeated\n",
    "            if lyrics.startswith(prompt):\n",
    "                lyrics = lyrics[len(prompt):].strip()\n",
    "            print(\"Lyric generation successful.\")\n",
    "            return lyrics\n",
    "        except Exception as e:\n",
    "            print(f\"Error during lyric generation: {e}\")\n",
    "            return f\"Error generating lyrics for prompt: '{prompt}'\" # Return an error message\n",
    "    else:\n",
    "        print(\"Lyric generator not initialized. Cannot generate lyrics.\")\n",
    "        return f\"Cannot generate lyrics for prompt: '{prompt}' (generator not available)\"\n",
    "\n",
    "\n",
    "# Function to get text embeddings (placeholder)\n",
    "def get_text_embedding(text, embedding_dim):\n",
    "    \\\"\\\"\\\"Placeholder function to get a fixed-size embedding for text.\\\"\\\"\\\"\n",
    "    print(f\"Getting embedding for text: '{text[:50]}...' using a placeholder model.\")\n",
    "    # Return as numpy array for now, will convert to tensor later in process_text_input\n",
    "    return np.random.rand(1, embedding_dim) # Shape (1, embedding_dim)\n",
    "\n",
    "\n",
    "# Function to process text input\n",
    "def process_text_input(user_input, embedding_dim, is_prompt=False):\n",
    "    \\\"\\\"\\\"Processes user text input (lyrics or prompt) and returns a conditioning embedding.\\\"\\\"\\\"\n",
    "    if is_prompt:\n",
    "        lyrics = generate_lyrics_with_language_model(user_input)\n",
    "    else:\n",
    "        lyrics = user_input # User provided lyrics directly\n",
    "\n",
    "    conditioning_embedding_np = get_text_embedding(lyrics, embedding_dim)\n",
    "\n",
    "    # Convert to PyTorch tensor and move to device\n",
    "    conditioning_embedding_tensor = torch.FloatTensor(conditioning_embedding_np).to(device)\n",
    "\n",
    "\n",
    "    return conditioning_embedding_tensor, lyrics # Return as PyTorch tensor\n",
    "\n",
    "\n",
    "# Function to extract combined features (MFCC + Chroma) from an audio file.\n",
    "# This function is used for both training data preparation and user audio conditioning.\n",
    "def extract_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "    \\\"\\\"\\\"Extract MFCC and Chroma features from an audio file.\\\"\\\"\\\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract Chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "        # Pad or truncate combined features to a fixed length\n",
    "        if combined_features.shape[1] < target_sequence_length:\n",
    "            combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "        else:\n",
    "            combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "        return combined_features.T # Return as numpy array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process user audio for conditioning - calls extract_features\n",
    "def process_user_audio_for_conditioning(audio_file_path, target_sequence_length, n_mfcc, n_chroma):\n",
    "    \\\"\\\"\\\"Processes user audio input and returns features suitable for conditioning.\\\"\\\"\\\"\n",
    "    processed_features_np = extract_features(audio_file_path, target_sequence_length, n_mfcc, n_chroma)\n",
    "\n",
    "    if processed_features_np is not None:\n",
    "        # Convert to PyTorch tensor, add a batch dimension, and move to device\n",
    "        processed_features_tensor = torch.FloatTensor(processed_features_np).unsqueeze(0).to(device) # Shape (1, target_sequence_length, total_feature_dimension)\n",
    "        print(f\"Processed user audio features shape for conditioning: {processed_features_tensor.shape}\")\n",
    "        return processed_features_tensor\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to extract MFCC features only (used for model input/output targets)\n",
    "def extract_mfcc_features_only(file_path, target_sequence_length, n_mfcc=20):\n",
    "    \\\"\\\"\\\"Extract only MFCC features from an audio file.\\\"\\\"\\\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Pad or truncate MFCCs to a fixed length\n",
    "        if mfccs.shape[1] < target_sequence_length:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, target_sequence_length - mfccs.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :target_sequence_length]\n",
    "\n",
    "        return mfccs.T # Return as numpy array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Model Definition (PyTorch) ---\n",
    "\n",
    "# Define the PyTorch model\n",
    "class MusicGenerator(nn.Module):\n",
    "    def __init__(self, musical_feature_dim, total_audio_features_dim, text_conditioning_dim, hidden_dim=128):\n",
    "        super(MusicGenerator, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.musical_feature_dim = musical_feature_dim\n",
    "        self.total_audio_features_dim = total_audio_features_dim\n",
    "        self.text_conditioning_dim = text_conditioning_dim\n",
    "\n",
    "        # GRU layers\n",
    "        # Input to GRU will be concatenation of musical features, text conditioning, and audio conditioning\n",
    "        # Dimension of combined input: musical_feature_dim + text_conditioning_dim + total_audio_features_dim\n",
    "        self.gru1 = nn.GRU(musical_feature_dim + text_conditioning_dim + total_audio_features_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.gru2 = nn.GRU(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True) # *2 for bidirectional output\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        # Output layer: predicts musical features for each time step\n",
    "        # Since the GRU returns sequences, we use a TimeDistributed-like approach with a linear layer\n",
    "        self.linear = nn.Linear(hidden_dim * 2, musical_feature_dim) # *2 for bidirectional output\n",
    "\n",
    "    def forward(self, musical_input, text_conditioning_input, audio_conditioning_input):\n",
    "        # musical_input shape: (batch_size, sequence_length, musical_feature_dim)\n",
    "        # text_conditioning_input shape: (batch_size, text_conditioning_dim)\n",
    "        # audio_conditioning_input shape: (batch_size, sequence_length, total_audio_features_dim)\n",
    "\n",
    "        # Repeat text conditioning across the sequence length\n",
    "        # text_conditioning_input shape: (batch_size, 1, text_conditioning_dim)\n",
    "        text_conditioning_repeated = text_conditioning_input.unsqueeze(1).repeat(1, musical_input.size(1), 1)\n",
    "        # text_conditioning_repeated shape: (batch_size, sequence_length, text_conditioning_dim)\n",
    "\n",
    "\n",
    "        # Concatenate inputs\n",
    "        combined_input = torch.cat((musical_input, text_conditioning_repeated, audio_conditioning_input), dim=-1)\n",
    "        # combined_input shape: (batch_size, sequence_length, musical_feature_dim + text_conditioning_dim + total_audio_features_dim)\n",
    "\n",
    "        # Pass through GRU layers\n",
    "        gru_out1, _ = self.gru1(combined_input)\n",
    "        dropout_out1 = self.dropout1(gru_out1)\n",
    "\n",
    "        gru_out2, _ = self.gru2(dropout_out1)\n",
    "        dropout_out2 = self.dropout2(gru_out2)\n",
    "\n",
    "        # Apply linear layer to each time step's output\n",
    "        output = self.linear(dropout_out2)\n",
    "        # output shape: (batch_size, sequence_length, musical_feature_dim)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Check if a GPU is available and use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model_gen_conditioned_pytorch = MusicGenerator(musical_feature_dim, total_audio_features_dim, text_conditioning_dim)\n",
    "\n",
    "# Move the model to the selected device\n",
    "model_gen_conditioned_pytorch.to(device)\n",
    "\n",
    "print(\"\\nPyTorch conditional music generation model architecture defined.\")\n",
    "print(model_gen_conditioned_pytorch)\n",
    "\n",
    "\n",
    "# --- Data Preparation ---\n",
    "# Download the dataset and directly assign the path within this cell\n",
    "print(\"Downloading the dataset to ensure path is available...\")\n",
    "try:\n",
    "    downloaded_dataset_root = kagglehub.dataset_download(\"soumendraprasad/musical-instruments-sound-dataset\")\n",
    "    print(f\"Dataset path: {downloaded_dataset_root}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    downloaded_dataset_root = None # Set to None if download fails\n",
    "\n",
    "\n",
    "if downloaded_dataset_root is None or not os.path.exists(downloaded_dataset_root):\n",
    "    print(\"Failed to get the dataset path. Cannot proceed with data preparation.\")\n",
    "else:\n",
    "    # Define feature extraction parameters (should be consistent with model definition)\n",
    "    n_mfcc = 20\n",
    "    max_padding = 174\n",
    "    n_chroma = 12 # Although not used for musical features output, needed for audio conditioning feature extraction\n",
    "    total_audio_features_dim = n_mfcc + n_chroma\n",
    "    text_conditioning_dim = 128\n",
    "\n",
    "\n",
    "    # Collect music instrument files from the downloaded dataset\n",
    "    music_instrument_files = []\n",
    "\n",
    "    # Assuming the structure from the previous file listing\n",
    "    train_submission_path = os.path.join(downloaded_dataset_root, 'Train_submission', 'Train_submission')\n",
    "    test_submission_path = os.path.join(downloaded_dataset_root, 'Test_submission', 'Test_submission') # Assuming Test_submission is also relevant for data\n",
    "\n",
    "    # Collect files from Train_submission\n",
    "    if os.path.exists(train_submission_path):\n",
    "        print(f\"Searching for music instrument files in: {train_submission_path}\")\n",
    "        for root, dirs, files in os.walk(train_submission_path):\n",
    "            for name in files:\n",
    "                if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                     # Optional: further filter by keywords if needed\n",
    "                    if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                         music_instrument_files.append(os.path.join(root, name))\n",
    "        print(f\"Found {len(music_instrument_files)} music instrument files in training submission.\")\n",
    "    else:\n",
    "        print(f\"Warning: Training submission directory not found at {train_submission_path}\")\n",
    "\n",
    "\n",
    "    # Collect files from Test_submission (if it exists and contains relevant audio)\n",
    "    # You might decide to use test data only for evaluation later, but for now, let's include relevant audio\n",
    "    if os.path.exists(test_submission_path):\n",
    "         print(f\"Searching for music instrument files in: {test_submission_path}\")\n",
    "         for root, dirs, files in os.walk(test_submission_path):\n",
    "             for name in files:\n",
    "                 if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                      # Optional: further filter by keywords if needed\n",
    "                     if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                          music_instrument_files.append(os.path.join(root, name))\n",
    "         print(f\"Found {len(music_instrument_files)} music instrument files in test submission.\")\n",
    "    else:\n",
    "         print(f\"Warning: Test submission directory not found at {test_submission_path}\")\n",
    "\n",
    "\n",
    "    # Use the collected music instrument files for feature extraction\n",
    "    files_to_process_music = music_instrument_files\n",
    "\n",
    "    # Prepare lists to store extracted features for training\n",
    "    X_train_music_features = [] # For musical input (MFCCs only)\n",
    "    X_train_audio_conditioning_features = [] # For audio conditioning input (MFCC + Chroma)\n",
    "\n",
    "\n",
    "    print(\"\\nExtracting musical features (MFCCs) and audio conditioning features (MFCC+Chroma) from music instrument files...\")\n",
    "\n",
    "    # Process a subset of files for faster testing, or all files for full training\n",
    "    # process_limit = 100 # Set a limit for testing\n",
    "    # files_to_process_subset = files_to_process_music[:process_limit]\n",
    "    files_to_process_subset = files_to_process_music # Process all files\n",
    "\n",
    "\n",
    "    for i, file_path in enumerate(files_to_process_subset):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processing file {i + 1}/{len(files_to_process_subset)}\")\n",
    "\n",
    "        # Extract MFCC features for the musical input/output\n",
    "        mfccs_only = extract_mfcc_features_only(file_path, max_padding, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract combined features for the audio conditioning input\n",
    "        combined_features = extract_features(file_path, max_padding, n_mfcc=n_mfcc, n_chroma=n_chroma)\n",
    "\n",
    "\n",
    "        if mfccs_only is not None and combined_features is not None:\n",
    "            X_train_music_features.append(mfccs_only)\n",
    "            X_train_audio_conditioning_features.append(combined_features)\n",
    "\n",
    "\n",
    "    # Convert extracted features to numpy arrays (keeping as numpy for TensorFlow training setup initially)\n",
    "    X_train_music_features = np.array(X_train_music_features)\n",
    "    X_train_audio_conditioning_features = np.array(X_train_audio_conditioning_features)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    # Ensure that the split is consistent for both sets of features\n",
    "    if len(X_train_music_features) > 0:\n",
    "        X_train_music, X_val_music, X_train_audio_cond, X_val_audio_cond = train_test_split(\n",
    "            X_train_music_features, X_train_audio_conditioning_features, test_size=0.2, random_state=42\n",
    "        )\n",
    "    else:\n",
    "        print(\"No features were successfully extracted. Cannot split data.\")\n",
    "        # Initialize empty arrays to prevent errors in subsequent steps\n",
    "        X_train_music = np.array([])\n",
    "        X_val_music = np.array([])\n",
    "        X_train_audio_cond = np.array([])\n",
    "        X_val_audio_cond = np.array([])\n",
    "\n",
    "\n",
    "    print(\"\\nFeature extraction and data splitting complete.\")\n",
    "    print(f\"Shape of training musical features (MFCCs): {X_train_music.shape}\")\n",
    "    print(f\"Shape of validation musical features (MFCCs): {X_val_music.shape}\")\n",
    "    print(f\"Shape of training audio conditioning features (MFCC+Chroma): {X_train_audio_cond.shape}\")\n",
    "    print(f\"Shape of validation audio conditioning features (MFCC+Chroma): {X_val_audio_cond.shape}\")\n",
    "\n",
    "    # Create dummy text conditioning data for training\n",
    "    # This data is not based on actual text input yet, just for model training setup\n",
    "    # It should match the expected shape of the text_conditioning_input layer\n",
    "    dummy_text_conditioning_train = np.random.rand(X_train_music.shape[0], text_conditioning_dim)\n",
    "    dummy_text_conditioning_val = np.random.rand(X_val_music.shape[0], text_conditioning_dim)\n",
    "\n",
    "\n",
    "    print(f\"\\nShape of dummy training text conditioning data: {dummy_text_conditioning_train.shape}\")\n",
    "    print(f\"Shape of dummy validation text conditioning data: {dummy_text_conditioning_val.shape}\")\n",
    "\n",
    "\n",
    "    # --- Train the Model (PyTorch) ---\n",
    "    # Check if training data is available before training\n",
    "    if 'X_train_music' in globals() and X_train_music.shape[0] > 0:\n",
    "        # Convert NumPy data to PyTorch tensors and move to device\n",
    "        X_train_music_tensor = torch.FloatTensor(X_train_music).to(device)\n",
    "        X_val_music_tensor = torch.FloatTensor(X_val_music).to(device)\n",
    "        X_train_audio_cond_tensor = torch.FloatTensor(X_train_audio_cond).to(device)\n",
    "        X_val_audio_cond_tensor = torch.FloatTensor(X_val_audio_cond).to(device)\n",
    "        dummy_text_conditioning_train_tensor = torch.FloatTensor(dummy_text_conditioning_train).to(device)\n",
    "        dummy_text_conditioning_val_tensor = torch.FloatTensor(dummy_text_conditioning_val).to(device)\n",
    "\n",
    "        # Create PyTorch DataLoaders\n",
    "        from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_music_tensor, dummy_text_conditioning_train_tensor, X_train_audio_cond_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        val_dataset = TensorDataset(X_val_music_tensor, dummy_text_conditioning_val_tensor, X_val_audio_cond_tensor)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32) # No need to shuffle validation data\n",
    "\n",
    "        print(\"\\nData converted to PyTorch tensors and DataLoaders created for training.\")\n",
    "\n",
    "        # Define Loss Function and Optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer_pytorch = torch.optim.Adam(model_gen_conditioned_pytorch.parameters(), lr=0.001) # You can adjust the learning rate\n",
    "\n",
    "\n",
    "        num_epochs = 50 # You can adjust the number of epochs\n",
    "        print(f\"\\nStarting PyTorch model training for {num_epochs} epochs...\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model_gen_conditioned_pytorch.train() # Set the model to training mode\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for i, (musical_inputs, text_cond_inputs, audio_cond_inputs) in enumerate(train_loader):\n",
    "                # Zero the parameter gradients\n",
    "                optimizer_pytorch.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model_gen_conditioned_pytorch(musical_inputs, text_cond_inputs, audio_cond_inputs)\n",
    "\n",
    "                # Calculate loss\n",
    "                # The target for musical_inputs is the same as the input in this autoencoder-like setup\n",
    "                loss = criterion(outputs, musical_inputs)\n",
    "\n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer_pytorch.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Print statistics\n",
    "                if (i + 1) % 10 == 0: # Print every 10 batches\n",
    "                    print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}] finished. Average Training Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "            # Validation step\n",
    "            model_gen_conditioned_pytorch.eval() # Set the model to evaluation mode\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad(): # Disable gradient calculation for validation\n",
    "                for musical_inputs_val, text_cond_inputs_val, audio_cond_inputs_val in val_loader:\n",
    "                    outputs_val = model_gen_conditioned_pytorch(musical_inputs_val, text_cond_inputs_val, audio_cond_inputs_val)\n",
    "                    loss_val = criterion(outputs_val, musical_inputs_val)\n",
    "                    val_loss += loss_val.item()\n",
    "\n",
    "            print(f'Validation Loss: {val_loss / len(val_loader):.4f}')\n",
    "\n",
    "        print(\"\\nPyTorch model training complete.\")\n",
    "    else:\n",
    "        print(\"Training data is empty. Skipping PyTorch model training.\")\n",
    "\n",
    "\n",
    "# --- Function to generate musical features iteratively (PyTorch) ---\n",
    "def generate_musical_features_iterative_pytorch(model, start_sequence, text_conditioning_input, audio_conditioning_input, total_generation_steps):\n",
    "    \"\"\"Generates new MFCC sequences step-by-step using the trained PyTorch conditional model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained PyTorch conditional generation model.\n",
    "        start_sequence: A PyTorch tensor representing the initial musical sequence\n",
    "                        (shape: (1, sequence_length, musical_feature_dim)).\n",
    "        text_conditioning_input: The text conditioning input (shape: (1, text_conditioning_dim)).\n",
    "        audio_conditioning_input: The audio conditioning input (sequence of features,\n",
    "                                   shape: (1, sequence_length, total_audio_features_dim)).\n",
    "                                   This is the *full* audio conditioning sequence.\n",
    "        total_generation_steps: The total number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated musical MFCC sequence (shape: (total_generation_steps, musical_feature_dim)),\n",
    "        or None if generation fails.\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating musical feature sequence iteratively for {total_generation_steps} steps using PyTorch model...\")\n",
    "\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    generated_sequence = [] # List to store generated time steps\n",
    "    current_musical_sequence = start_sequence.clone().to(device) # Start with the initial sequence on the correct device\n",
    "    sequence_length = start_sequence.shape[1] # Expected input sequence length for the model\n",
    "    musical_feature_dim = start_sequence.shape[2] # Dimension of musical features\n",
    "\n",
    "    # Ensure conditioning inputs are on the correct device\n",
    "    text_conditioning_input = text_conditioning_input.to(device)\n",
    "    audio_conditioning_input = audio_conditioning_input.to(device)\n",
    "\n",
    "\n",
    "    # Ensure audio_conditioning_input has a batch dimension and correct sequence length\n",
    "    if audio_conditioning_input.ndim != 3 or audio_conditioning_input.shape[1] != sequence_length:\n",
    "         print(\"Error: audio_conditioning_input must have shape (1, sequence_length, total_audio_features_dim).\")\n",
    "         return None\n",
    "\n",
    "    # Ensure text_conditioning_input has a batch dimension and correct dimension\n",
    "    if text_conditioning_input.ndim != 2 or text_conditioning_input.shape[1] != text_conditioning_dim:\n",
    "        print(\"Error: text_conditioning_input must have shape (1, text_conditioning_dim).\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation during generation\n",
    "        for i in range(total_generation_steps):\n",
    "            # The model expects [musical_input, text_conditioning_input, audio_conditioning_input]\n",
    "            model_inputs = [current_musical_sequence, text_conditioning_input, audio_conditioning_input]\n",
    "\n",
    "            # Predict the next time step of musical features\n",
    "            try:\n",
    "                # The model predicts a sequence of length `musical_sequence_length`.\n",
    "                # We are interested in the prediction for the *next* time step.\n",
    "                predicted_sequence = model(*model_inputs) # Pass inputs unpacking the list\n",
    "                # predicted_sequence shape: (1, musical_sequence_length, musical_feature_dim)\n",
    "\n",
    "                # Take the last time step of the prediction\n",
    "                predicted_next_step = predicted_sequence[:, -1:, :] # Shape (1, 1, musical_feature_dim)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during model prediction at step {i}: {e}\")\n",
    "                return None\n",
    "\n",
    "\n",
    "            # Append the predicted time step to the generated sequence (as numpy array)\n",
    "            generated_sequence.append(predicted_next_step.squeeze().cpu().numpy()) # Move to CPU and convert to numpy\n",
    "\n",
    "            # Update the current musical sequence by removing the oldest time step and adding the new one\n",
    "            # This sliding window approach is standard for iterative sequence generation.\n",
    "            # Ensure the sequence length remains constant\n",
    "            current_musical_sequence = torch.cat((current_musical_sequence[:, 1:, :], predicted_next_step), dim=1)\n",
    "\n",
    "            # Print progress\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Generated {i + 1}/{total_generation_steps} steps.\")\n",
    "\n",
    "\n",
    "    # Convert the list of generated steps into a numpy array\n",
    "    generated_sequence_np = np.array(generated_sequence) # Shape (total_generation_steps, musical_feature_dim)\n",
    "\n",
    "    print(\"\\nIterative musical feature generation complete.\")\n",
    "    print(f\"Shape of the generated musical features: {generated_sequence_np.shape}\")\n",
    "\n",
    "    # Add inspection of generated features\n",
    "    print(\"\\nGenerated Musical Features Statistics:\")\n",
    "    print(f\"Shape: {generated_sequence_np.shape}\")\n",
    "    print(f\"Mean: {np.mean(generated_sequence_np)}\")\n",
    "    print(f\"Min: {np.min(generated_sequence_np)}\")\n",
    "    print(f\"Max: {np.max(generated_sequence_np)}\")\n",
    "    print(f\"Standard Deviation: {np.std(generated_sequence_np)}\")\n",
    "\n",
    "    # Save generated features to a file for inspection\n",
    "    features_output_filename = 'generated_musical_features_iterative_pytorch.npy'\n",
    "    np.save(features_output_filename, generated_sequence_np)\n",
    "    print(f\"Generated musical features saved to {features_output_filename}\")\n",
    "\n",
    "\n",
    "    return generated_sequence_np\n",
    "\n",
    "\n",
    "# --- Function to convert musical features to audio and save ---\n",
    "def convert_features_to_audio(musical_features, sr=22050, output_filename_wav='generated_music.wav', output_filename_mp3='generated_music.mp3'):\n",
    "    \"\"\"Converts musical features (MFCCs) to an audio file and saves it.\n",
    "\n",
    "    Args:\n",
    "        musical_features: A numpy array containing the generated musical MFCC sequence (shape: (time_steps, n_mfcc)).\n",
    "        sr: Sampling rate for audio output.\n",
    "        output_filename_wav: Filename for the output WAV file.\n",
    "        output_filename_mp3: Filename for the output MP3 file.\n",
    "\n",
    "    Returns:\n",
    "        The path to the generated audio file (MP3 or WAV), or None if conversion/saving fails.\n",
    "    \"\"\"\n",
    "    if musical_features is None or musical_features.shape[0] == 0:\n",
    "        print(\"No musical features provided for audio conversion.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\nConverting generated musical features to audio...\")\n",
    "    try:\n",
    "        # Assuming the features are MFCCs. Transpose back to (n_mfcc, time_steps) for inverse transform\n",
    "        audio_time_series = librosa.feature.inverse.mfcc_to_audio(musical_features.T, sr=sr)\n",
    "\n",
    "        # Add inspection of generated audio time series\n",
    "        print(\"\\nGenerated Audio Time Series Statistics:\")\n",
    "        print(f\"Shape: {audio_time_series.shape}\")\n",
    "        print(f\"Mean: {np.mean(audio_time_series)}\")\n",
    "        print(f\"Min: {np.min(audio_time_series)}\")\n",
    "        print(f\"Max: {np.max(audio_time_series)}\")\n",
    "        print(f\"Standard Deviation: {np.std(audio_time_series)}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MFCC to audio conversion: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Save the generated audio to a WAV file first\n",
    "    print(f\"Saving generated audio to {output_filename_wav}...\")\n",
    "    try:\n",
    "        sf.write(output_filename_wav, audio_time_series, sr)\n",
    "        print(f\"Generated audio saved successfully to {output_filename_wav}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving WAV file: {e}\")\n",
    "        return None\n",
    "\n",
    "    generated_audio_file = output_filename_wav # Default to WAV\n",
    "\n",
    "    # Attempt to convert the WAV file to MP3\n",
    "    print(f\"Attempting to convert {output_filename_wav} to {output_filename_mp3}...\")\n",
    "    try:\n",
    "        audio = AudioSegment.from_wav(output_filename_wav)\n",
    "        audio.export(output_filename_mp3, format=\"mp3\")\n",
    "        print(f\"Conversion to MP3 successful. Output saved to {output_filename_mp3}\")\n",
    "        generated_audio_file = output_filename_mp3 # Update to MP3 if successful\n",
    "        # Optionally, remove the intermediate WAV file\n",
    "        # os.remove(output_filename_wav)\n",
    "        # print(f\"Removed intermediate WAV file: {output_filename_wav}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MP3 conversion: {e}. Proceeding with WAV file.\")\n",
    "        # generated_audio_file remains the WAV file path\n",
    "\n",
    "    return generated_audio_file\n",
    "\n",
    "\n",
    "# --- Function to convert generated features to symbolic music (music21) (Step 4 in Plan) ---\n",
    "def convert_features_to_music21(musical_features_np, sr=22050, hop_length=512):\n",
    "    \"\"\"Converts generated musical features (MFCCs) to a music21 stream.\n",
    "\n",
    "    This is a placeholder function. Converting MFCCs directly to\n",
    "    meaningful symbolic music (notes, chords, rhythm) is a complex task\n",
    "    and requires more advanced techniques (e.g., a separate model trained\n",
    "    to map features to musical events).\n",
    "\n",
    "    Args:\n",
    "        musical_features_np: A numpy array containing the generated musical MFCC sequence (shape: (time_steps, n_mfcc)).\n",
    "        sr: Sampling rate.\n",
    "        hop_length: Hop length used during feature extraction.\n",
    "\n",
    "    Returns:\n",
    "        A music21 stream, or None if conversion is not implemented or fails.\n",
    "    \"\"\"\n",
    "    print(\"\\nAttempting to convert musical features to music21 stream...\")\n",
    "    print(\"Note: Direct conversion of MFCCs to symbolic music is complex and requires further development.\")\n",
    "    # Placeholder implementation: Create a simple stream for demonstration\n",
    "    from music21 import stream, note, duration, tempo, pitch\n",
    "\n",
    "    s = stream.Stream()\n",
    "\n",
    "    # Add a tempo indication (optional, but good practice)\n",
    "    # Estimate tempo from features or use a default\n",
    "    # For simplicity, using a default tempo\n",
    "    s.append(tempo.MetronomeMark(number=120))\n",
    "\n",
    "    # Simple approach: Map a feature value (e.g., the first MFCC coefficient) to pitch\n",
    "    # and use a fixed duration based on the hop length. This is a very basic mapping.\n",
    "    time_per_step = hop_length / sr # Duration of each feature vector in seconds\n",
    "\n",
    "    # Define a mapping range for MFCC values to MIDI pitches (adjust as needed)\n",
    "    # Find min/max of the first MFCC coefficient in the generated sequence\n",
    "    if musical_features_np.shape[0] > 0:\n",
    "        min_mfcc1 = np.min(musical_features_np[:, 0])\n",
    "        max_mfcc1 = np.max(musical_features_np[:, 0])\n",
    "        midi_pitch_range = 60 # Map to a range of 60 MIDI pitches (e.g., from 30 to 90)\n",
    "        min_midi_pitch = 30\n",
    "\n",
    "        for i, features in enumerate(musical_features_np):\n",
    "            # Use the first MFCC coefficient for pitch mapping\n",
    "            mfcc1 = features[0]\n",
    "\n",
    "            # Normalize the MFCC value to the range [0, 1]\n",
    "            if (max_mfcc1 - min_mfcc1) > 0:\n",
    "                normalized_mfcc1 = (mfcc1 - min_mfcc1) / (max_mfcc1 - min_mfcc1)\n",
    "            else:\n",
    "                normalized_mfcc1 = 0.5 # Default to middle if range is zero\n",
    "\n",
    "            # Map the normalized value to a MIDI pitch\n",
    "            midi_pitch_val = int(min_midi_pitch + normalized_mfcc1 * midi_pitch_range)\n",
    "            midi_pitch_val = max(0, min(127, midi_pitch_val)) # Clamp to valid MIDI range\n",
    "\n",
    "            try:\n",
    "                 # Create a note\n",
    "                 n = note.Note(midi_pitch_val)\n",
    "                 # Set the duration based on the time per feature step\n",
    "                 # Using a fixed duration for simplicity, could be refined\n",
    "                 n.duration = duration.Duration(time_per_step)\n",
    "\n",
    "                 s.append(n)\n",
    "            except Exception as note_error:\n",
    "                 print(f\"Error creating music21 note at step {i}: {note_error}\")\n",
    "                 # Optionally, append a rest or skip this step\n",
    "                 s.append(note.Rest(duration=duration.Duration(time_per_step)))\n",
    "\n",
    "\n",
    "            # Avoid creating an excessively large stream for demonstration\n",
    "            if i > 200: # Limit the number of elements for a quicker conversion/display\n",
    "                 break\n",
    "    else:\n",
    "         print(\"No generated features to convert to music21.\")\n",
    "         return None\n",
    "\n",
    "\n",
    "    print(f\"Created a music21 stream with {len(s.elements)} elements.\") # Use .elements to count top-level elements\n",
    "\n",
    "    return s\n",
    "\n",
    "# --- Function to save and display music21 stream (Step 5 in Plan) ---\n",
    "def save_and_display_music21_stream(music21_stream, output_filename_midi='generated_music.mid'):\n",
    "    \"\"\"Saves a music21 stream as a MIDI file and displays its text representation.\"\"\"\n",
    "    if music21_stream is None:\n",
    "        print(\"No music21 stream provided to save or display.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nSaving music21 stream to {output_filename_midi}...\")\n",
    "    try:\n",
    "        music21_stream.write('midi', fp=output_filename_midi)\n",
    "        print(f\"Music21 stream saved successfully to {output_filename_midi}\")\n",
    "\n",
    "        # Display the stream as text\n",
    "        print(\"\\nGenerated Music21 Stream (text representation):\")\n",
    "        print(music21_stream.show('text'))\n",
    "\n",
    "        return output_filename_midi # Return the path to the saved MIDI file\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving or displaying music21 stream: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Main music generation function (uses iterative generation with PyTorch model) ---\n",
    "def generate_music_from_input_v4(model, user_text_input=None, user_audio_input_path=None, input_type='text_prompt', target_audio_duration_sec=30, sr=22050, max_padding=174, n_mfcc=20, n_chroma=12, text_conditioning_dim=128):\n",
    "    \"\"\"Generates music based on user input (text and/or audio) using the PyTorch model and iterative generation.\n",
    "\n",
    "    Args:\n",
    "        model: The trained PyTorch conditional music generation model.\n",
    "        user_text_input: The user's text input (string for lyrics or prompt).\n",
    "        user_audio_input_path: Path to the user's audio file.\n",
    "        input_type: Type of text input ('text_lyrics', 'text_prompt'). 'audio_only' is also possible if no text is provided.\n",
    "        target_audio_duration_sec: The desired duration of the generated audio in seconds.\n",
    "        sr: Sampling rate for audio output.\n",
    "        max_padding: Target sequence length for padding/truncating features (used for initial sequence).\n",
    "        n_mfcc: Number of MFCCs extracted.\n",
    "        n_chroma: Number of chroma bins extracted.\n",
    "        text_conditioning_dim: Dimension of the text embedding.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - The path to the generated audio file (MP3 or WAV),\n",
    "        - The generated lyrics (string),\n",
    "        - The generated musical features (numpy array),\n",
    "        or (None, None, None) if an error occurred.\n",
    "    \"\"\"\n",
    "    text_conditioning_input_for_model = None\n",
    "    audio_conditioning_input_for_model = None\n",
    "    generated_lyrics = None\n",
    "\n",
    "    # Process text input if provided\n",
    "    if user_text_input:\n",
    "        is_prompt = (input_type == 'text_prompt')\n",
    "        conditioning_input_for_model, generated_lyrics = process_text_input(user_text_input, text_conditioning_dim, is_prompt=is_prompt)\n",
    "        if conditioning_input_for_model is None:\n",
    "            print(\"Error processing text input for conditioning.\")\n",
    "            return None, None, None\n",
    "        text_conditioning_input_for_model = conditioning_input_for_model\n",
    "\n",
    "    else:\n",
    "        print(\"No text input provided. Using dummy text conditioning.\")\n",
    "        # Create dummy text conditioning input as a PyTorch tensor and move to device\n",
    "        text_conditioning_input_for_model = torch.zeros((1, text_conditioning_dim)).to(device)\n",
    "\n",
    "\n",
    "    # Process audio input if provided\n",
    "    if user_audio_input_path and os.path.exists(user_audio_input_path):\n",
    "        print(\"Processing user audio input for conditioning.\")\n",
    "        # For iterative generation, we still need the full audio conditioning sequence\n",
    "        audio_conditioning_input_for_model = process_user_audio_for_conditioning(user_audio_input_path, max_padding, n_mfcc, n_chroma)\n",
    "        if audio_conditioning_input_for_model is None:\n",
    "            print(\"Error processing user audio input for conditioning.\")\n",
    "            return None, None, None\n",
    "    else:\n",
    "        print(\"No user audio file provided or found. Using dummy audio conditioning.\")\n",
    "        # Create dummy audio conditioning input as a PyTorch tensor and move to device\n",
    "        audio_conditioning_input_for_model = torch.zeros((1, max_padding, total_audio_features_dim)).to(device)\n",
    "\n",
    "\n",
    "    # Ensure both conditioning inputs are available\n",
    "    if text_conditioning_input_for_model is None or audio_conditioning_input_for_model is None:\n",
    "         print(\"Missing conditioning input(s). Cannot proceed.\")\n",
    "         return None, None, None\n",
    "\n",
    "\n",
    "    # Generate a starting musical sequence (e.g., take a random sample from the training data)\n",
    "    # This starting sequence will be the initial input for the iterative generation.\n",
    "    # Ensure X_train_music is available from data loading\n",
    "    if 'X_train_music' not in globals() or X_train_music.shape[0] == 0:\n",
    "         print(\"Training data (X_train_music) is not available or empty. Cannot generate a starting sequence.\")\n",
    "         return None, None, None\n",
    "\n",
    "    # Select a random starting sequence and convert to PyTorch tensor, add batch dim, move to device\n",
    "    start_index = np.random.randint(0, X_train_music.shape[0])\n",
    "    start_sequence_np = X_train_music[start_index:start_index+1, :, :]\n",
    "    start_sequence_tensor = torch.FloatTensor(start_sequence_np).to(device)\n",
    "\n",
    "\n",
    "    # Calculate the total number of time steps to generate\n",
    "    # Each time step in the feature sequence corresponds to hop_length samples (default 512 in librosa mfcc)\n",
    "    # Duration of one feature time step = hop_length / sr\n",
    "    # Total time steps needed = target_audio_duration_sec / (hop_length / sr) = target_audio_duration_sec * sr / hop_length\n",
    "    # Assuming default hop_length of 512\n",
    "    hop_length = 512\n",
    "    total_generation_steps = int(target_audio_duration_sec * sr / hop_length)\n",
    "    print(f\"Targeting {target_audio_duration_sec} seconds, which requires approximately {total_generation_steps} generation steps.\")\n",
    "\n",
    "\n",
    "    # Generate new musical features iteratively using the trained PyTorch conditional model\n",
    "    generated_musical_features_np = generate_musical_features_iterative_pytorch(\n",
    "        model,\n",
    "        start_sequence_tensor,\n",
    "        text_conditioning_input_for_model,\n",
    "        audio_conditioning_input_for_model,\n",
    "        total_generation_steps\n",
    "    )\n",
    "\n",
    "\n",
    "    if generated_musical_features_np is None:\n",
    "        print(\"Feature generation failed.\")\n",
    "        return None, generated_lyrics, None # Return None for audio path and features\n",
    "\n",
    "\n",
    "    # Convert the generated musical features to audio and save\n",
    "    generated_audio_file = convert_features_to_audio(generated_musical_features_np, sr=sr)\n",
    "\n",
    "\n",
    "    # Optionally, save the generated lyrics to a text file if lyrics were generated\n",
    "    lyrics_output_filename = 'generated_lyrics.txt'\n",
    "    if generated_lyrics:\n",
    "        try:\n",
    "            with open(lyrics_output_filename, 'w') as f:\n",
    "                f.write(generated_lyrics)\n",
    "            print(f\"Generated lyrics saved to {lyrics_output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving generated lyrics to file: {e}\")\n",
    "\n",
    "\n",
    "    return generated_audio_file, generated_lyrics, generated_musical_features_np # Return features\n",
    "\n",
    "\n",
    "# --- Example Usage (requires trained PyTorch model and data) ---\n",
    "# Assume model_gen_conditioned_pytorch is trained and X_train_music is defined.\n",
    "\n",
    "# Example: Generate music from a text prompt and user audio, aiming for 30 seconds\n",
    "user_text_prompt = \"Generate a piece of background music.\"\n",
    "# IMPORTANT: Replace '/content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg' with the actual path to your uploaded audio file\n",
    "# Ensure the audio file exists before running\n",
    "user_audio_input_path = '/content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg'\n",
    "if not os.path.exists(user_audio_input_path):\n",
    "    print(f\"Error: User audio file not found at {user_audio_input_path}. Please upload the file.\")\n",
    "    user_audio_input_path = None # Set to None if file doesn't exist\n",
    "\n",
    "target_duration_seconds = 30\n",
    "\n",
    "if user_audio_input_path is not None and 'model_gen_conditioned_pytorch' in globals() and 'X_train_music' in globals() and X_train_music.shape[0] > 0:\n",
    "    print(f\"\\nAttempting to generate music from text prompt: '{user_text_prompt}' using audio file at {user_audio_input_path}, targeting {target_duration_seconds} seconds.\")\n",
    "    generated_music_file, generated_lyrics_text, generated_features_np = generate_music_from_input_v4(\n",
    "        model_gen_conditioned_pytorch, # Use the PyTorch model\n",
    "        user_text_input=user_text_prompt,\n",
    "        user_audio_input_path=user_audio_input_path,\n",
    "        input_type='text_prompt',\n",
    "        target_audio_duration_sec=target_duration_seconds,\n",
    "        sr=sr,\n",
    "        max_padding=max_padding,\n",
    "        n_mfcc=n_mfcc,\n",
    "        n_chroma=n_chroma,\n",
    "        text_conditioning_dim=text_conditioning_dim\n",
    "    )\n",
    "\n",
    "\n",
    "    if generated_music_file:\n",
    "        print(f\"\\nGenerated music file: {generated_music_file}\")\n",
    "        if generated_lyrics_text:\n",
    "            print(f\"Generated lyrics:\\n{generated_lyrics_text}\")\n",
    "        # Display links to the generated files\n",
    "        print(\"\\nDownload the generated music file:\")\n",
    "        display(FileLink(generated_music_file))\n",
    "        if generated_lyrics_text: # Only display lyrics link if lyrics were generated\n",
    "            print(\"Download the generated lyrics file:\")\n",
    "            display(FileLink('generated_lyrics.txt'))\n",
    "\n",
    "        # Play the generated audio\n",
    "        print(f\"\\nPlaying generated audio file: {generated_music_file}\")\n",
    "        display(Audio(generated_music_file))\n",
    "\n",
    "        # Step 4: Convert generated features to symbolic music (music21)\n",
    "        if generated_features_np is not None:\n",
    "            music21_stream = convert_features_to_music21(generated_features_np, sr=sr, hop_length=512) # Pass hop_length\n",
    "\n",
    "            # Step 5: Save and display symbolic music (music21)\n",
    "            if music21_stream:\n",
    "                generated_midi_file = save_and_display_music21_stream(music21_stream)\n",
    "                if generated_midi_file:\n",
    "                    print(\"\\nDownload the generated MIDI file:\")\n",
    "                    display(FileLink(generated_midi_file))\n",
    "        else:\n",
    "            print(\"\\nNo generated features available for music21 conversion.\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"\\nMusic generation failed.\")\n",
    "else:\n",
    "    if user_audio_input_path is None:\n",
    "         print(\"\\nSkipping music generation as user audio file was not found.\")\n",
    "    else:\n",
    "         print(\"\\nSkipping music generation. PyTorch model or training data not available.\")\n",
    "\"\"\"\n",
    "    get_ipython().run_cell(raw_cell=regenerated_cell_content, cell_id='a1b11bff')\n",
    "\n",
    "print(\"\\nProceeding with music generation using the updated function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "f3dca6a4",
    "outputId": "b4e84c71-9d9b-40b3-f2c0-2cf0681ef097"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 717) (ipython-input-3123280782.py, line 717)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3123280782.py\"\u001b[0;36m, line \u001b[0;32m717\u001b[0m\n\u001b[0;31m    user_text_input: The user's text input (string for lyrics or prompt).\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 717)\n"
     ]
    }
   ],
   "source": [
    "# Regenerate the content of cell 'a1b11bff' to ensure functions and model are defined\n",
    "# This is a safeguard to ensure all necessary code is present before execution\n",
    "# In a real scenario, if the code was already correctly updated,\n",
    "# you would just run the cell. However, regenerating ensures\n",
    "# the latest version with all planned changes is used.\n",
    "# Note: This approach assumes the user wants to re-execute the entire cell,\n",
    "# including data loading, training, and generation.\n",
    "\n",
    "# Get the content of cell 'a1b11bff' (assuming it contains the latest code)\n",
    "# In a real environment, you might need a different method to get cell content by ID.\n",
    "# For this interaction, we'll assume the content is available or regenerate it.\n",
    "\n",
    "# Attempt to get cell content by ID (if the environment supports it)\n",
    "try:\n",
    "    cell_id_to_run = 'a1b11bff'\n",
    "    cell_content_to_run = get_ipython().get_cell(cell_id_to_run).get('text', '')\n",
    "    print(f\"Retrieved content for cell '{cell_id_to_run}'. Executing...\")\n",
    "    get_ipython().run_cell(raw_cell=cell_content_to_run, cell_id=cell_id_to_run)\n",
    "except AttributeError:\n",
    "    print(\"Could not retrieve cell content by ID. Regenerating and executing the cell content directly.\")\n",
    "    # If getting cell content by ID fails, regenerate the code and execute\n",
    "    # This is a fallback to ensure the code runs, but might not reflect\n",
    "    # real-time cell modifications if the user made manual changes not seen by the model.\n",
    "\n",
    "    # The content of cell 'a1b11bff' as regenerated based on the plan\n",
    "    # (This should match the code in the last successful `modify_cells` command for this cell)\n",
    "    # Correcting potential syntax issues in the triple-quoted string by carefully escaping special characters and ensuring proper formatting\n",
    "    regenerated_cell_content = \"\"\"\n",
    "# Regenerate the content of cell '519a67b2' to ensure functions and model are defined\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "from pydub import AudioSegment # Import AudioSegment for MP3 conversion\n",
    "from transformers import pipeline # Import pipeline for using pre-trained models\n",
    "from IPython.display import display, FileLink, Audio # Import display, FileLink, and Audio\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split for data splitting\n",
    "import kagglehub # Import kagglehub for dataset download\n",
    "import tensorflow as tf # Import tensorflow\n",
    "from tensorflow.keras.models import Model # Import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, GRU, Input, TimeDistributed, Bidirectional # Import necessary layers\n",
    "import torch # Import PyTorch\n",
    "import torch.nn as nn # Import PyTorch neural network module\n",
    "from music21 import stream, note, duration, tempo # Import music21 elements\n",
    "\n",
    "\n",
    "# Define sampling rate (needed for audio processing and saving)\n",
    "sr = 22050 # Common sampling rate, adjust if your audio files use a different rate\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This needs to be consistent with feature extraction\n",
    "n_chroma = 12 # Number of chroma bins\n",
    "total_audio_features_dim = n_mfcc + n_chroma # Total features for audio conditioning\n",
    "\n",
    "# Define the dimensions for the music generation model\n",
    "musical_feature_dim = n_mfcc # The model output is MFCCs\n",
    "musical_sequence_length = max_padding # The model input sequence length\n",
    "\n",
    "# Define the conditioning input dimensions\n",
    "text_conditioning_dim = 128 # Dimension of the text embedding\n",
    "\n",
    "# --- Helper functions ---\n",
    "\n",
    "# Initialize the text generation pipeline (if not already initialized)\n",
    "lyric_generator = None # Initialize to None\n",
    "try:\n",
    "    lyric_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "    print(\"Text generation pipeline (distilgpt2) initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing text generation pipeline: {e}\")\n",
    "    lyric_generator = None\n",
    "\n",
    "\n",
    "def generate_lyrics_with_language_model(prompt):\n",
    "    \\\"\\\"\\\"Generates lyrics based on a prompt using a language model.\\\"\\\"\\\"\n",
    "    if lyric_generator:\n",
    "        print(f\"Generating lyrics for prompt: '{prompt}' using DistilGPT-2.\")\n",
    "        try:\n",
    "            # Generate text with a reasonable length and without generating too many sequences\n",
    "            generated_text = lyric_generator(prompt, max_length=100, num_return_sequences=1, truncation=True)\n",
    "            lyrics = generated_text[0]['generated_text']\n",
    "            # Basic post-processing: remove the prompt from the generated text if it's repeated\n",
    "            if lyrics.startswith(prompt):\n",
    "                lyrics = lyrics[len(prompt):].strip()\n",
    "            print(\"Lyric generation successful.\")\n",
    "            return lyrics\n",
    "        except Exception as e:\n",
    "            print(f\"Error during lyric generation: {e}\")\n",
    "            return f\"Error generating lyrics for prompt: '{prompt}'\" # Return an error message\n",
    "    else:\n",
    "        print(\"Lyric generator not initialized. Cannot generate lyrics.\")\n",
    "        return f\"Cannot generate lyrics for prompt: '{prompt}' (generator not available)\"\n",
    "\n",
    "\n",
    "# Function to get text embeddings (placeholder)\n",
    "def get_text_embedding(text, embedding_dim):\n",
    "    \\\"\\\"\\\"Placeholder function to get a fixed-size embedding for text.\\\"\\\"\\\"\n",
    "    print(f\"Getting embedding for text: '{text[:50]}...' using a placeholder model.\")\n",
    "    # Return as numpy array for now, will convert to tensor later in process_text_input\n",
    "    return np.random.rand(1, embedding_dim) # Shape (1, embedding_dim)\n",
    "\n",
    "\n",
    "# Function to process text input\n",
    "def process_text_input(user_input, embedding_dim, is_prompt=False):\n",
    "    \\\"\\\"\\\"Processes user text input (lyrics or prompt) and returns a conditioning embedding.\\\"\\\"\\\"\n",
    "    if is_prompt:\n",
    "        lyrics = generate_lyrics_with_language_model(user_input)\n",
    "    else:\n",
    "        lyrics = user_input # User provided lyrics directly\n",
    "\n",
    "    conditioning_embedding_np = get_text_embedding(lyrics, embedding_dim)\n",
    "\n",
    "    # Convert to PyTorch tensor and move to device\n",
    "    conditioning_embedding_tensor = torch.FloatTensor(conditioning_embedding_np).to(device)\n",
    "\n",
    "\n",
    "    return conditioning_embedding_tensor, lyrics # Return as PyTorch tensor\n",
    "\n",
    "\n",
    "# Function to extract combined features (MFCC + Chroma) from an audio file.\n",
    "# This function is used for both training data preparation and user audio conditioning.\n",
    "def extract_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "    \\\"\\\"\\\"Extract MFCC and Chroma features from an audio file.\\\"\\\"\\\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract Chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "        # Pad or truncate combined features to a fixed length\n",
    "        if combined_features.shape[1] < target_sequence_length:\n",
    "            combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "        else:\n",
    "            combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "        return combined_features.T # Return as numpy array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process user audio for conditioning - calls extract_features\n",
    "def process_user_audio_for_conditioning(audio_file_path, target_sequence_length, n_mfcc, n_chroma):\n",
    "    \\\"\\\"\\\"Processes user audio input and returns features suitable for conditioning.\\\"\\\"\\\"\n",
    "    processed_features_np = extract_features(audio_file_path, target_sequence_length, n_mfcc, n_chroma)\n",
    "\n",
    "    if processed_features_np is not None:\n",
    "        # Convert to PyTorch tensor, add a batch dimension, and move to device\n",
    "        processed_features_tensor = torch.FloatTensor(processed_features_np).unsqueeze(0).to(device) # Shape (1, target_sequence_length, total_feature_dimension)\n",
    "        print(f\"Processed user audio features shape for conditioning: {processed_features_tensor.shape}\")\n",
    "        return processed_features_tensor\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to extract MFCC features only (used for model input/output targets)\n",
    "def extract_mfcc_features_only(file_path, target_sequence_length, n_mfcc=20):\n",
    "    \\\"\\\"\\\"Extract only MFCC features from an audio file.\\\"\\\"\\\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Pad or truncate MFCCs to a fixed length\n",
    "        if mfccs.shape[1] < target_sequence_length:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, target_sequence_length - mfccs.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :target_sequence_length]\n",
    "\n",
    "        return mfccs.T # Return as numpy array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Model Definition (PyTorch) ---\n",
    "\n",
    "# Define the PyTorch model\n",
    "class MusicGenerator(nn.Module):\n",
    "    def __init__(self, musical_feature_dim, total_audio_features_dim, text_conditioning_dim, hidden_dim=128):\n",
    "        super(MusicGenerator, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.musical_feature_dim = musical_feature_dim\n",
    "        self.total_audio_features_dim = total_audio_features_dim\n",
    "        self.text_conditioning_dim = text_conditioning_dim\n",
    "\n",
    "        # GRU layers\n",
    "        # Input to GRU will be concatenation of musical features, text conditioning, and audio conditioning\n",
    "        # Dimension of combined input: musical_feature_dim + text_conditioning_dim + total_audio_features_dim\n",
    "        self.gru1 = nn.GRU(musical_feature_dim + text_conditioning_dim + total_audio_features_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.gru2 = nn.GRU(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True) # *2 for bidirectional output\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        # Output layer: predicts musical features for each time step\n",
    "        # Since the GRU returns sequences, we use a TimeDistributed-like approach with a linear layer\n",
    "        self.linear = nn.Linear(hidden_dim * 2, musical_feature_dim) # *2 for bidirectional output\n",
    "\n",
    "    def forward(self, musical_input, text_conditioning_input, audio_conditioning_input):\n",
    "        # musical_input shape: (batch_size, sequence_length, musical_feature_dim)\n",
    "        # text_conditioning_input shape: (batch_size, text_conditioning_dim)\n",
    "        # audio_conditioning_input shape: (batch_size, sequence_length, total_audio_features_dim)\n",
    "\n",
    "        # Repeat text conditioning across the sequence length\n",
    "        # text_conditioning_input shape: (batch_size, 1, text_conditioning_dim)\n",
    "        text_conditioning_repeated = text_conditioning_input.unsqueeze(1).repeat(1, musical_input.size(1), 1)\n",
    "        # text_conditioning_repeated shape: (batch_size, sequence_length, text_conditioning_dim)\n",
    "\n",
    "\n",
    "        # Concatenate inputs\n",
    "        combined_input = torch.cat((musical_input, text_conditioning_repeated, audio_conditioning_input), dim=-1)\n",
    "        # combined_input shape: (batch_size, sequence_length, musical_feature_dim + text_conditioning_dim + total_audio_features_dim)\n",
    "\n",
    "        # Pass through GRU layers\n",
    "        gru_out1, _ = self.gru1(combined_input)\n",
    "        dropout_out1 = self.dropout1(gru_out1)\n",
    "\n",
    "        gru_out2, _ = self.gru2(dropout_out1)\n",
    "        dropout_out2 = self.dropout2(gru_out2)\n",
    "\n",
    "        # Apply linear layer to each time step's output\n",
    "        output = self.linear(dropout_out2)\n",
    "        # output shape: (batch_size, sequence_length, musical_feature_dim)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Check if a GPU is available and use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model_gen_conditioned_pytorch = MusicGenerator(musical_feature_dim, total_audio_features_dim, text_conditioning_dim)\n",
    "\n",
    "# Move the model to the selected device\n",
    "model_gen_conditioned_pytorch.to(device)\n",
    "\n",
    "print(\"\\nPyTorch conditional music generation model architecture defined.\")\n",
    "print(model_gen_conditioned_pytorch)\n",
    "\n",
    "\n",
    "# --- Data Preparation ---\n",
    "# Download the dataset and directly assign the path within this cell\n",
    "print(\"Downloading the dataset to ensure path is available...\")\n",
    "try:\n",
    "    downloaded_dataset_root = kagglehub.dataset_download(\"soumendraprasad/musical-instruments-sound-dataset\")\n",
    "    print(f\"Dataset path: {downloaded_dataset_root}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    downloaded_dataset_root = None # Set to None if download fails\n",
    "\n",
    "\n",
    "if downloaded_dataset_root is None or not os.path.exists(downloaded_dataset_root):\n",
    "    print(\"Failed to get the dataset path. Cannot proceed with data preparation.\")\n",
    "else:\n",
    "    # Define feature extraction parameters (should be consistent with model definition)\n",
    "    n_mfcc = 20\n",
    "    max_padding = 174\n",
    "    n_chroma = 12 # Although not used for musical features output, needed for audio conditioning feature extraction\n",
    "    total_audio_features_dim = n_mfcc + n_chroma\n",
    "    text_conditioning_dim = 128\n",
    "\n",
    "\n",
    "    # Collect music instrument files from the downloaded dataset\n",
    "    music_instrument_files = []\n",
    "\n",
    "    # Assuming the structure from the previous file listing\n",
    "    train_submission_path = os.path.join(downloaded_dataset_root, 'Train_submission', 'Train_submission')\n",
    "    test_submission_path = os.path.join(downloaded_dataset_root, 'Test_submission', 'Test_submission') # Assuming Test_submission is also relevant for data\n",
    "\n",
    "    # Collect files from Train_submission\n",
    "    if os.path.exists(train_submission_path):\n",
    "        print(f\"Searching for music instrument files in: {train_submission_path}\")\n",
    "        for root, dirs, files in os.walk(train_submission_path):\n",
    "            for name in files:\n",
    "                if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                     # Optional: further filter by keywords if needed\n",
    "                    if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                         music_instrument_files.append(os.path.join(root, name))\n",
    "        print(f\"Found {len(music_instrument_files)} music instrument files in training submission.\")\n",
    "    else:\n",
    "        print(f\"Warning: Training submission directory not found at {train_submission_path}\")\n",
    "\n",
    "\n",
    "    # Collect files from Test_submission (if it exists and contains relevant audio)\n",
    "    # You might decide to use test data only for evaluation later, but for now, let's include relevant audio\n",
    "    if os.path.exists(test_submission_path):\n",
    "         print(f\"Searching for music instrument files in: {test_submission_path}\")\n",
    "         for root, dirs, files in os.walk(test_submission_path):\n",
    "             for name in files:\n",
    "                 if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                      # Optional: further filter by keywords if needed\n",
    "                     if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                          music_instrument_files.append(os.path.join(root, name))\n",
    "         print(f\"Found {len(music_instrument_files)} music instrument files in test submission.\")\n",
    "    else:\n",
    "         print(f\"Warning: Test submission directory not found at {test_submission_path}\")\n",
    "\n",
    "\n",
    "    # Use the collected music instrument files for feature extraction\n",
    "    files_to_process_music = music_instrument_files\n",
    "\n",
    "    # Prepare lists to store extracted features for training\n",
    "    X_train_music_features = [] # For musical input (MFCCs only)\n",
    "    X_train_audio_conditioning_features = [] # For audio conditioning input (MFCC + Chroma)\n",
    "\n",
    "\n",
    "    print(\"\\nExtracting musical features (MFCCs) and audio conditioning features (MFCC+Chroma) from music instrument files...\")\n",
    "\n",
    "    # Process a subset of files for faster testing, or all files for full training\n",
    "    # process_limit = 100 # Set a limit for testing\n",
    "    # files_to_process_subset = files_to_process_music[:process_limit]\n",
    "    files_to_process_subset = files_to_process_music # Process all files\n",
    "\n",
    "\n",
    "    for i, file_path in enumerate(files_to_process_subset):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processing file {i + 1}/{len(files_to_process_subset)}\")\n",
    "\n",
    "        # Extract MFCC features for the musical input/output\n",
    "        mfccs_only = extract_mfcc_features_only(file_path, max_padding, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract combined features for the audio conditioning input\n",
    "        combined_features = extract_features(file_path, max_padding, n_mfcc=n_mfcc, n_chroma=n_chroma)\n",
    "\n",
    "\n",
    "        if mfccs_only is not None and combined_features is not None:\n",
    "            X_train_music_features.append(mfccs_only)\n",
    "            X_train_audio_conditioning_features.append(combined_features)\n",
    "\n",
    "\n",
    "    # Convert extracted features to numpy arrays (keeping as numpy for TensorFlow training setup initially)\n",
    "    X_train_music_features = np.array(X_train_music_features)\n",
    "    X_train_audio_conditioning_features = np.array(X_train_audio_conditioning_features)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    # Ensure that the split is consistent for both sets of features\n",
    "    if len(X_train_music_features) > 0:\n",
    "        X_train_music, X_val_music, X_train_audio_cond, X_val_audio_cond = train_test_split(\n",
    "            X_train_music_features, X_train_audio_conditioning_features, test_size=0.2, random_state=42\n",
    "        )\n",
    "    else:\n",
    "        print(\"No features were successfully extracted. Cannot split data.\")\n",
    "        # Initialize empty arrays to prevent errors in subsequent steps\n",
    "        X_train_music = np.array([])\n",
    "        X_val_music = np.array([])\n",
    "        X_train_audio_cond = np.array([])\n",
    "        X_val_audio_cond = np.array([])\n",
    "\n",
    "\n",
    "    print(\"\\nFeature extraction and data splitting complete.\")\n",
    "    print(f\"Shape of training musical features (MFCCs): {X_train_music.shape}\")\n",
    "    print(f\"Shape of validation musical features (MFCCs): {X_val_music.shape}\")\n",
    "    print(f\"Shape of training audio conditioning features (MFCC+Chroma): {X_train_audio_cond.shape}\")\n",
    "    print(f\"Shape of validation audio conditioning features (MFCC+Chroma): {X_val_audio_cond.shape}\")\n",
    "\n",
    "    # Create dummy text conditioning data for training\n",
    "    # This data is not based on actual text input yet, just for model training setup\n",
    "    # It should match the expected shape of the text_conditioning_input layer\n",
    "    dummy_text_conditioning_train = np.random.rand(X_train_music.shape[0], text_conditioning_dim)\n",
    "    dummy_text_conditioning_val = np.random.rand(X_val_music.shape[0], text_conditioning_dim)\n",
    "\n",
    "\n",
    "    print(f\"\\nShape of dummy training text conditioning data: {dummy_text_conditioning_train.shape}\")\n",
    "    print(f\"Shape of dummy validation text conditioning data: {dummy_text_conditioning_val.shape}\")\n",
    "\n",
    "\n",
    "    # --- Train the Model (PyTorch) ---\n",
    "    # Check if training data is available before training\n",
    "    if 'X_train_music' in globals() and X_train_music.shape[0] > 0:\n",
    "        # Convert NumPy data to PyTorch tensors and move to device\n",
    "        X_train_music_tensor = torch.FloatTensor(X_train_music).to(device)\n",
    "        X_val_music_tensor = torch.FloatTensor(X_val_music).to(device)\n",
    "        X_train_audio_cond_tensor = torch.FloatTensor(X_train_audio_cond).to(device)\n",
    "        X_val_audio_cond_tensor = torch.FloatTensor(X_val_audio_cond).to(device)\n",
    "        dummy_text_conditioning_train_tensor = torch.FloatTensor(dummy_text_conditioning_train).to(device)\n",
    "        dummy_text_conditioning_val_tensor = torch.FloatTensor(dummy_text_conditioning_val).to(device)\n",
    "\n",
    "        # Create PyTorch DataLoaders\n",
    "        from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_music_tensor, dummy_text_conditioning_train_tensor, X_train_audio_cond_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        val_dataset = TensorDataset(X_val_music_tensor, dummy_text_conditioning_val_tensor, X_val_audio_cond_tensor)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32) # No need to shuffle validation data\n",
    "\n",
    "        print(\"\\nData converted to PyTorch tensors and DataLoaders created for training.\")\n",
    "\n",
    "        # Define Loss Function and Optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer_pytorch = torch.optim.Adam(model_gen_conditioned_pytorch.parameters(), lr=0.001) # You can adjust the learning rate\n",
    "\n",
    "\n",
    "        num_epochs = 50 # You can adjust the number of epochs\n",
    "        print(f\"\\nStarting PyTorch model training for {num_epochs} epochs...\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model_gen_conditioned_pytorch.train() # Set the model to training mode\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for i, (musical_inputs, text_cond_inputs, audio_cond_inputs) in enumerate(train_loader):\n",
    "                # Zero the parameter gradients\n",
    "                optimizer_pytorch.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model_gen_conditioned_pytorch(musical_inputs, text_cond_inputs, audio_cond_inputs)\n",
    "\n",
    "                # Calculate loss\n",
    "                # The target for musical_inputs is the same as the input in this autoencoder-like setup\n",
    "                loss = criterion(outputs, musical_inputs)\n",
    "\n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer_pytorch.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Print statistics\n",
    "                if (i + 1) % 10 == 0: # Print every 10 batches\n",
    "                    print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}] finished. Average Training Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "            # Validation step\n",
    "            model_gen_conditioned_pytorch.eval() # Set the model to evaluation mode\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad(): # Disable gradient calculation for validation\n",
    "                for musical_inputs_val, text_cond_inputs_val, audio_cond_inputs_val in val_loader:\n",
    "                    outputs_val = model_gen_conditioned_pytorch(musical_inputs_val, text_cond_inputs_val, audio_cond_inputs_val)\n",
    "                    loss_val = criterion(outputs_val, musical_inputs_val)\n",
    "                    val_loss += loss_val.item()\n",
    "\n",
    "            print(f'Validation Loss: {val_loss / len(val_loader):.4f}')\n",
    "\n",
    "        print(\"\\nPyTorch model training complete.\")\n",
    "    else:\n",
    "        print(\"Training data is empty. Skipping PyTorch model training.\")\n",
    "\n",
    "\n",
    "# --- Function to generate musical features iteratively (PyTorch) ---\n",
    "def generate_musical_features_iterative_pytorch(model, start_sequence, text_conditioning_input, audio_conditioning_input, total_generation_steps):\n",
    "    \"\"\"Generates new MFCC sequences step-by-step using the trained PyTorch conditional model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained PyTorch conditional generation model.\n",
    "        start_sequence: A PyTorch tensor representing the initial musical sequence\n",
    "                        (shape: (1, sequence_length, musical_feature_dim)).\n",
    "        text_conditioning_input: The text conditioning input (shape: (1, text_conditioning_dim)).\n",
    "        audio_conditioning_input: The audio conditioning input (sequence of features,\n",
    "                                   shape: (1, sequence_length, total_audio_features_dim)).\n",
    "                                   This is the *full* audio conditioning sequence.\n",
    "        total_generation_steps: The total number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated musical MFCC sequence (shape: (total_generation_steps, musical_feature_dim)),\n",
    "        or None if generation fails.\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating musical feature sequence iteratively for {total_generation_steps} steps using PyTorch model...\")\n",
    "\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    generated_sequence = [] # List to store generated time steps\n",
    "    current_musical_sequence = start_sequence.clone().to(device) # Start with the initial sequence on the correct device\n",
    "    sequence_length = start_sequence.shape[1] # Expected input sequence length for the model\n",
    "    musical_feature_dim = start_sequence.shape[2] # Dimension of musical features\n",
    "\n",
    "    # Ensure conditioning inputs are on the correct device\n",
    "    text_conditioning_input = text_conditioning_input.to(device)\n",
    "    audio_conditioning_input = audio_conditioning_input.to(device)\n",
    "\n",
    "\n",
    "    # Ensure audio_conditioning_input has a batch dimension and correct sequence length\n",
    "    if audio_conditioning_input.ndim != 3 or audio_conditioning_input.shape[1] != sequence_length:\n",
    "         print(\"Error: audio_conditioning_input must have shape (1, sequence_length, total_audio_features_dim).\")\n",
    "         return None\n",
    "\n",
    "    # Ensure text_conditioning_input has a batch dimension and correct dimension\n",
    "    if text_conditioning_input.ndim != 2 or text_conditioning_input.shape[1] != text_conditioning_dim:\n",
    "        print(\"Error: text_conditioning_input must have shape (1, text_conditioning_dim).\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation during generation\n",
    "        for i in range(total_generation_steps):\n",
    "            # The model expects [musical_input, text_conditioning_input, audio_conditioning_input]\n",
    "            model_inputs = [current_musical_sequence, text_conditioning_input, audio_conditioning_input]\n",
    "\n",
    "            # Predict the next time step of musical features\n",
    "            try:\n",
    "                # The model predicts a sequence of length `musical_sequence_length`.\n",
    "                # We are interested in the prediction for the *next* time step.\n",
    "                predicted_sequence = model(*model_inputs) # Pass inputs unpacking the list\n",
    "                # predicted_sequence shape: (1, musical_sequence_length, musical_feature_dim)\n",
    "\n",
    "                # Take the last time step of the prediction\n",
    "                predicted_next_step = predicted_sequence[:, -1:, :] # Shape (1, 1, musical_feature_dim)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during model prediction at step {i}: {e}\")\n",
    "                return None\n",
    "\n",
    "\n",
    "            # Append the predicted time step to the generated sequence (as numpy array)\n",
    "            generated_sequence.append(predicted_next_step.squeeze().cpu().numpy()) # Move to CPU and convert to numpy\n",
    "\n",
    "            # Update the current musical sequence by removing the oldest time step and adding the new one\n",
    "            # This sliding window approach is standard for iterative sequence generation.\n",
    "            # Ensure the sequence length remains constant\n",
    "            current_musical_sequence = torch.cat((current_musical_sequence[:, 1:, :], predicted_next_step), dim=1)\n",
    "\n",
    "            # Print progress\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Generated {i + 1}/{total_generation_steps} steps.\")\n",
    "\n",
    "\n",
    "    # Convert the list of generated steps into a numpy array\n",
    "    generated_sequence_np = np.array(generated_sequence) # Shape (total_generation_steps, musical_feature_dim)\n",
    "\n",
    "    print(\"\\nIterative musical feature generation complete.\")\n",
    "    print(f\"Shape of the generated musical features: {generated_sequence_np.shape}\")\n",
    "\n",
    "    # Add inspection of generated features\n",
    "    print(\"\\nGenerated Musical Features Statistics:\")\n",
    "    print(f\"Shape: {generated_sequence_np.shape}\")\n",
    "    print(f\"Mean: {np.mean(generated_sequence_np)}\")\n",
    "    print(f\"Min: {np.min(generated_sequence_np)}\")\n",
    "    print(f\"Max: {np.max(generated_sequence_np)}\")\n",
    "    print(f\"Standard Deviation: {np.std(generated_sequence_np)}\")\n",
    "\n",
    "    # Save generated features to a file for inspection\n",
    "    features_output_filename = 'generated_musical_features_iterative_pytorch.npy'\n",
    "    np.save(features_output_filename, generated_sequence_np)\n",
    "    print(f\"Generated musical features saved to {features_output_filename}\")\n",
    "\n",
    "\n",
    "    return generated_sequence_np\n",
    "\n",
    "\n",
    "# --- Function to convert musical features to audio and save ---\n",
    "def convert_features_to_audio(musical_features, sr=22050, output_filename_wav='generated_music.wav', output_filename_mp3='generated_music.mp3'):\n",
    "    \"\"\"Converts musical features (MFCCs) to an audio file and saves it.\n",
    "\n",
    "    Args:\n",
    "        musical_features: A numpy array containing the generated musical MFCC sequence (shape: (time_steps, n_mfcc)).\n",
    "        sr: Sampling rate for audio output.\n",
    "        output_filename_wav: Filename for the output WAV file.\n",
    "        output_filename_mp3: Filename for the output MP3 file.\n",
    "\n",
    "    Returns:\n",
    "        The path to the generated audio file (MP3 or WAV), or None if conversion/saving fails.\n",
    "    \"\"\"\n",
    "    if musical_features is None or musical_features.shape[0] == 0:\n",
    "        print(\"No musical features provided for audio conversion.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\nConverting generated musical features to audio...\")\n",
    "    try:\n",
    "        # Assuming the features are MFCCs. Transpose back to (n_mfcc, time_steps) for inverse transform\n",
    "        audio_time_series = librosa.feature.inverse.mfcc_to_audio(musical_features.T, sr=sr)\n",
    "\n",
    "        # Add inspection of generated audio time series\n",
    "        print(\"\\nGenerated Audio Time Series Statistics:\")\n",
    "        print(f\"Shape: {audio_time_series.shape}\")\n",
    "        print(f\"Mean: {np.mean(audio_time_series)}\")\n",
    "        print(f\"Min: {np.min(audio_time_series)}\")\n",
    "        print(f\"Max: {np.max(audio_time_series)}\")\n",
    "        print(f\"Standard Deviation: {np.std(audio_time_series)}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MFCC to audio conversion: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Save the generated audio to a WAV file first\n",
    "    print(f\"Saving generated audio to {output_filename_wav}...\")\n",
    "    try:\n",
    "        sf.write(output_filename_wav, audio_time_series, sr)\n",
    "        print(f\"Generated audio saved successfully to {output_filename_wav}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving WAV file: {e}\")\n",
    "        return None\n",
    "\n",
    "    generated_audio_file = output_filename_wav # Default to WAV\n",
    "\n",
    "    # Attempt to convert the WAV file to MP3\n",
    "    print(f\"Attempting to convert {output_filename_wav} to {output_filename_mp3}...\")\n",
    "    try:\n",
    "        audio = AudioSegment.from_wav(output_filename_wav)\n",
    "        audio.export(output_filename_mp3, format=\"mp3\")\n",
    "        print(f\"Conversion to MP3 successful. Output saved to {output_filename_mp3}\")\n",
    "        generated_audio_file = output_filename_mp3 # Update to MP3 if successful\n",
    "        # Optionally, remove the intermediate WAV file\n",
    "        # os.remove(output_filename_wav)\n",
    "        # print(f\"Removed intermediate WAV file: {output_filename_wav}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MP3 conversion: {e}. Proceeding with WAV file.\")\n",
    "        # generated_audio_file remains the WAV file path\n",
    "\n",
    "    return generated_audio_file\n",
    "\n",
    "\n",
    "# --- Function to convert generated features to symbolic music (music21) (Step 4 in Plan) ---\n",
    "def convert_features_to_music21(musical_features_np, sr=22050, hop_length=512):\n",
    "    \"\"\"Converts generated musical features (MFCCs) to a music21 stream.\n",
    "\n",
    "    This is a placeholder function. Converting MFCCs directly to\n",
    "    meaningful symbolic music (notes, chords, rhythm) is a complex task\n",
    "    and requires more advanced techniques (e.g., a separate model trained\n",
    "    to map features to musical events).\n",
    "\n",
    "    Args:\n",
    "        musical_features_np: A numpy array containing the generated musical MFCC sequence (shape: (time_steps, n_mfcc)).\n",
    "        sr: Sampling rate.\n",
    "        hop_length: Hop length used during feature extraction.\n",
    "\n",
    "    Returns:\n",
    "        A music21 stream, or None if conversion is not implemented or fails.\n",
    "    \"\"\"\n",
    "    print(\"\\nAttempting to convert musical features to music21 stream...\")\n",
    "    print(\"Note: Direct conversion of MFCCs to symbolic music is complex and requires further development.\")\n",
    "    # Placeholder implementation: Create a simple stream for demonstration\n",
    "    from music21 import stream, note, duration, tempo, pitch\n",
    "\n",
    "    s = stream.Stream()\n",
    "\n",
    "    # Add a tempo indication (optional, but good practice)\n",
    "    # Estimate tempo from features or use a default\n",
    "    # For simplicity, using a default tempo\n",
    "    s.append(tempo.MetronomeMark(number=120))\n",
    "\n",
    "    # Simple approach: Map a feature value (e.g., the first MFCC coefficient) to pitch\n",
    "    # and use a fixed duration based on the hop length. This is a very basic mapping.\n",
    "    time_per_step = hop_length / sr # Duration of each feature vector in seconds\n",
    "\n",
    "    # Define a mapping range for MFCC values to MIDI pitches (adjust as needed)\n",
    "    # Find min/max of the first MFCC coefficient in the generated sequence\n",
    "    if musical_features_np.shape[0] > 0:\n",
    "        min_mfcc1 = np.min(musical_features_np[:, 0])\n",
    "        max_mfcc1 = np.max(musical_features_np[:, 0])\n",
    "        midi_pitch_range = 60 # Map to a range of 60 MIDI pitches (e.g., from 30 to 90)\n",
    "        min_midi_pitch = 30\n",
    "\n",
    "        for i, features in enumerate(musical_features_np):\n",
    "            # Use the first MFCC coefficient for pitch mapping\n",
    "            mfcc1 = features[0]\n",
    "\n",
    "            # Normalize the MFCC value to the range [0, 1]\n",
    "            if (max_mfcc1 - min_mfcc1) > 0:\n",
    "                normalized_mfcc1 = (mfcc1 - min_mfcc1) / (max_mfcc1 - min_mfcc1)\n",
    "            else:\n",
    "                normalized_mfcc1 = 0.5 # Default to middle if range is zero\n",
    "\n",
    "            # Map the normalized value to a MIDI pitch\n",
    "            midi_pitch_val = int(min_midi_pitch + normalized_mfcc1 * midi_pitch_range)\n",
    "            midi_pitch_val = max(0, min(127, midi_pitch_val)) # Clamp to valid MIDI range\n",
    "\n",
    "            try:\n",
    "                 # Create a note\n",
    "                 n = note.Note(midi_pitch_val)\n",
    "                 # Set the duration based on the time per feature step\n",
    "                 # Using a fixed duration for simplicity, could be refined\n",
    "                 n.duration = duration.Duration(time_per_step)\n",
    "\n",
    "                 s.append(n)\n",
    "            except Exception as note_error:\n",
    "                 print(f\"Error creating music21 note at step {i}: {note_error}\")\n",
    "                 # Optionally, append a rest or skip this step\n",
    "                 s.append(note.Rest(duration=duration.Duration(time_per_step)))\n",
    "\n",
    "\n",
    "            # Avoid creating an excessively large stream for demonstration\n",
    "            if i > 200: # Limit the number of elements for a quicker conversion/display\n",
    "                 break\n",
    "    else:\n",
    "         print(\"No generated features to convert to music21.\")\n",
    "         return None\n",
    "\n",
    "\n",
    "    print(f\"Created a music21 stream with {len(s.elements)} elements.\") # Use .elements to count top-level elements\n",
    "\n",
    "    return s\n",
    "\n",
    "# --- Function to save and display music21 stream (Step 5 in Plan) ---\n",
    "def save_and_display_music21_stream(music21_stream, output_filename_midi='generated_music.mid'):\n",
    "    \"\"\"Saves a music21 stream as a MIDI file and displays its text representation.\"\"\"\n",
    "    if music21_stream is None:\n",
    "        print(\"No music21 stream provided to save or display.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nSaving music21 stream to {output_filename_midi}...\")\n",
    "    try:\n",
    "        music21_stream.write('midi', fp=output_filename_midi)\n",
    "        print(f\"Music21 stream saved successfully to {output_filename_midi}\")\n",
    "\n",
    "        # Display the stream as text\n",
    "        print(\"\\nGenerated Music21 Stream (text representation):\")\n",
    "        print(music21_stream.show('text'))\n",
    "\n",
    "        return output_filename_midi # Return the path to the saved MIDI file\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving or displaying music21 stream: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Main music generation function (uses iterative generation with PyTorch model) ---\n",
    "def generate_music_from_input_v4(model, user_text_input=None, user_audio_input_path=None, input_type='text_prompt', target_audio_duration_sec=30, sr=22050, max_padding=174, n_mfcc=20, n_chroma=12, text_conditioning_dim=128):\n",
    "    \"\"\"Generates music based on user input (text and/or audio) using the PyTorch model and iterative generation.\n",
    "\n",
    "    Args:\n",
    "        model: The trained PyTorch conditional music generation model.\n",
    "        user_text_input: The user's text input (string for lyrics or prompt).\n",
    "        user_audio_input_path: Path to the user's audio file.\n",
    "        input_type: Type of text input ('text_lyrics', 'text_prompt'). 'audio_only' is also possible if no text is provided.\n",
    "        target_audio_duration_sec: The desired duration of the generated audio in seconds.\n",
    "        sr: Sampling rate for audio output.\n",
    "        max_padding: Target sequence length for padding/truncating features (used for initial sequence).\n",
    "        n_mfcc: Number of MFCCs extracted.\n",
    "        n_chroma: Number of chroma bins extracted.\n",
    "        text_conditioning_dim: Dimension of the text embedding.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - The path to the generated audio file (MP3 or WAV),\n",
    "        - The generated lyrics (string),\n",
    "        - The generated musical features (numpy array),\n",
    "        or (None, None, None) if an error occurred.\n",
    "    \"\"\"\n",
    "    text_conditioning_input_for_model = None\n",
    "    audio_conditioning_input_for_model = None\n",
    "    generated_lyrics = None\n",
    "\n",
    "    # Process text input if provided\n",
    "    if user_text_input:\n",
    "        is_prompt = (input_type == 'text_prompt')\n",
    "        conditioning_input_for_model, generated_lyrics = process_text_input(user_text_input, text_conditioning_dim, is_prompt=is_prompt)\n",
    "        if conditioning_input_for_model is None:\n",
    "            print(\"Error processing text input for conditioning.\")\n",
    "            return None, None, None\n",
    "        text_conditioning_input_for_model = conditioning_input_for_model\n",
    "\n",
    "    else:\n",
    "        print(\"No text input provided. Using dummy text conditioning.\")\n",
    "        # Create dummy text conditioning input as a PyTorch tensor and move to device\n",
    "        text_conditioning_input_for_model = torch.zeros((1, text_conditioning_dim)).to(device)\n",
    "\n",
    "\n",
    "    # Process audio input if provided\n",
    "    if user_audio_input_path and os.path.exists(user_audio_input_path):\n",
    "        print(\"Processing user audio input for conditioning.\")\n",
    "        # For iterative generation, we still need the full audio conditioning sequence\n",
    "        audio_conditioning_input_for_model = process_user_audio_for_conditioning(user_audio_input_path, max_padding, n_mfcc, n_chroma)\n",
    "        if audio_conditioning_input_for_model is None:\n",
    "            print(\"Error processing user audio input for conditioning.\")\n",
    "            return None, None, None\n",
    "    else:\n",
    "        print(\"No user audio file provided or found. Using dummy audio conditioning.\")\n",
    "        # Create dummy audio conditioning input as a PyTorch tensor and move to device\n",
    "        audio_conditioning_input_for_model = torch.zeros((1, max_padding, total_audio_features_dim)).to(device)\n",
    "\n",
    "\n",
    "    # Ensure both conditioning inputs are available\n",
    "    if text_conditioning_input_for_model is None or audio_conditioning_input_for_model is None:\n",
    "         print(\"Missing conditioning input(s). Cannot proceed.\")\n",
    "         return None, None, None\n",
    "\n",
    "\n",
    "    # Generate a starting musical sequence (e.g., take a random sample from the training data)\n",
    "    # This starting sequence will be the initial input for the iterative generation.\n",
    "    # Ensure X_train_music is available from data loading\n",
    "    if 'X_train_music' not in globals() or X_train_music.shape[0] == 0:\n",
    "         print(\"Training data (X_train_music) is not available or empty. Cannot generate a starting sequence.\")\n",
    "         return None, None, None\n",
    "\n",
    "    # Select a random starting sequence and convert to PyTorch tensor, add batch dim, move to device\n",
    "    start_index = np.random.randint(0, X_train_music.shape[0])\n",
    "    start_sequence_np = X_train_music[start_index:start_index+1, :, :]\n",
    "    start_sequence_tensor = torch.FloatTensor(start_sequence_np).to(device)\n",
    "\n",
    "\n",
    "    # Calculate the total number of time steps to generate\n",
    "    # Each time step in the feature sequence corresponds to hop_length samples (default 512 in librosa mfcc)\n",
    "    # Duration of one feature time step = hop_length / sr\n",
    "    # Total time steps needed = target_audio_duration_sec / (hop_length / sr) = target_audio_duration_sec * sr / hop_length\n",
    "    # Assuming default hop_length of 512\n",
    "    hop_length = 512\n",
    "    total_generation_steps = int(target_audio_duration_sec * sr / hop_length)\n",
    "    print(f\"Targeting {target_audio_duration_sec} seconds, which requires approximately {total_generation_steps} generation steps.\")\n",
    "\n",
    "\n",
    "    # Generate new musical features iteratively using the trained PyTorch conditional model\n",
    "    generated_musical_features_np = generate_musical_features_iterative_pytorch(\n",
    "        model,\n",
    "        start_sequence_tensor,\n",
    "        text_conditioning_input_for_model,\n",
    "        audio_conditioning_input_for_model,\n",
    "        total_generation_steps\n",
    "    )\n",
    "\n",
    "\n",
    "    if generated_musical_features_np is None:\n",
    "        print(\"Feature generation failed.\")\n",
    "        return None, generated_lyrics, None # Return None for audio path and features\n",
    "\n",
    "\n",
    "    # Convert the generated musical features to audio and save\n",
    "    generated_audio_file = convert_features_to_audio(generated_musical_features_np, sr=sr)\n",
    "\n",
    "\n",
    "    # Optionally, save the generated lyrics to a text file if lyrics were generated\n",
    "    lyrics_output_filename = 'generated_lyrics.txt'\n",
    "    if generated_lyrics:\n",
    "        try:\n",
    "            with open(lyrics_output_filename, 'w') as f:\n",
    "                f.write(generated_lyrics)\n",
    "            print(f\"Generated lyrics saved to {lyrics_output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving generated lyrics to file: {e}\")\n",
    "\n",
    "\n",
    "    return generated_audio_file, generated_lyrics, generated_musical_features_np # Return features\n",
    "\n",
    "\n",
    "# --- Example Usage (requires trained PyTorch model and data) ---\n",
    "# Assume model_gen_conditioned_pytorch is trained and X_train_music is defined.\n",
    "\n",
    "# Example: Generate music from a text prompt and user audio, aiming for 30 seconds\n",
    "user_text_prompt = \"Generate a piece of background music.\"\n",
    "# IMPORTANT: Replace '/content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg' with the actual path to your uploaded audio file\n",
    "# Ensure the audio file exists before running\n",
    "user_audio_input_path = '/content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg'\n",
    "if not os.path.exists(user_audio_input_path):\n",
    "    print(f\"Error: User audio file not found at {user_audio_input_path}. Please upload the file.\")\n",
    "    user_audio_input_path = None # Set to None if file doesn't exist\n",
    "\n",
    "target_duration_seconds = 30\n",
    "\n",
    "if user_audio_input_path is not None and 'model_gen_conditioned_pytorch' in globals() and 'X_train_music' in globals() and X_train_music.shape[0] > 0:\n",
    "    print(f\"\\nAttempting to generate music from text prompt: '{user_text_prompt}' using audio file at {user_audio_input_path}, targeting {target_duration_seconds} seconds.\")\n",
    "    generated_music_file, generated_lyrics_text, generated_features_np = generate_music_from_input_v4(\n",
    "        model_gen_conditioned_pytorch, # Use the PyTorch model\n",
    "        user_text_input=user_text_prompt,\n",
    "        user_audio_input_path=user_audio_input_path,\n",
    "        input_type='text_prompt',\n",
    "        target_audio_duration_sec=target_duration_seconds,\n",
    "        sr=sr,\n",
    "        max_padding=max_padding,\n",
    "        n_mfcc=n_mfcc,\n",
    "        n_chroma=n_chroma,\n",
    "        text_conditioning_dim=text_conditioning_dim\n",
    "    )\n",
    "\n",
    "\n",
    "    if generated_music_file:\n",
    "        print(f\"\\nGenerated music file: {generated_music_file}\")\n",
    "        if generated_lyrics_text:\n",
    "            print(f\"Generated lyrics:\\n{generated_lyrics_text}\")\n",
    "        # Display links to the generated files\n",
    "        print(\"\\nDownload the generated music file:\")\n",
    "        display(FileLink(generated_music_file))\n",
    "        if generated_lyrics_text: # Only display lyrics link if lyrics were generated\n",
    "            print(\"Download the generated lyrics file:\")\n",
    "            display(FileLink('generated_lyrics.txt'))\n",
    "\n",
    "        # Play the generated audio\n",
    "        print(f\"\\nPlaying generated audio file: {generated_music_file}\")\n",
    "        display(Audio(generated_music_file))\n",
    "\n",
    "        # Step 4: Convert generated features to symbolic music (music21)\n",
    "        if generated_features_np is not None:\n",
    "            music21_stream = convert_features_to_music21(generated_features_np, sr=sr, hop_length=512) # Pass hop_length\n",
    "\n",
    "            # Step 5: Save and display symbolic music (music21)\n",
    "            if music21_stream:\n",
    "                generated_midi_file = save_and_display_music21_stream(music21_stream)\n",
    "                if generated_midi_file:\n",
    "                    print(\"\\nDownload the generated MIDI file:\")\n",
    "                    display(FileLink(generated_midi_file))\n",
    "        else:\n",
    "            print(\"\\nNo generated features available for music21 conversion.\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"\\nMusic generation failed.\")\n",
    "else:\n",
    "    if user_audio_input_path is None:\n",
    "         print(\"\\nSkipping music generation as user audio file was not found.\")\n",
    "    else:\n",
    "         print(\"\\nSkipping music generation. PyTorch model or training data not available.\")\n",
    "\"\"\"\n",
    "    get_ipython().run_cell(raw_cell=regenerated_cell_content, cell_id='a1b11bff')\n",
    "\n",
    "print(\"\\nProceeding with music generation using the updated function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "c0898587",
    "outputId": "d3ac72a9-f7d0-49bc-ec47-0fe25a9aecdd"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (ipython-input-2070600658.py, line 868)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2070600658.py\"\u001b[0;36m, line \u001b[0;32m868\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# Regenerate the content of cell 'a1b11bff' to ensure functions and model are defined\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "from pydub import AudioSegment # Import AudioSegment for MP3 conversion\n",
    "from transformers import pipeline # Import pipeline for using pre-trained models\n",
    "from IPython.display import display, FileLink, Audio # Import display, FileLink, and Audio\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split for data splitting\n",
    "import kagglehub # Import kagglehub for dataset download\n",
    "import tensorflow as tf # Import tensorflow\n",
    "from tensorflow.keras.models import Model # Import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, GRU, Input, TimeDistributed, Bidirectional # Import necessary layers\n",
    "import torch # Import PyTorch\n",
    "import torch.nn as nn # Import PyTorch neural network module\n",
    "from music21 import stream, note, duration, tempo # Import music21 elements\n",
    "\n",
    "\n",
    "# Define sampling rate (needed for audio processing and saving)\n",
    "sr = 22050 # Common sampling rate, adjust if your audio files use a different rate\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This needs to be consistent with feature extraction\n",
    "n_chroma = 12 # Number of chroma bins\n",
    "total_audio_features_dim = n_mfcc + n_chroma # Total features for audio conditioning\n",
    "\n",
    "# Define the dimensions for the music generation model\n",
    "musical_feature_dim = n_mfcc # The model output is MFCCs\n",
    "musical_sequence_length = max_padding # The model input sequence length\n",
    "\n",
    "# Define the conditioning input dimensions\n",
    "text_conditioning_dim = 128 # Dimension of the text embedding\n",
    "\n",
    "# --- Helper functions ---\n",
    "\n",
    "# Initialize the text generation pipeline (if not already initialized)\n",
    "lyric_generator = None # Initialize to None\n",
    "try:\n",
    "    lyric_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "    print(\"Text generation pipeline (distilgpt2) initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing text generation pipeline: {e}\")\n",
    "    lyric_generator = None\n",
    "\n",
    "\n",
    "def generate_lyrics_with_language_model(prompt):\n",
    "    \"\"\"Generates lyrics based on a prompt using a language model.\"\"\"\n",
    "    if lyric_generator:\n",
    "        print(f\"Generating lyrics for prompt: '{prompt}' using DistilGPT-2.\")\n",
    "        try:\n",
    "            # Generate text with a reasonable length and without generating too many sequences\n",
    "            generated_text = lyric_generator(prompt, max_length=100, num_return_sequences=1, truncation=True)\n",
    "            lyrics = generated_text[0]['generated_text']\n",
    "            # Basic post-processing: remove the prompt from the generated text if it's repeated\n",
    "            if lyrics.startswith(prompt):\n",
    "                lyrics = lyrics[len(prompt):].strip()\n",
    "            print(\"Lyric generation successful.\")\n",
    "            return lyrics\n",
    "        except Exception as e:\n",
    "            print(f\"Error during lyric generation: {e}\")\n",
    "            return f\"Error generating lyrics for prompt: '{prompt}'\" # Return an error message\n",
    "    else:\n",
    "        print(\"Lyric generator not initialized. Cannot generate lyrics.\")\n",
    "        return f\"Cannot generate lyrics for prompt: '{prompt}' (generator not available)\"\n",
    "\n",
    "\n",
    "# Function to get text embeddings (placeholder)\n",
    "def get_text_embedding(text, embedding_dim):\n",
    "    \"\"\"Placeholder function to get a fixed-size embedding for text.\"\"\"\n",
    "    print(f\"Getting embedding for text: '{text[:50]}...' using a placeholder model.\")\n",
    "    # Return as numpy array for now, will convert to tensor later in process_text_input\n",
    "    return np.random.rand(1, embedding_dim) # Shape (1, embedding_dim)\n",
    "\n",
    "\n",
    "# Function to process text input\n",
    "def process_text_input(user_input, embedding_dim, is_prompt=False):\n",
    "    \"\"\"Processes user text input (lyrics or prompt) and returns a conditioning embedding.\"\"\"\n",
    "    if is_prompt:\n",
    "        lyrics = generate_lyrics_with_language_model(user_input)\n",
    "    else:\n",
    "        lyrics = user_input # User provided lyrics directly\n",
    "\n",
    "    conditioning_embedding_np = get_text_embedding(lyrics, embedding_dim)\n",
    "\n",
    "    # Convert to PyTorch tensor and move to device\n",
    "    conditioning_embedding_tensor = torch.FloatTensor(conditioning_embedding_np).to(device)\n",
    "\n",
    "\n",
    "    return conditioning_embedding_tensor, lyrics # Return as PyTorch tensor\n",
    "\n",
    "\n",
    "# Function to extract combined features (MFCC + Chroma) from an audio file.\n",
    "# This function is used for both training data preparation and user audio conditioning.\n",
    "def extract_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "    \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract Chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "        # Pad or truncate combined features to a fixed length\n",
    "        if combined_features.shape[1] < target_sequence_length:\n",
    "            combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "        else:\n",
    "            combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "        return combined_features.T # Return as numpy array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process user audio for conditioning - calls extract_features\n",
    "def process_user_audio_for_conditioning(audio_file_path, target_sequence_length, n_mfcc, n_chroma):\n",
    "    \"\"\"Processes user audio input and returns features suitable for conditioning.\"\"\"\n",
    "    processed_features_np = extract_features(audio_file_path, target_sequence_length, n_mfcc, n_chroma)\n",
    "\n",
    "    if processed_features_np is not None:\n",
    "        # Convert to PyTorch tensor, add a batch dimension, and move to device\n",
    "        processed_features_tensor = torch.FloatTensor(processed_features_np).unsqueeze(0).to(device) # Shape (1, target_sequence_length, total_feature_dimension)\n",
    "        print(f\"Processed user audio features shape for conditioning: {processed_features_tensor.shape}\")\n",
    "        return processed_features_tensor\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to extract MFCC features only (used for model input/output targets)\n",
    "def extract_mfcc_features_only(file_path, target_sequence_length, n_mfcc=20):\n",
    "    \"\"\"Extract only MFCC features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Pad or truncate MFCCs to a fixed length\n",
    "        if mfccs.shape[1] < target_sequence_length:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, target_sequence_length - mfccs.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :target_sequence_length]\n",
    "\n",
    "        return mfccs.T # Return as numpy array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Model Definition (PyTorch) ---\n",
    "\n",
    "# Define the PyTorch model\n",
    "class MusicGenerator(nn.Module):\n",
    "    def __init__(self, musical_feature_dim, total_audio_features_dim, text_conditioning_dim, hidden_dim=128):\n",
    "        super(MusicGenerator, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.musical_feature_dim = musical_feature_dim\n",
    "        self.total_audio_features_dim = total_audio_features_dim\n",
    "        self.text_conditioning_dim = text_conditioning_dim\n",
    "\n",
    "        # GRU layers\n",
    "        # Input to GRU will be concatenation of musical features, text conditioning, and audio conditioning\n",
    "        # Dimension of combined input: musical_feature_dim + text_conditioning_dim + total_audio_features_dim\n",
    "        self.gru1 = nn.GRU(musical_feature_dim + text_conditioning_dim + total_audio_features_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.gru2 = nn.GRU(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True) # *2 for bidirectional output\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        # Output layer: predicts musical features for each time step\n",
    "        # Since the GRU returns sequences, we use a TimeDistributed-like approach with a linear layer\n",
    "        self.linear = nn.Linear(hidden_dim * 2, musical_feature_dim) # *2 for bidirectional output\n",
    "\n",
    "    def forward(self, musical_input, text_conditioning_input, audio_conditioning_input):\n",
    "        # musical_input shape: (batch_size, sequence_length, musical_feature_dim)\n",
    "        # text_conditioning_input shape: (batch_size, text_conditioning_dim)\n",
    "        # audio_conditioning_input shape: (batch_size, sequence_length, total_audio_features_dim)\n",
    "\n",
    "        # Repeat text conditioning across the sequence length\n",
    "        # text_conditioning_input shape: (batch_size, 1, text_conditioning_dim)\n",
    "        text_conditioning_repeated = text_conditioning_input.unsqueeze(1).repeat(1, musical_input.size(1), 1)\n",
    "        # text_conditioning_repeated shape: (batch_size, sequence_length, text_conditioning_dim)\n",
    "\n",
    "\n",
    "        # Concatenate inputs\n",
    "        combined_input = torch.cat((musical_input, text_conditioning_repeated, audio_conditioning_input), dim=-1)\n",
    "        # combined_input shape: (batch_size, sequence_length, musical_feature_dim + text_conditioning_dim + total_audio_features_dim)\n",
    "\n",
    "        # Pass through GRU layers\n",
    "        gru_out1, _ = self.gru1(combined_input)\n",
    "        dropout_out1 = self.dropout1(gru_out1)\n",
    "\n",
    "        gru_out2, _ = self.gru2(dropout_out1)\n",
    "        dropout_out2 = self.dropout2(gru_out2)\n",
    "\n",
    "        # Apply linear layer to each time step's output\n",
    "        output = self.linear(dropout_out2)\n",
    "        # output shape: (batch_size, sequence_length, musical_feature_dim)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Check if a GPU is available and use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model_gen_conditioned_pytorch = MusicGenerator(musical_feature_dim, total_audio_features_dim, text_conditioning_dim)\n",
    "\n",
    "# Move the model to the selected device\n",
    "model_gen_conditioned_pytorch.to(device)\n",
    "\n",
    "print(\"\\nPyTorch conditional music generation model architecture defined.\")\n",
    "print(model_gen_conditioned_pytorch)\n",
    "\n",
    "\n",
    "# --- Data Preparation ---\n",
    "# Download the dataset and directly assign the path within this cell\n",
    "print(\"Downloading the dataset to ensure path is available...\")\n",
    "try:\n",
    "    downloaded_dataset_root = kagglehub.dataset_download(\"soumendraprasad/musical-instruments-sound-dataset\")\n",
    "    print(f\"Dataset path: {downloaded_dataset_root}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    downloaded_dataset_root = None # Set to None if download fails\n",
    "\n",
    "\n",
    "if downloaded_dataset_root is None or not os.path.exists(downloaded_dataset_root):\n",
    "    print(\"Failed to get the dataset path. Cannot proceed with data preparation.\")\n",
    "else:\n",
    "    # Define feature extraction parameters (should be consistent with model definition)\n",
    "    n_mfcc = 20\n",
    "    max_padding = 174\n",
    "    n_chroma = 12 # Although not used for musical features output, needed for audio conditioning feature extraction\n",
    "    total_audio_features_dim = n_mfcc + n_chroma\n",
    "    text_conditioning_dim = 128\n",
    "\n",
    "\n",
    "    # Collect music instrument files from the downloaded dataset\n",
    "    music_instrument_files = []\n",
    "\n",
    "    # Assuming the structure from the previous file listing\n",
    "    train_submission_path = os.path.join(downloaded_dataset_root, 'Train_submission', 'Train_submission')\n",
    "    test_submission_path = os.path.join(downloaded_dataset_root, 'Test_submission', 'Test_submission') # Assuming Test_submission is also relevant for data\n",
    "\n",
    "    # Collect files from Train_submission\n",
    "    if os.path.exists(train_submission_path):\n",
    "        print(f\"Searching for music instrument files in: {train_submission_path}\")\n",
    "        for root, dirs, files in os.walk(train_submission_path):\n",
    "            for name in files:\n",
    "                if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                     # Optional: further filter by keywords if needed\n",
    "                    if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                         music_instrument_files.append(os.path.join(root, name))\n",
    "        print(f\"Found {len(music_instrument_files)} music instrument files in training submission.\")\n",
    "    else:\n",
    "        print(f\"Warning: Training submission directory not found at {train_submission_path}\")\n",
    "\n",
    "\n",
    "    # Collect files from Test_submission (if it exists and contains relevant audio)\n",
    "    # You might decide to use test data only for evaluation later, but for now, let's include relevant audio\n",
    "    if os.path.exists(test_submission_path):\n",
    "         print(f\"Searching for music instrument files in: {test_submission_path}\")\n",
    "         for root, dirs, files in os.walk(test_submission_path):\n",
    "             for name in files:\n",
    "                 if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                      # Optional: further filter by keywords if needed\n",
    "                     if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                          music_instrument_files.append(os.path.join(root, name))\n",
    "         print(f\"Found {len(music_instrument_files)} music instrument files in test submission.\")\n",
    "    else:\n",
    "         print(f\"Warning: Test submission directory not found at {test_submission_path}\")\n",
    "\n",
    "\n",
    "    # Use the collected music instrument files for feature extraction\n",
    "    files_to_process_music = music_instrument_files\n",
    "\n",
    "    # Prepare lists to store extracted features for training\n",
    "    X_train_music_features = [] # For musical input (MFCCs only)\n",
    "    X_train_audio_conditioning_features = [] # For audio conditioning input (MFCC + Chroma)\n",
    "\n",
    "\n",
    "    print(\"\\nExtracting musical features (MFCCs) and audio conditioning features (MFCC+Chroma) from music instrument files...\")\n",
    "\n",
    "    # Process a subset of files for faster testing, or all files for full training\n",
    "    # process_limit = 100 # Set a limit for testing\n",
    "    # files_to_process_subset = files_to_process_music[:process_limit]\n",
    "    files_to_process_subset = files_to_process_music # Process all files\n",
    "\n",
    "\n",
    "    for i, file_path in enumerate(files_to_process_subset):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processing file {i + 1}/{len(files_to_process_subset)}\")\n",
    "\n",
    "        # Extract MFCC features for the musical input/output\n",
    "        mfccs_only = extract_mfcc_features_only(file_path, max_padding, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract combined features for the audio conditioning input\n",
    "        combined_features = extract_features(file_path, max_padding, n_mfcc=n_mfcc, n_chroma=n_chroma)\n",
    "\n",
    "\n",
    "        if mfccs_only is not None and combined_features is not None:\n",
    "            X_train_music_features.append(mfccs_only)\n",
    "            X_train_audio_conditioning_features.append(combined_features)\n",
    "\n",
    "\n",
    "    # Convert extracted features to numpy arrays (keeping as numpy for TensorFlow training setup initially)\n",
    "    X_train_music_features = np.array(X_train_music_features)\n",
    "    X_train_audio_conditioning_features = np.array(X_train_audio_conditioning_features)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    # Ensure that the split is consistent for both sets of features\n",
    "    if len(X_train_music_features) > 0:\n",
    "        X_train_music, X_val_music, X_train_audio_cond, X_val_audio_cond = train_test_split(\n",
    "            X_train_music_features, X_train_audio_conditioning_features, test_size=0.2, random_state=42\n",
    "        )\n",
    "    else:\n",
    "        print(\"No features were successfully extracted. Cannot split data.\")\n",
    "        # Initialize empty arrays to prevent errors in subsequent steps\n",
    "        X_train_music = np.array([])\n",
    "        X_val_music = np.array([])\n",
    "        X_train_audio_cond = np.array([])\n",
    "        X_val_audio_cond = np.array([])\n",
    "\n",
    "\n",
    "    print(\"\\nFeature extraction and data splitting complete.\")\n",
    "    print(f\"Shape of training musical features (MFCCs): {X_train_music.shape}\")\n",
    "    print(f\"Shape of validation musical features (MFCCs): {X_val_music.shape}\")\n",
    "    print(f\"Shape of training audio conditioning features (MFCC+Chroma): {X_train_audio_cond.shape}\")\n",
    "    print(f\"Shape of validation audio conditioning features (MFCC+Chroma): {X_val_audio_cond.shape}\")\n",
    "\n",
    "    # Create dummy text conditioning data for training\n",
    "    # This data is not based on actual text input yet, just for model training setup\n",
    "    # It should match the expected shape of the text_conditioning_input layer\n",
    "    dummy_text_conditioning_train = np.random.rand(X_train_music.shape[0], text_conditioning_dim)\n",
    "    dummy_text_conditioning_val = np.random.rand(X_val_music.shape[0], text_conditioning_dim)\n",
    "\n",
    "\n",
    "    print(f\"\\nShape of dummy training text conditioning data: {dummy_text_conditioning_train.shape}\")\n",
    "    print(f\"Shape of dummy validation text conditioning data: {dummy_text_conditioning_val.shape}\")\n",
    "\n",
    "\n",
    "    # --- Train the Model (PyTorch) ---\n",
    "    # Check if training data is available before training\n",
    "    if 'X_train_music' in globals() and X_train_music.shape[0] > 0:\n",
    "        # Convert NumPy data to PyTorch tensors and move to device\n",
    "        X_train_music_tensor = torch.FloatTensor(X_train_music).to(device)\n",
    "        X_val_music_tensor = torch.FloatTensor(X_val_music).to(device)\n",
    "        X_train_audio_cond_tensor = torch.FloatTensor(X_train_audio_cond).to(device)\n",
    "        X_val_audio_cond_tensor = torch.FloatTensor(X_val_audio_cond).to(device)\n",
    "        dummy_text_conditioning_train_tensor = torch.FloatTensor(dummy_text_conditioning_train).to(device)\n",
    "        dummy_text_conditioning_val_tensor = torch.FloatTensor(dummy_text_conditioning_val).to(device)\n",
    "\n",
    "        # Create PyTorch DataLoaders\n",
    "        from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_music_tensor, dummy_text_conditioning_train_tensor, X_train_audio_cond_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        val_dataset = TensorDataset(X_val_music_tensor, dummy_text_conditioning_val_tensor, X_val_audio_cond_tensor)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32) # No need to shuffle validation data\n",
    "\n",
    "        print(\"\\nData converted to PyTorch tensors and DataLoaders created for training.\")\n",
    "\n",
    "        # Define Loss Function and Optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer_pytorch = torch.optim.Adam(model_gen_conditioned_pytorch.parameters(), lr=0.001) # You can adjust the learning rate\n",
    "\n",
    "\n",
    "        num_epochs = 50 # You can adjust the number of epochs\n",
    "        print(f\"\\nStarting PyTorch model training for {num_epochs} epochs...\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model_gen_conditioned_pytorch.train() # Set the model to training mode\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for i, (musical_inputs, text_cond_inputs, audio_cond_inputs) in enumerate(train_loader):\n",
    "                # Zero the parameter gradients\n",
    "                optimizer_pytorch.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model_gen_conditioned_pytorch(musical_inputs, text_cond_inputs, audio_cond_inputs)\n",
    "\n",
    "                # Calculate loss\n",
    "                # The target for musical_inputs is the same as the input in this autoencoder-like setup\n",
    "                loss = criterion(outputs, musical_inputs)\n",
    "\n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer_pytorch.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Print statistics\n",
    "                if (i + 1) % 10 == 0: # Print every 10 batches\n",
    "                    print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}] finished. Average Training Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "            # Validation step\n",
    "            model_gen_conditioned_pytorch.eval() # Set the model to evaluation mode\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad(): # Disable gradient calculation for validation\n",
    "                for musical_inputs_val, text_cond_inputs_val, audio_cond_inputs_val in val_loader:\n",
    "                    outputs_val = model_gen_conditioned_pytorch(musical_inputs_val, text_cond_inputs_val, audio_cond_inputs_val)\n",
    "                    loss_val = criterion(outputs_val, musical_inputs_val)\n",
    "                    val_loss += loss_val.item()\n",
    "\n",
    "            print(f'Validation Loss: {val_loss / len(val_loader):.4f}')\n",
    "\n",
    "        print(\"\\nPyTorch model training complete.\")\n",
    "    else:\n",
    "        print(\"Training data is empty. Skipping PyTorch model training.\")\n",
    "\n",
    "\n",
    "# --- Function to generate musical features iteratively (PyTorch) ---\n",
    "def generate_musical_features_iterative_pytorch(model, start_sequence, text_conditioning_input, audio_conditioning_input, total_generation_steps):\n",
    "    \"\"\"Generates new MFCC sequences step-by-step using the trained PyTorch conditional model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained PyTorch conditional generation model.\n",
    "        start_sequence: A PyTorch tensor representing the initial musical sequence\n",
    "                        (shape: (1, sequence_length, musical_feature_dim)).\n",
    "        text_conditioning_input: The text conditioning input (shape: (1, text_conditioning_dim)).\n",
    "        audio_conditioning_input: The audio conditioning input (sequence of features,\n",
    "                                   shape: (1, sequence_length, total_audio_features_dim)).\n",
    "                                   This is the *full* audio conditioning sequence.\n",
    "        total_generation_steps: The total number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated musical MFCC sequence (shape: (total_generation_steps, musical_feature_dim)),\n",
    "        or None if generation fails.\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating musical feature sequence iteratively for {total_generation_steps} steps using PyTorch model...\")\n",
    "\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    generated_sequence = [] # List to store generated time steps\n",
    "    current_musical_sequence = start_sequence.clone().to(device) # Start with the initial sequence on the correct device\n",
    "    sequence_length = start_sequence.shape[1] # Expected input sequence length for the model\n",
    "    musical_feature_dim = start_sequence.shape[2] # Dimension of musical features\n",
    "\n",
    "    # Ensure conditioning inputs are on the correct device\n",
    "    text_conditioning_input = text_conditioning_input.to(device)\n",
    "    audio_conditioning_input = audio_conditioning_input.to(device)\n",
    "\n",
    "\n",
    "    # Ensure audio_conditioning_input has a batch dimension and correct sequence length\n",
    "    if audio_conditioning_input.ndim != 3 or audio_conditioning_input.shape[1] != sequence_length:\n",
    "         print(\"Error: audio_conditioning_input must have shape (1, sequence_length, total_audio_features_dim).\")\n",
    "         return None\n",
    "\n",
    "    # Ensure text_conditioning_input has a batch dimension and correct dimension\n",
    "    if text_conditioning_input.ndim != 2 or text_conditioning_input.shape[1] != text_conditioning_dim:\n",
    "        print(\"Error: text_conditioning_input must have shape (1, text_conditioning_dim).\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation during generation\n",
    "        for i in range(total_generation_steps):\n",
    "            # The model expects [musical_input, text_conditioning_input, audio_conditioning_input]\n",
    "            model_inputs = [current_musical_sequence, text_conditioning_input, audio_conditioning_input]\n",
    "\n",
    "            # Predict the next time step of musical features\n",
    "            try:\n",
    "                # The model predicts a sequence of length `musical_sequence_length`.\n",
    "                # We are interested in the prediction for the *next* time step.\n",
    "                predicted_sequence = model(*model_inputs) # Pass inputs unpacking the list\n",
    "                # predicted_sequence shape: (1, musical_sequence_length, musical_feature_dim)\n",
    "\n",
    "                # Take the last time step of the prediction\n",
    "                predicted_next_step = predicted_sequence[:, -1:, :] # Shape (1, 1, musical_feature_dim)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during model prediction at step {i}: {e}\")\n",
    "                return None\n",
    "\n",
    "\n",
    "            # Append the predicted time step to the generated sequence (as numpy array)\n",
    "            generated_sequence.append(predicted_next_step.squeeze().cpu().numpy()) # Move to CPU and convert to numpy\n",
    "\n",
    "            # Update the current musical sequence by removing the oldest time step and adding the new one\n",
    "            # This sliding window approach is standard for iterative sequence generation.\n",
    "            # Ensure the sequence length remains constant\n",
    "            current_musical_sequence = torch.cat((current_musical_sequence[:, 1:, :], predicted_next_step), dim=1)\n",
    "\n",
    "            # Print progress\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Generated {i + 1}/{total_generation_steps} steps.\")\n",
    "\n",
    "\n",
    "    # Convert the list of generated steps into a numpy array\n",
    "    generated_sequence_np = np.array(generated_sequence) # Shape (total_generation_steps, musical_feature_dim)\n",
    "\n",
    "    print(\"\\nIterative musical feature generation complete.\")\n",
    "    print(f\"Shape of the generated musical features: {generated_sequence_np.shape}\")\n",
    "\n",
    "    # Add inspection of generated features\n",
    "    print(\"\\nGenerated Musical Features Statistics:\")\n",
    "    print(f\"Shape: {generated_sequence_np.shape}\")\n",
    "    print(f\"Mean: {np.mean(generated_sequence_np)}\")\n",
    "    print(f\"Min: {np.min(generated_sequence_np)}\")\n",
    "    print(f\"Max: {np.max(generated_sequence_np)}\")\n",
    "    print(f\"Standard Deviation: {np.std(generated_sequence_np)}\")\n",
    "\n",
    "    # Save generated features to a file for inspection\n",
    "    features_output_filename = 'generated_musical_features_iterative_pytorch.npy'\n",
    "    np.save(features_output_filename, generated_sequence_np)\n",
    "    print(f\"Generated musical features saved to {features_output_filename}\")\n",
    "\n",
    "\n",
    "    return generated_sequence_np\n",
    "\n",
    "\n",
    "# --- Function to convert musical features to audio and save ---\n",
    "def convert_features_to_audio(musical_features, sr=22050, output_filename_wav='generated_music.wav', output_filename_mp3='generated_music.mp3'):\n",
    "    \"\"\"Converts musical features (MFCCs) to an audio file and saves it.\n",
    "\n",
    "    Args:\n",
    "        musical_features: A numpy array containing the generated musical MFCC sequence (shape: (time_steps, n_mfcc)).\n",
    "        sr: Sampling rate for audio output.\n",
    "        output_filename_wav: Filename for the output WAV file.\n",
    "        output_filename_mp3: Filename for the output MP3 file.\n",
    "\n",
    "    Returns:\n",
    "        The path to the generated audio file (MP3 or WAV), or None if conversion/saving fails.\n",
    "    \"\"\"\n",
    "    if musical_features is None or musical_features.shape[0] == 0:\n",
    "        print(\"No musical features provided for audio conversion.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\nConverting generated musical features to audio...\")\n",
    "    try:\n",
    "        # Assuming the features are MFCCs. Transpose back to (n_mfcc, time_steps) for inverse transform\n",
    "        audio_time_series = librosa.feature.inverse.mfcc_to_audio(musical_features.T, sr=sr)\n",
    "\n",
    "        # Add inspection of generated audio time series\n",
    "        print(\"\\nGenerated Audio Time Series Statistics:\")\n",
    "        print(f\"Shape: {audio_time_series.shape}\")\n",
    "        print(f\"Mean: {np.mean(audio_time_series)}\")\n",
    "        print(f\"Min: {np.min(audio_time_series)}\")\n",
    "        print(f\"Max: {np.max(audio_time_series)}\")\n",
    "        print(f\"Standard Deviation: {np.std(audio_time_series)}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MFCC to audio conversion: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Save the generated audio to a WAV file first\n",
    "    print(f\"Saving generated audio to {output_filename_wav}...\")\n",
    "    try:\n",
    "        sf.write(output_filename_wav, audio_time_series, sr)\n",
    "        print(f\"Generated audio saved successfully to {output_filename_wav}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving WAV file: {e}\")\n",
    "        return None\n",
    "\n",
    "    generated_audio_file = output_filename_wav # Default to WAV\n",
    "\n",
    "    # Attempt to convert the WAV file to MP3\n",
    "    print(f\"Attempting to convert {output_filename_wav} to {output_filename_mp3}...\")\n",
    "    try:\n",
    "        audio = AudioSegment.from_wav(output_filename_wav)\n",
    "        audio.export(output_filename_mp3, format=\"mp3\")\n",
    "        print(f\"Conversion to MP3 successful. Output saved to {output_filename_mp3}\")\n",
    "        generated_audio_file = output_filename_mp3 # Update to MP3 if successful\n",
    "        # Optionally, remove the intermediate WAV file\n",
    "        # os.remove(output_filename_wav)\n",
    "        # print(f\"Removed intermediate WAV file: {output_filename_wav}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MP3 conversion: {e}. Proceeding with WAV file.\")\n",
    "        # generated_audio_file remains the WAV file path\n",
    "\n",
    "    return generated_audio_file\n",
    "\n",
    "\n",
    "# --- Function to convert generated features to symbolic music (music21) (Step 4 in Plan) ---\n",
    "def convert_features_to_music21(musical_features_np, sr=22050, hop_length=512):\n",
    "    \"\"\"Converts generated musical features (MFCCs) to a music21 stream.\n",
    "\n",
    "    This is a placeholder function. Converting MFCCs directly to\n",
    "    meaningful symbolic music (notes, chords, rhythm) is a complex task\n",
    "    and requires more advanced techniques (e.g., a separate model trained\n",
    "    to map features to musical events).\n",
    "\n",
    "    Args:\n",
    "        musical_features_np: A numpy array containing the generated musical MFCC sequence (shape: (time_steps, n_mfcc)).\n",
    "        sr: Sampling rate.\n",
    "        hop_length: Hop length used during feature extraction.\n",
    "\n",
    "    Returns:\n",
    "        A music21 stream, or None if conversion is not implemented or fails.\n",
    "    \"\"\"\n",
    "    print(\"\\nAttempting to convert musical features to music21 stream...\")\n",
    "    print(\"Note: Direct conversion of MFCCs to symbolic music is complex and requires further development.\")\n",
    "    # Placeholder implementation: Create a simple stream for demonstration\n",
    "    from music21 import stream, note, duration, tempo, pitch\n",
    "\n",
    "    s = stream.Stream()\n",
    "\n",
    "    # Add a tempo indication (optional, but good practice)\n",
    "    # Estimate tempo from features or use a default\n",
    "    # For simplicity, using a default tempo\n",
    "    s.append(tempo.MetronomeMark(number=120))\n",
    "\n",
    "    # Simple approach: Map a feature value (e.g., the first MFCC coefficient) to pitch\n",
    "    # and use a fixed duration based on the hop length. This is a very basic mapping.\n",
    "    time_per_step = hop_length / sr # Duration of each feature vector in seconds\n",
    "\n",
    "    # Define a mapping range for MFCC values to MIDI pitches (adjust as needed)\n",
    "    # Find min/max of the first MFCC coefficient in the generated sequence\n",
    "    if musical_features_np.shape[0] > 0:\n",
    "        min_mfcc1 = np.min(musical_features_np[:, 0])\n",
    "        max_mfcc1 = np.max(musical_features_np[:, 0])\n",
    "        midi_pitch_range = 60 # Map to a range of 60 MIDI pitches (e.g., from 30 to 90)\n",
    "        min_midi_pitch = 30\n",
    "\n",
    "        for i, features in enumerate(musical_features_np):\n",
    "            # Use the first MFCC coefficient for pitch mapping\n",
    "            mfcc1 = features[0]\n",
    "\n",
    "            # Normalize the MFCC value to the range [0, 1]\n",
    "            if (max_mfcc1 - min_mfcc1) > 0:\n",
    "                normalized_mfcc1 = (mfcc1 - min_mfcc1) / (max_mfcc1 - min_mfcc1)\n",
    "            else:\n",
    "                normalized_mfcc1 = 0.5 # Default to middle if range is zero\n",
    "\n",
    "            # Map the normalized value to a MIDI pitch\n",
    "            midi_pitch_val = int(min_midi_pitch + normalized_mfcc1 * midi_pitch_range)\n",
    "            midi_pitch_val = max(0, min(127, midi_pitch_val)) # Clamp to valid MIDI range\n",
    "\n",
    "            try:\n",
    "                 # Create a note\n",
    "                 n = note.Note(midi_pitch_val)\n",
    "                 # Set the duration based on the time per feature step\n",
    "                 # Using a fixed duration for simplicity, could be refined\n",
    "                 n.duration = duration.Duration(time_per_step)\n",
    "\n",
    "                 s.append(n)\n",
    "            except Exception as note_error:\n",
    "                 print(f\"Error creating music21 note at step {i}: {note_error}\")\n",
    "                 # Optionally, append a rest or skip this step\n",
    "                 s.append(note.Rest(duration=duration.Duration(time_per_step)))\n",
    "\n",
    "\n",
    "            # Avoid creating an excessively large stream for demonstration\n",
    "            if i > 200: # Limit the number of elements for a quicker conversion/display\n",
    "                 break\n",
    "    else:\n",
    "         print(\"No generated features to convert to music21.\")\n",
    "         return None\n",
    "\n",
    "\n",
    "    print(f\"Created a music21 stream with {len(s.elements)} elements.\") # Use .elements to count top-level elements\n",
    "\n",
    "    return s\n",
    "\n",
    "# --- Function to save and display music21 stream (Step 5 in Plan) ---\n",
    "def save_and_display_music21_stream(music21_stream, output_filename_midi='generated_music.mid'):\n",
    "    \"\"\"Saves a music21 stream as a MIDI file and displays its text representation.\"\"\"\n",
    "    if music21_stream is None:\n",
    "        print(\"No music21 stream provided to save or display.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nSaving music21 stream to {output_filename_midi}...\")\n",
    "    try:\n",
    "        music21_stream.write('midi', fp=output_filename_midi)\n",
    "        print(f\"Music21 stream saved successfully to {output_filename_midi}\")\n",
    "\n",
    "        # Display the stream as text\n",
    "        print(\"\\nGenerated Music21 Stream (text representation):\")\n",
    "        print(music21_stream.show('text'))\n",
    "\n",
    "        return output_filename_midi # Return the path to the saved MIDI file\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving or displaying music21 stream: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Main music generation function (uses iterative generation with PyTorch model) ---\n",
    "def generate_music_from_input_v4(model, user_text_input=None, user_audio_input_path=None, input_type='text_prompt', target_audio_duration_sec=30, sr=22050, max_padding=174, n_mfcc=20, n_chroma=12, text_conditioning_dim=128):\n",
    "    \"\"\"Generates music based on user input (text and/or audio) using the PyTorch model and iterative generation.\n",
    "\n",
    "    Args:\n",
    "        model: The trained PyTorch conditional music generation model.\n",
    "        user_text_input: The user's text input (string for lyrics or prompt).\n",
    "        user_audio_input_path: Path to the user's audio file.\n",
    "        input_type: Type of text input ('text_lyrics', 'text_prompt'). 'audio_only' is also possible if no text is provided.\n",
    "        target_audio_duration_sec: The desired duration of the generated audio in seconds.\n",
    "        sr: Sampling rate for audio output.\n",
    "        max_padding: Target sequence length for padding/truncating features (used for initial sequence).\n",
    "        n_mfcc: Number of MFCCs extracted.\n",
    "        n_chroma: Number of chroma bins extracted.\n",
    "        text_conditioning_dim: Dimension of the text embedding.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - The path to the generated audio file (MP3 or WAV),\n",
    "        - The generated lyrics (string),\n",
    "        - The generated musical features (numpy array),\n",
    "        or (None, None, None) if an error occurred.\n",
    "    \"\"\"\n",
    "    text_conditioning_input_for_model = None\n",
    "    audio_conditioning_input_for_model = None\n",
    "    generated_lyrics = None\n",
    "\n",
    "    # Process text input if provided\n",
    "    if user_text_input:\n",
    "        is_prompt = (input_type == 'text_prompt')\n",
    "        conditioning_input_for_model, generated_lyrics = process_text_input(user_text_input, text_conditioning_dim, is_prompt=is_prompt)\n",
    "        if conditioning_input_for_model is None:\n",
    "            print(\"Error processing text input for conditioning.\")\n",
    "            return None, None, None\n",
    "        text_conditioning_input_for_model = conditioning_input_for_model\n",
    "\n",
    "    else:\n",
    "        print(\"No text input provided. Using dummy text conditioning.\")\n",
    "        # Create dummy text conditioning input as a PyTorch tensor and move to device\n",
    "        text_conditioning_input_for_model = torch.zeros((1, text_conditioning_dim)).to(device)\n",
    "\n",
    "\n",
    "    # Process audio input if provided\n",
    "    if user_audio_input_path and os.path.exists(user_audio_input_path):\n",
    "        print(\"Processing user audio input for conditioning.\")\n",
    "        # For iterative generation, we still need the full audio conditioning sequence\n",
    "        audio_conditioning_input_for_model = process_user_audio_for_conditioning(user_audio_input_path, max_padding, n_mfcc, n_chroma)\n",
    "        if audio_conditioning_input_for_model is None:\n",
    "            print(\"Error processing user audio input for conditioning.\")\n",
    "            return None, None, None\n",
    "    else:\n",
    "        print(\"No user audio file provided or found. Using dummy audio conditioning.\")\n",
    "        # Create dummy audio conditioning input as a PyTorch tensor and move to device\n",
    "        audio_conditioning_input_for_model = torch.zeros((1, max_padding, total_audio_features_dim)).to(device)\n",
    "\n",
    "\n",
    "    # Ensure both conditioning inputs are available\n",
    "    if text_conditioning_input_for_model is None or audio_conditioning_input_for_model is None:\n",
    "         print(\"Missing conditioning input(s). Cannot proceed.\")\n",
    "         return None, None, None\n",
    "\n",
    "\n",
    "    # Generate a starting musical sequence (e.g., take a random sample from the training data)\n",
    "    # This starting sequence will be the initial input for the iterative generation.\n",
    "    # Ensure X_train_music is available from data loading\n",
    "    if 'X_train_music' not in globals() or X_train_music.shape[0] == 0:\n",
    "         print(\"Training data (X_train_music) is not available or empty. Cannot generate a starting sequence.\")\n",
    "         return None, None, None\n",
    "\n",
    "    # Select a random starting sequence and convert to PyTorch tensor, add batch dim, move to device\n",
    "    start_index = np.random.randint(0, X_train_music.shape[0])\n",
    "    start_sequence_np = X_train_music[start_index:start_index+1, :, :]\n",
    "    start_sequence_tensor = torch.FloatTensor(start_sequence_np).to(device)\n",
    "\n",
    "\n",
    "    # Calculate the total number of time steps to generate\n",
    "    # Each time step in the feature sequence corresponds to hop_length samples (default 512 in librosa mfcc)\n",
    "    # Duration of one feature time step = hop_length / sr\n",
    "    # Total time steps needed = target_audio_duration_sec / (hop_length / sr) = target_audio_duration_sec * sr / hop_length\n",
    "    # Assuming default hop_length of 512\n",
    "    hop_length = 512\n",
    "    total_generation_steps = int(target_audio_duration_sec * sr / hop_length)\n",
    "    print(f\"Targeting {target_audio_duration_sec} seconds, which requires approximately {total_generation_steps} generation steps.\")\n",
    "\n",
    "\n",
    "    # Generate new musical features iteratively using the trained PyTorch conditional model\n",
    "    generated_musical_features_np = generate_musical_features_iterative_pytorch(\n",
    "        model,\n",
    "        start_sequence_tensor,\n",
    "        text_conditioning_input_for_model,\n",
    "        audio_conditioning_input_for_model,\n",
    "        total_generation_steps\n",
    "    )\n",
    "\n",
    "\n",
    "    if generated_musical_features_np is None:\n",
    "        print(\"Feature generation failed.\")\n",
    "        return None, generated_lyrics, None # Return None for audio path and features\n",
    "\n",
    "\n",
    "    # Convert the generated musical features to audio and save\n",
    "    generated_audio_file = convert_features_to_audio(generated_musical_features_np, sr=sr)\n",
    "\n",
    "\n",
    "    # Optionally, save the generated lyrics to a text file if lyrics were generated\n",
    "    lyrics_output_filename = 'generated_lyrics.txt'\n",
    "    if generated_lyrics:\n",
    "        try:\n",
    "            with open(lyrics_output_filename, 'w') as f:\n",
    "                f.write(generated_lyrics)\n",
    "            print(f\"Generated lyrics saved to {lyrics_output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving generated lyrics to file: {e}\")\n",
    "\n",
    "\n",
    "    return generated_audio_file, generated_lyrics, generated_musical_features_np # Return features\n",
    "\n",
    "\n",
    "# --- Example Usage (requires trained PyTorch model and data) ---\n",
    "# Assume model_gen_conditioned_pytorch is trained and X_train_music is defined.\n",
    "\n",
    "# Example: Generate music from a text prompt and user audio, aiming for 30 seconds\n",
    "user_text_prompt = \"Generate a piece of background music.\"\n",
    "# IMPORTANT: Replace '/content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg' with the actual path to your uploaded audio file\n",
    "# Ensure the audio file exists before running\n",
    "user_audio_input_path = '/content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg'\n",
    "if not os.path.exists(user_audio_input_path):\n",
    "    print(f\"Error: User audio file not found at {user_audio_input_path}. Please upload the file.\")\n",
    "    user_audio_input_path = None # Set to None if file doesn't exist\n",
    "\n",
    "target_duration_seconds = 30\n",
    "\n",
    "if user_audio_input_path is not None and 'model_gen_conditioned_pytorch' in globals() and 'X_train_music' in globals() and X_train_music.shape[0] > 0:\n",
    "    print(f\"\\nAttempting to generate music from text prompt: '{user_text_prompt}' using audio file at {user_audio_input_path}, targeting {target_duration_seconds} seconds.\")\n",
    "    generated_music_file, generated_lyrics_text, generated_features_np = generate_music_from_input_v4(\n",
    "        model_gen_conditioned_pytorch, # Use the PyTorch model\n",
    "        user_text_input=user_text_prompt,\n",
    "        user_audio_input_path=user_audio_input_path,\n",
    "        input_type='text_prompt',\n",
    "        target_audio_duration_sec=target_duration_seconds,\n",
    "        sr=sr,\n",
    "        max_padding=max_padding,\n",
    "        n_mfcc=n_mfcc,\n",
    "        n_chroma=n_chroma,\n",
    "        text_conditioning_dim=text_conditioning_dim\n",
    "    )\n",
    "\n",
    "\n",
    "    if generated_music_file:\n",
    "        print(f\"\\nGenerated music file: {generated_music_file}\")\n",
    "        if generated_lyrics_text:\n",
    "            print(f\"Generated lyrics:\\n{generated_lyrics_text}\")\n",
    "        # Display links to the generated files\n",
    "        print(\"\\nDownload the generated music file:\")\n",
    "        display(FileLink(generated_music_file))\n",
    "        if generated_lyrics_text: # Only display lyrics link if lyrics were generated\n",
    "            print(\"Download the generated lyrics file:\")\n",
    "            display(FileLink('generated_lyrics.txt'))\n",
    "\n",
    "        # Play the generated audio\n",
    "        print(f\"\\nPlaying generated audio file: {generated_music_file}\")\n",
    "        display(Audio(generated_music_file))\n",
    "\n",
    "        # Step 4: Convert generated features to symbolic music (music21)\n",
    "        if generated_features_np is not None:\n",
    "            music21_stream = convert_features_to_music21(generated_features_np, sr=sr, hop_length=512) # Pass hop_length\n",
    "\n",
    "            # Step 5: Save and display symbolic music (music21)\n",
    "            if music21_stream:\n",
    "                generated_midi_file = save_and_display_music21_stream(music21_stream)\n",
    "                if generated_midi_file:\n",
    "                    print(\"\\nDownload the generated MIDI file:\")\n",
    "                    display(FileLink(generated_midi_file))\n",
    "        else:\n",
    "            print(\"\\nNo generated features available for music21 conversion.\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"\\nMusic generation failed.\")\n",
    "else:\n",
    "    if user_audio_input_path is None:\n",
    "         print(\"\\nSkipping music generation as user audio file was not found.\")\n",
    "    else:\n",
    "         print(\"\\nSkipping music generation. PyTorch model or training data not available.\")\n",
    "\"\"\"\n",
    "    get_ipython().run_cell(raw_cell=regenerated_cell_content, cell_id='a1b11bff')\n",
    "\n",
    "print(\"\\nProceeding with music generation using the updated function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "31353a6b",
    "outputId": "aaff2a37-9554-4b24-eaf1-8a877c40e77e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generation pipeline (distilgpt2) initialized.\n",
      "Using device: cpu\n",
      "\n",
      "PyTorch conditional music generation model architecture defined.\n",
      "MusicGenerator(\n",
      "  (gru1): GRU(180, 128, batch_first=True, bidirectional=True)\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (gru2): GRU(256, 128, batch_first=True, bidirectional=True)\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  (linear): Linear(in_features=256, out_features=20, bias=True)\n",
      ")\n",
      "Downloading the dataset to ensure path is available...\n",
      "Using Colab cache for faster access to the 'musical-instruments-sound-dataset' dataset.\n",
      "Dataset path: /kaggle/input/musical-instruments-sound-dataset\n",
      "Searching for music instrument files in: /kaggle/input/musical-instruments-sound-dataset/Train_submission/Train_submission\n",
      "Found 1965 music instrument files in training submission.\n",
      "Searching for music instrument files in: /kaggle/input/musical-instruments-sound-dataset/Test_submission/Test_submission\n",
      "Found 2029 music instrument files in test submission.\n",
      "\n",
      "Extracting musical features (MFCCs) and audio conditioning features (MFCC+Chroma) from music instrument files...\n",
      "Processing file 100/2029\n",
      "Processing file 200/2029\n",
      "Processing file 300/2029\n",
      "Processing file 400/2029\n",
      "Processing file 500/2029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 600/2029\n",
      "Processing file 700/2029\n",
      "Processing file 800/2029\n",
      "Processing file 900/2029\n",
      "Processing file 1000/2029\n",
      "Processing file 1100/2029\n",
      "Processing file 1200/2029\n",
      "Processing file 1300/2029\n",
      "Processing file 1400/2029\n",
      "Processing file 1500/2029\n",
      "Processing file 1600/2029\n",
      "Processing file 1700/2029\n",
      "Processing file 1800/2029\n",
      "Processing file 1900/2029\n",
      "Processing file 2000/2029\n",
      "\n",
      "Feature extraction and data splitting complete.\n",
      "Shape of training musical features (MFCCs): (1623, 174, 20)\n",
      "Shape of validation musical features (MFCCs): (406, 174, 20)\n",
      "Shape of training audio conditioning features (MFCC+Chroma): (1623, 174, 32)\n",
      "Shape of validation audio conditioning features (MFCC+Chroma): (406, 174, 32)\n",
      "\n",
      "Shape of dummy training text conditioning data: (1623, 128)\n",
      "Shape of dummy validation text conditioning data: (406, 128)\n",
      "\n",
      "Data converted to PyTorch tensors and DataLoaders created for training.\n",
      "\n",
      "Starting PyTorch model training for 50 epochs...\n",
      "Epoch [1/50], Step [10/51], Loss: 13449.7012\n",
      "Epoch [1/50], Step [20/51], Loss: 14592.0928\n",
      "Epoch [1/50], Step [30/51], Loss: 14678.7480\n",
      "Epoch [1/50], Step [40/51], Loss: 15509.6582\n",
      "Epoch [1/50], Step [50/51], Loss: 13662.3594\n",
      "Epoch [1/50] finished. Average Training Loss: 13847.3875\n",
      "Validation Loss: 13435.3794\n",
      "Epoch [2/50], Step [10/51], Loss: 15376.0469\n",
      "Epoch [2/50], Step [20/51], Loss: 13039.8906\n",
      "Epoch [2/50], Step [30/51], Loss: 11712.8271\n",
      "Epoch [2/50], Step [40/51], Loss: 14018.5850\n",
      "Epoch [2/50], Step [50/51], Loss: 11421.7549\n",
      "Epoch [2/50] finished. Average Training Loss: 13084.5676\n",
      "Validation Loss: 12730.7169\n",
      "Epoch [3/50], Step [10/51], Loss: 11902.7031\n",
      "Epoch [3/50], Step [20/51], Loss: 11400.7451\n",
      "Epoch [3/50], Step [30/51], Loss: 12222.6641\n",
      "Epoch [3/50], Step [40/51], Loss: 11578.3047\n",
      "Epoch [3/50], Step [50/51], Loss: 12506.3115\n",
      "Epoch [3/50] finished. Average Training Loss: 12393.2137\n",
      "Validation Loss: 12083.6963\n",
      "Epoch [4/50], Step [10/51], Loss: 10878.8271\n",
      "Epoch [4/50], Step [20/51], Loss: 10210.5752\n",
      "Epoch [4/50], Step [30/51], Loss: 12466.6709\n",
      "Epoch [4/50], Step [40/51], Loss: 11066.1377\n",
      "Epoch [4/50], Step [50/51], Loss: 9590.2969\n",
      "Epoch [4/50] finished. Average Training Loss: 11774.7674\n",
      "Validation Loss: 11483.3280\n",
      "Epoch [5/50], Step [10/51], Loss: 11460.9219\n",
      "Epoch [5/50], Step [20/51], Loss: 12167.3115\n",
      "Epoch [5/50], Step [30/51], Loss: 10098.0771\n",
      "Epoch [5/50], Step [40/51], Loss: 8957.3691\n",
      "Epoch [5/50], Step [50/51], Loss: 13089.6533\n",
      "Epoch [5/50] finished. Average Training Loss: 11183.0627\n",
      "Validation Loss: 10922.8018\n",
      "Epoch [6/50], Step [10/51], Loss: 11441.8545\n",
      "Epoch [6/50], Step [20/51], Loss: 11103.1025\n",
      "Epoch [6/50], Step [30/51], Loss: 9422.7441\n",
      "Epoch [6/50], Step [40/51], Loss: 10299.8389\n",
      "Epoch [6/50], Step [50/51], Loss: 9226.9805\n",
      "Epoch [6/50] finished. Average Training Loss: 10649.6637\n",
      "Validation Loss: 10394.0442\n",
      "Epoch [7/50], Step [10/51], Loss: 8943.8193\n",
      "Epoch [7/50], Step [20/51], Loss: 9475.4814\n",
      "Epoch [7/50], Step [30/51], Loss: 11358.6494\n",
      "Epoch [7/50], Step [40/51], Loss: 9495.4717\n",
      "Epoch [7/50], Step [50/51], Loss: 9937.2188\n",
      "Epoch [7/50] finished. Average Training Loss: 10137.8018\n",
      "Validation Loss: 9898.0624\n",
      "Epoch [8/50], Step [10/51], Loss: 9964.6797\n",
      "Epoch [8/50], Step [20/51], Loss: 10378.8369\n",
      "Epoch [8/50], Step [30/51], Loss: 10275.1406\n",
      "Epoch [8/50], Step [40/51], Loss: 7424.1484\n",
      "Epoch [8/50], Step [50/51], Loss: 8544.8994\n",
      "Epoch [8/50] finished. Average Training Loss: 9650.3503\n",
      "Validation Loss: 9423.8741\n",
      "Epoch [9/50], Step [10/51], Loss: 9458.5889\n",
      "Epoch [9/50], Step [20/51], Loss: 9504.1484\n",
      "Epoch [9/50], Step [30/51], Loss: 8389.4678\n",
      "Epoch [9/50], Step [40/51], Loss: 9751.5508\n",
      "Epoch [9/50], Step [50/51], Loss: 9460.4590\n",
      "Epoch [9/50] finished. Average Training Loss: 9188.3754\n",
      "Validation Loss: 8976.2770\n",
      "Epoch [10/50], Step [10/51], Loss: 9922.9541\n",
      "Epoch [10/50], Step [20/51], Loss: 7711.3477\n",
      "Epoch [10/50], Step [30/51], Loss: 8622.2676\n",
      "Epoch [10/50], Step [40/51], Loss: 9318.0156\n",
      "Epoch [10/50], Step [50/51], Loss: 8996.6562\n",
      "Epoch [10/50] finished. Average Training Loss: 8754.6368\n",
      "Validation Loss: 8552.5275\n",
      "Epoch [11/50], Step [10/51], Loss: 8799.3936\n",
      "Epoch [11/50], Step [20/51], Loss: 6711.8315\n",
      "Epoch [11/50], Step [30/51], Loss: 7661.2134\n",
      "Epoch [11/50], Step [40/51], Loss: 8167.2212\n",
      "Epoch [11/50], Step [50/51], Loss: 7921.9277\n",
      "Epoch [11/50] finished. Average Training Loss: 8327.0488\n",
      "Validation Loss: 8146.2528\n",
      "Epoch [12/50], Step [10/51], Loss: 5449.0493\n",
      "Epoch [12/50], Step [20/51], Loss: 7196.1108\n",
      "Epoch [12/50], Step [30/51], Loss: 8661.6270\n",
      "Epoch [12/50], Step [40/51], Loss: 9265.7686\n",
      "Epoch [12/50], Step [50/51], Loss: 7020.6987\n",
      "Epoch [12/50] finished. Average Training Loss: 7928.0971\n",
      "Validation Loss: 7760.9620\n",
      "Epoch [13/50], Step [10/51], Loss: 6972.3872\n",
      "Epoch [13/50], Step [20/51], Loss: 7289.3188\n",
      "Epoch [13/50], Step [30/51], Loss: 7487.0947\n",
      "Epoch [13/50], Step [40/51], Loss: 8857.2949\n",
      "Epoch [13/50], Step [50/51], Loss: 6313.4570\n",
      "Epoch [13/50] finished. Average Training Loss: 7565.2755\n",
      "Validation Loss: 7392.5504\n",
      "Epoch [14/50], Step [10/51], Loss: 8084.0747\n",
      "Epoch [14/50], Step [20/51], Loss: 6659.1353\n",
      "Epoch [14/50], Step [30/51], Loss: 7356.1016\n",
      "Epoch [14/50], Step [40/51], Loss: 7195.2651\n",
      "Epoch [14/50], Step [50/51], Loss: 7550.9502\n",
      "Epoch [14/50] finished. Average Training Loss: 7195.5207\n",
      "Validation Loss: 7040.2240\n",
      "Epoch [15/50], Step [10/51], Loss: 6537.3354\n",
      "Epoch [15/50], Step [20/51], Loss: 6905.2998\n",
      "Epoch [15/50], Step [30/51], Loss: 8600.2686\n",
      "Epoch [15/50], Step [40/51], Loss: 5777.7246\n",
      "Epoch [15/50], Step [50/51], Loss: 7205.0127\n",
      "Epoch [15/50] finished. Average Training Loss: 6847.0417\n",
      "Validation Loss: 6706.4539\n",
      "Epoch [16/50], Step [10/51], Loss: 7297.1812\n",
      "Epoch [16/50], Step [20/51], Loss: 5929.4966\n",
      "Epoch [16/50], Step [30/51], Loss: 6619.7676\n",
      "Epoch [16/50], Step [40/51], Loss: 6619.4961\n",
      "Epoch [16/50], Step [50/51], Loss: 5680.4341\n",
      "Epoch [16/50] finished. Average Training Loss: 6525.2701\n",
      "Validation Loss: 6390.5762\n",
      "Epoch [17/50], Step [10/51], Loss: 7506.2559\n",
      "Epoch [17/50], Step [20/51], Loss: 6553.0845\n",
      "Epoch [17/50], Step [30/51], Loss: 5986.4351\n",
      "Epoch [17/50], Step [40/51], Loss: 5724.9443\n",
      "Epoch [17/50], Step [50/51], Loss: 7299.3745\n",
      "Epoch [17/50] finished. Average Training Loss: 6215.7517\n",
      "Validation Loss: 6087.9468\n",
      "Epoch [18/50], Step [10/51], Loss: 5861.0361\n",
      "Epoch [18/50], Step [20/51], Loss: 5481.2456\n",
      "Epoch [18/50], Step [30/51], Loss: 5301.8252\n",
      "Epoch [18/50], Step [40/51], Loss: 6760.0625\n",
      "Epoch [18/50], Step [50/51], Loss: 5266.5396\n",
      "Epoch [18/50] finished. Average Training Loss: 5914.1472\n",
      "Validation Loss: 5799.7971\n",
      "Epoch [19/50], Step [10/51], Loss: 5833.0757\n",
      "Epoch [19/50], Step [20/51], Loss: 6332.9956\n",
      "Epoch [19/50], Step [30/51], Loss: 4930.5972\n",
      "Epoch [19/50], Step [40/51], Loss: 5333.4971\n",
      "Epoch [19/50], Step [50/51], Loss: 5417.7104\n",
      "Epoch [19/50] finished. Average Training Loss: 5636.5832\n",
      "Validation Loss: 5527.9127\n",
      "Epoch [20/50], Step [10/51], Loss: 4196.6426\n",
      "Epoch [20/50], Step [20/51], Loss: 5553.7554\n",
      "Epoch [20/50], Step [30/51], Loss: 6547.2949\n",
      "Epoch [20/50], Step [40/51], Loss: 5498.1177\n",
      "Epoch [20/50], Step [50/51], Loss: 5738.1890\n",
      "Epoch [20/50] finished. Average Training Loss: 5369.0403\n",
      "Validation Loss: 5267.1217\n",
      "Epoch [21/50], Step [10/51], Loss: 5443.3022\n",
      "Epoch [21/50], Step [20/51], Loss: 5834.9150\n",
      "Epoch [21/50], Step [30/51], Loss: 5738.4663\n",
      "Epoch [21/50], Step [40/51], Loss: 5006.9263\n",
      "Epoch [21/50], Step [50/51], Loss: 5728.0913\n",
      "Epoch [21/50] finished. Average Training Loss: 5121.7563\n",
      "Validation Loss: 5022.1970\n",
      "Epoch [22/50], Step [10/51], Loss: 5333.6753\n",
      "Epoch [22/50], Step [20/51], Loss: 4617.7896\n",
      "Epoch [22/50], Step [30/51], Loss: 4000.6980\n",
      "Epoch [22/50], Step [40/51], Loss: 4184.5576\n",
      "Epoch [22/50], Step [50/51], Loss: 4101.1860\n",
      "Epoch [22/50] finished. Average Training Loss: 4876.8350\n",
      "Validation Loss: 4788.4337\n",
      "Epoch [23/50], Step [10/51], Loss: 5323.5669\n",
      "Epoch [23/50], Step [20/51], Loss: 4572.7783\n",
      "Epoch [23/50], Step [30/51], Loss: 4928.6401\n",
      "Epoch [23/50], Step [40/51], Loss: 4474.1587\n",
      "Epoch [23/50], Step [50/51], Loss: 4391.3955\n",
      "Epoch [23/50] finished. Average Training Loss: 4650.0711\n",
      "Validation Loss: 4566.5129\n",
      "Epoch [24/50], Step [10/51], Loss: 4910.8760\n",
      "Epoch [24/50], Step [20/51], Loss: 3790.5408\n",
      "Epoch [24/50], Step [30/51], Loss: 3240.3196\n",
      "Epoch [24/50], Step [40/51], Loss: 4447.5156\n",
      "Epoch [24/50], Step [50/51], Loss: 4181.1821\n",
      "Epoch [24/50] finished. Average Training Loss: 4430.9380\n",
      "Validation Loss: 4356.7869\n",
      "Epoch [25/50], Step [10/51], Loss: 4116.6914\n",
      "Epoch [25/50], Step [20/51], Loss: 5034.2544\n",
      "Epoch [25/50], Step [30/51], Loss: 5197.8657\n",
      "Epoch [25/50], Step [40/51], Loss: 4225.1147\n",
      "Epoch [25/50], Step [50/51], Loss: 3521.9160\n",
      "Epoch [25/50] finished. Average Training Loss: 4219.2607\n",
      "Validation Loss: 4157.6719\n",
      "Epoch [26/50], Step [10/51], Loss: 3960.3569\n",
      "Epoch [26/50], Step [20/51], Loss: 3197.8838\n",
      "Epoch [26/50], Step [30/51], Loss: 3471.7490\n",
      "Epoch [26/50], Step [40/51], Loss: 4036.3340\n",
      "Epoch [26/50], Step [50/51], Loss: 3046.0359\n",
      "Epoch [26/50] finished. Average Training Loss: 4030.2970\n",
      "Validation Loss: 3969.5058\n",
      "Epoch [27/50], Step [10/51], Loss: 4073.7261\n",
      "Epoch [27/50], Step [20/51], Loss: 3695.8110\n",
      "Epoch [27/50], Step [30/51], Loss: 4729.7036\n",
      "Epoch [27/50], Step [40/51], Loss: 3369.3865\n",
      "Epoch [27/50], Step [50/51], Loss: 4177.5562\n",
      "Epoch [27/50] finished. Average Training Loss: 3841.7909\n",
      "Validation Loss: 3791.9825\n",
      "Epoch [28/50], Step [10/51], Loss: 4206.7676\n",
      "Epoch [28/50], Step [20/51], Loss: 3617.8367\n",
      "Epoch [28/50], Step [30/51], Loss: 4096.0400\n",
      "Epoch [28/50], Step [40/51], Loss: 3639.4841\n",
      "Epoch [28/50], Step [50/51], Loss: 4208.5449\n",
      "Epoch [28/50] finished. Average Training Loss: 3675.1288\n",
      "Validation Loss: 3624.1923\n",
      "Epoch [29/50], Step [10/51], Loss: 2693.5071\n",
      "Epoch [29/50], Step [20/51], Loss: 3143.7109\n",
      "Epoch [29/50], Step [30/51], Loss: 3044.2725\n",
      "Epoch [29/50], Step [40/51], Loss: 3627.2471\n",
      "Epoch [29/50], Step [50/51], Loss: 3152.5928\n",
      "Epoch [29/50] finished. Average Training Loss: 3507.7631\n",
      "Validation Loss: 3464.0520\n",
      "Epoch [30/50], Step [10/51], Loss: 3863.7412\n",
      "Epoch [30/50], Step [20/51], Loss: 4194.1108\n",
      "Epoch [30/50], Step [30/51], Loss: 3856.6587\n",
      "Epoch [30/50], Step [40/51], Loss: 3414.7141\n",
      "Epoch [30/50], Step [50/51], Loss: 3951.8931\n",
      "Epoch [30/50] finished. Average Training Loss: 3348.7799\n",
      "Validation Loss: 3313.6069\n",
      "Epoch [31/50], Step [10/51], Loss: 2933.6602\n",
      "Epoch [31/50], Step [20/51], Loss: 3311.7822\n",
      "Epoch [31/50], Step [30/51], Loss: 2896.0410\n",
      "Epoch [31/50], Step [40/51], Loss: 3445.4551\n",
      "Epoch [31/50], Step [50/51], Loss: 2849.4453\n",
      "Epoch [31/50] finished. Average Training Loss: 3198.7700\n",
      "Validation Loss: 3168.3842\n",
      "Epoch [32/50], Step [10/51], Loss: 3477.8091\n",
      "Epoch [32/50], Step [20/51], Loss: 3491.8511\n",
      "Epoch [32/50], Step [30/51], Loss: 3005.7629\n",
      "Epoch [32/50], Step [40/51], Loss: 3750.7505\n",
      "Epoch [32/50], Step [50/51], Loss: 2784.2944\n",
      "Epoch [32/50] finished. Average Training Loss: 3062.7758\n",
      "Validation Loss: 3032.8567\n",
      "Epoch [33/50], Step [10/51], Loss: 2654.7290\n",
      "Epoch [33/50], Step [20/51], Loss: 3322.8113\n",
      "Epoch [33/50], Step [30/51], Loss: 2765.6045\n",
      "Epoch [33/50], Step [40/51], Loss: 3244.0925\n",
      "Epoch [33/50], Step [50/51], Loss: 2750.2195\n",
      "Epoch [33/50] finished. Average Training Loss: 2925.6748\n",
      "Validation Loss: 2902.3048\n",
      "Epoch [34/50], Step [10/51], Loss: 2481.9353\n",
      "Epoch [34/50], Step [20/51], Loss: 3795.6558\n",
      "Epoch [34/50], Step [30/51], Loss: 2944.7002\n",
      "Epoch [34/50], Step [40/51], Loss: 2748.5598\n",
      "Epoch [34/50], Step [50/51], Loss: 2895.5862\n",
      "Epoch [34/50] finished. Average Training Loss: 2795.0884\n",
      "Validation Loss: 2778.0143\n",
      "Epoch [35/50], Step [10/51], Loss: 2997.1060\n",
      "Epoch [35/50], Step [20/51], Loss: 2708.1165\n",
      "Epoch [35/50], Step [30/51], Loss: 2561.0474\n",
      "Epoch [35/50], Step [40/51], Loss: 2271.3091\n",
      "Epoch [35/50], Step [50/51], Loss: 2936.9468\n",
      "Epoch [35/50] finished. Average Training Loss: 2676.6561\n",
      "Validation Loss: 2657.8941\n",
      "Epoch [36/50], Step [10/51], Loss: 1910.1284\n",
      "Epoch [36/50], Step [20/51], Loss: 3111.3799\n",
      "Epoch [36/50], Step [30/51], Loss: 2724.4316\n",
      "Epoch [36/50], Step [40/51], Loss: 2602.2434\n",
      "Epoch [36/50], Step [50/51], Loss: 2053.7083\n",
      "Epoch [36/50] finished. Average Training Loss: 2558.3739\n",
      "Validation Loss: 2542.0484\n",
      "Epoch [37/50], Step [10/51], Loss: 3258.0676\n",
      "Epoch [37/50], Step [20/51], Loss: 2826.9124\n",
      "Epoch [37/50], Step [30/51], Loss: 2562.0825\n",
      "Epoch [37/50], Step [40/51], Loss: 2223.2332\n",
      "Epoch [37/50], Step [50/51], Loss: 2315.1804\n",
      "Epoch [37/50] finished. Average Training Loss: 2448.9992\n",
      "Validation Loss: 2431.5177\n",
      "Epoch [38/50], Step [10/51], Loss: 2167.9976\n",
      "Epoch [38/50], Step [20/51], Loss: 2525.4485\n",
      "Epoch [38/50], Step [30/51], Loss: 2758.3965\n",
      "Epoch [38/50], Step [40/51], Loss: 3132.0049\n",
      "Epoch [38/50], Step [50/51], Loss: 2004.1227\n",
      "Epoch [38/50] finished. Average Training Loss: 2336.6903\n",
      "Validation Loss: 2324.0966\n",
      "Epoch [39/50], Step [10/51], Loss: 2153.2942\n",
      "Epoch [39/50], Step [20/51], Loss: 2169.2886\n",
      "Epoch [39/50], Step [30/51], Loss: 2028.8800\n",
      "Epoch [39/50], Step [40/51], Loss: 1844.6632\n",
      "Epoch [39/50], Step [50/51], Loss: 1784.9006\n",
      "Epoch [39/50] finished. Average Training Loss: 2232.3902\n",
      "Validation Loss: 2220.8593\n",
      "Epoch [40/50], Step [10/51], Loss: 2131.8711\n",
      "Epoch [40/50], Step [20/51], Loss: 2237.1646\n",
      "Epoch [40/50], Step [30/51], Loss: 2314.7595\n",
      "Epoch [40/50], Step [40/51], Loss: 3182.4258\n",
      "Epoch [40/50], Step [50/51], Loss: 2584.2715\n",
      "Epoch [40/50] finished. Average Training Loss: 2128.5362\n",
      "Validation Loss: 2120.3618\n",
      "Epoch [41/50], Step [10/51], Loss: 2664.5454\n",
      "Epoch [41/50], Step [20/51], Loss: 2087.1787\n",
      "Epoch [41/50], Step [30/51], Loss: 1480.6965\n",
      "Epoch [41/50], Step [40/51], Loss: 2139.4290\n",
      "Epoch [41/50], Step [50/51], Loss: 2338.9409\n",
      "Epoch [41/50] finished. Average Training Loss: 2033.8855\n",
      "Validation Loss: 2024.5448\n",
      "Epoch [42/50], Step [10/51], Loss: 2419.8118\n",
      "Epoch [42/50], Step [20/51], Loss: 1390.7992\n",
      "Epoch [42/50], Step [30/51], Loss: 1611.3750\n",
      "Epoch [42/50], Step [40/51], Loss: 2273.2092\n",
      "Epoch [42/50], Step [50/51], Loss: 2048.5037\n",
      "Epoch [42/50] finished. Average Training Loss: 1938.6794\n",
      "Validation Loss: 1931.9775\n",
      "Epoch [43/50], Step [10/51], Loss: 2073.2227\n",
      "Epoch [43/50], Step [20/51], Loss: 2856.2412\n",
      "Epoch [43/50], Step [30/51], Loss: 1473.0164\n",
      "Epoch [43/50], Step [40/51], Loss: 2065.2148\n",
      "Epoch [43/50], Step [50/51], Loss: 1479.8625\n",
      "Epoch [43/50] finished. Average Training Loss: 1846.6476\n",
      "Validation Loss: 1843.0942\n",
      "Epoch [44/50], Step [10/51], Loss: 1576.5973\n",
      "Epoch [44/50], Step [20/51], Loss: 2155.5630\n",
      "Epoch [44/50], Step [30/51], Loss: 1775.2531\n",
      "Epoch [44/50], Step [40/51], Loss: 1605.8666\n",
      "Epoch [44/50], Step [50/51], Loss: 1620.3553\n",
      "Epoch [44/50] finished. Average Training Loss: 1765.7848\n",
      "Validation Loss: 1757.2702\n",
      "Epoch [45/50], Step [10/51], Loss: 1826.3119\n",
      "Epoch [45/50], Step [20/51], Loss: 1633.2175\n",
      "Epoch [45/50], Step [30/51], Loss: 2829.0928\n",
      "Epoch [45/50], Step [40/51], Loss: 1496.2039\n",
      "Epoch [45/50], Step [50/51], Loss: 2103.2725\n",
      "Epoch [45/50] finished. Average Training Loss: 1686.8693\n",
      "Validation Loss: 1676.2452\n",
      "Epoch [46/50], Step [10/51], Loss: 2156.5027\n",
      "Epoch [46/50], Step [20/51], Loss: 1587.1534\n",
      "Epoch [46/50], Step [30/51], Loss: 2384.1702\n",
      "Epoch [46/50], Step [40/51], Loss: 1512.4530\n",
      "Epoch [46/50], Step [50/51], Loss: 1060.3314\n",
      "Epoch [46/50] finished. Average Training Loss: 1602.6925\n",
      "Validation Loss: 1597.6064\n",
      "Epoch [47/50], Step [10/51], Loss: 947.1683\n",
      "Epoch [47/50], Step [20/51], Loss: 1315.4934\n",
      "Epoch [47/50], Step [30/51], Loss: 1396.8945\n",
      "Epoch [47/50], Step [40/51], Loss: 1470.6094\n",
      "Epoch [47/50], Step [50/51], Loss: 1637.7789\n",
      "Epoch [47/50] finished. Average Training Loss: 1524.5778\n",
      "Validation Loss: 1522.8893\n",
      "Epoch [48/50], Step [10/51], Loss: 1647.7759\n",
      "Epoch [48/50], Step [20/51], Loss: 1973.0323\n",
      "Epoch [48/50], Step [30/51], Loss: 1692.4891\n",
      "Epoch [48/50], Step [40/51], Loss: 1878.2318\n",
      "Epoch [48/50], Step [50/51], Loss: 1539.7164\n",
      "Epoch [48/50] finished. Average Training Loss: 1452.8145\n",
      "Validation Loss: 1451.9961\n",
      "Epoch [49/50], Step [10/51], Loss: 1505.0271\n",
      "Epoch [49/50], Step [20/51], Loss: 1872.4713\n",
      "Epoch [49/50], Step [30/51], Loss: 1485.2728\n",
      "Epoch [49/50], Step [40/51], Loss: 1092.5900\n",
      "Epoch [49/50], Step [50/51], Loss: 1630.2678\n",
      "Epoch [49/50] finished. Average Training Loss: 1385.0535\n",
      "Validation Loss: 1384.1912\n",
      "Epoch [50/50], Step [10/51], Loss: 1491.8599\n",
      "Epoch [50/50], Step [20/51], Loss: 942.3839\n",
      "Epoch [50/50], Step [30/51], Loss: 1665.6453\n",
      "Epoch [50/50], Step [40/51], Loss: 1096.7777\n",
      "Epoch [50/50], Step [50/51], Loss: 1028.9344\n",
      "Epoch [50/50] finished. Average Training Loss: 1322.2874\n",
      "Validation Loss: 1320.8914\n",
      "\n",
      "PyTorch model training complete.\n",
      "Error: User audio file not found at /content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg. Please upload the file.\n",
      "\n",
      "Skipping music generation as user audio file was not found.\n"
     ]
    }
   ],
   "source": [
    "# Regenerate the content of cell 'a1b11bff' to ensure functions and model are defined\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "from pydub import AudioSegment # Import AudioSegment for MP3 conversion\n",
    "from transformers import pipeline # Import pipeline for using pre-trained models\n",
    "from IPython.display import display, FileLink, Audio # Import display, FileLink, and Audio\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split for data splitting\n",
    "import kagglehub # Import kagglehub for dataset download\n",
    "import tensorflow as tf # Import tensorflow\n",
    "from tensorflow.keras.models import Model # Import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, GRU, Input, TimeDistributed, Bidirectional # Import necessary layers\n",
    "import torch # Import PyTorch\n",
    "import torch.nn as nn # Import PyTorch neural network module\n",
    "from music21 import stream, note, duration, tempo # Import music21 elements\n",
    "\n",
    "\n",
    "# Define sampling rate (needed for audio processing and saving)\n",
    "sr = 22050 # Common sampling rate, adjust if your audio files use a different rate\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This needs to be consistent with feature extraction\n",
    "n_chroma = 12 # Number of chroma bins\n",
    "total_audio_features_dim = n_mfcc + n_chroma # Total features for audio conditioning\n",
    "\n",
    "# Define the dimensions for the music generation model\n",
    "musical_feature_dim = n_mfcc # The model output is MFCCs\n",
    "musical_sequence_length = max_padding # The model input sequence length\n",
    "\n",
    "# Define the conditioning input dimensions\n",
    "text_conditioning_dim = 128 # Dimension of the text embedding\n",
    "\n",
    "# --- Helper functions ---\n",
    "\n",
    "# Initialize the text generation pipeline (if not already initialized)\n",
    "lyric_generator = None # Initialize to None\n",
    "try:\n",
    "    lyric_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "    print(\"Text generation pipeline (distilgpt2) initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing text generation pipeline: {e}\")\n",
    "    lyric_generator = None\n",
    "\n",
    "\n",
    "def generate_lyrics_with_language_model(prompt):\n",
    "    \"\"\"Generates lyrics based on a prompt using a language model.\"\"\"\n",
    "    if lyric_generator:\n",
    "        print(f\"Generating lyrics for prompt: '{prompt}' using DistilGPT-2.\")\n",
    "        try:\n",
    "            # Generate text with a reasonable length and without generating too many sequences\n",
    "            generated_text = lyric_generator(prompt, max_length=100, num_return_sequences=1, truncation=True)\n",
    "            lyrics = generated_text[0]['generated_text']\n",
    "            # Basic post-processing: remove the prompt from the generated text if it's repeated\n",
    "            if lyrics.startswith(prompt):\n",
    "                lyrics = lyrics[len(prompt):].strip()\n",
    "            print(\"Lyric generation successful.\")\n",
    "            return lyrics\n",
    "        except Exception as e:\n",
    "            print(f\"Error during lyric generation: {e}\")\n",
    "            return f\"Error generating lyrics for prompt: '{prompt}'\" # Return an error message\n",
    "    else:\n",
    "        print(\"Lyric generator not initialized. Cannot generate lyrics.\")\n",
    "        return f\"Cannot generate lyrics for prompt: '{prompt}' (generator not available)\"\n",
    "\n",
    "\n",
    "# Function to get text embeddings (placeholder)\n",
    "def get_text_embedding(text, embedding_dim):\n",
    "    \"\"\"Placeholder function to get a fixed-size embedding for text.\"\"\"\n",
    "    print(f\"Getting embedding for text: '{text[:50]}...' using a placeholder model.\")\n",
    "    # Return as numpy array for now, will convert to tensor later in process_text_input\n",
    "    return np.random.rand(1, embedding_dim) # Shape (1, embedding_dim)\n",
    "\n",
    "\n",
    "# Function to process text input\n",
    "def process_text_input(user_input, embedding_dim, is_prompt=False):\n",
    "    \"\"\"Processes user text input (lyrics or prompt) and returns a conditioning embedding.\"\"\"\n",
    "    if is_prompt:\n",
    "        lyrics = generate_lyrics_with_language_model(user_input)\n",
    "    else:\n",
    "        lyrics = user_input # User provided lyrics directly\n",
    "\n",
    "    conditioning_embedding_np = get_text_embedding(lyrics, embedding_dim)\n",
    "\n",
    "    # Convert to PyTorch tensor and move to device\n",
    "    conditioning_embedding_tensor = torch.FloatTensor(conditioning_embedding_np).to(device)\n",
    "\n",
    "\n",
    "    return conditioning_embedding_tensor, lyrics # Return as PyTorch tensor\n",
    "\n",
    "\n",
    "# Function to extract combined features (MFCC + Chroma) from an audio file.\n",
    "# This function is used for both training data preparation and user audio conditioning.\n",
    "def extract_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "    \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract Chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "        # Pad or truncate combined features to a fixed length\n",
    "        if combined_features.shape[1] < target_sequence_length:\n",
    "            combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "        else:\n",
    "            combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "        return combined_features.T # Return as numpy array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process user audio for conditioning - calls extract_features\n",
    "def process_user_audio_for_conditioning(audio_file_path, target_sequence_length, n_mfcc, n_chroma):\n",
    "    \"\"\"Processes user audio input and returns features suitable for conditioning.\"\"\"\n",
    "    processed_features_np = extract_features(audio_file_path, target_sequence_length, n_mfcc, n_chroma)\n",
    "\n",
    "    if processed_features_np is not None:\n",
    "        # Convert to PyTorch tensor, add a batch dimension, and move to device\n",
    "        processed_features_tensor = torch.FloatTensor(processed_features_np).unsqueeze(0).to(device) # Shape (1, target_sequence_length, total_feature_dimension)\n",
    "        print(f\"Processed user audio features shape for conditioning: {processed_features_tensor.shape}\")\n",
    "        return processed_features_tensor\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to extract MFCC features only (used for model input/output targets)\n",
    "def extract_mfcc_features_only(file_path, target_sequence_length, n_mfcc=20):\n",
    "    \"\"\"Extract only MFCC features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Pad or truncate MFCCs to a fixed length\n",
    "        if mfccs.shape[1] < target_sequence_length:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, target_sequence_length - mfccs.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :target_sequence_length]\n",
    "\n",
    "        return mfccs.T # Return as numpy array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Model Definition (PyTorch) ---\n",
    "\n",
    "# Define the PyTorch model\n",
    "class MusicGenerator(nn.Module):\n",
    "    def __init__(self, musical_feature_dim, total_audio_features_dim, text_conditioning_dim, hidden_dim=128):\n",
    "        super(MusicGenerator, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.musical_feature_dim = musical_feature_dim\n",
    "        self.total_audio_features_dim = total_audio_features_dim\n",
    "        self.text_conditioning_dim = text_conditioning_dim\n",
    "\n",
    "        # GRU layers\n",
    "        # Input to GRU will be concatenation of musical features, text conditioning, and audio conditioning\n",
    "        # Dimension of combined input: musical_feature_dim + text_conditioning_dim + total_audio_features_dim\n",
    "        self.gru1 = nn.GRU(musical_feature_dim + text_conditioning_dim + total_audio_features_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.gru2 = nn.GRU(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True) # *2 for bidirectional output\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        # Output layer: predicts musical features for each time step\n",
    "        # Since the GRU returns sequences, we use a TimeDistributed-like approach with a linear layer\n",
    "        self.linear = nn.Linear(hidden_dim * 2, musical_feature_dim) # *2 for bidirectional output\n",
    "\n",
    "    def forward(self, musical_input, text_conditioning_input, audio_conditioning_input):\n",
    "        # musical_input shape: (batch_size, sequence_length, musical_feature_dim)\n",
    "        # text_conditioning_input shape: (batch_size, text_conditioning_dim)\n",
    "        # audio_conditioning_input shape: (batch_size, sequence_length, total_audio_features_dim)\n",
    "\n",
    "        # Repeat text conditioning across the sequence length\n",
    "        # text_conditioning_input shape: (batch_size, 1, text_conditioning_dim)\n",
    "        text_conditioning_repeated = text_conditioning_input.unsqueeze(1).repeat(1, musical_input.size(1), 1)\n",
    "        # text_conditioning_repeated shape: (batch_size, sequence_length, text_conditioning_dim)\n",
    "\n",
    "\n",
    "        # Concatenate inputs\n",
    "        combined_input = torch.cat((musical_input, text_conditioning_repeated, audio_conditioning_input), dim=-1)\n",
    "        # combined_input shape: (batch_size, sequence_length, musical_feature_dim + text_conditioning_dim + total_audio_features_dim)\n",
    "\n",
    "        # Pass through GRU layers\n",
    "        gru_out1, _ = self.gru1(combined_input)\n",
    "        dropout_out1 = self.dropout1(gru_out1)\n",
    "\n",
    "        gru_out2, _ = self.gru2(dropout_out1)\n",
    "        dropout_out2 = self.dropout2(gru_out2)\n",
    "\n",
    "        # Apply linear layer to each time step's output\n",
    "        output = self.linear(dropout_out2)\n",
    "        # output shape: (batch_size, sequence_length, musical_feature_dim)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Check if a GPU is available and use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model_gen_conditioned_pytorch = MusicGenerator(musical_feature_dim, total_audio_features_dim, text_conditioning_dim)\n",
    "\n",
    "# Move the model to the selected device\n",
    "model_gen_conditioned_pytorch.to(device)\n",
    "\n",
    "print(\"\\nPyTorch conditional music generation model architecture defined.\")\n",
    "print(model_gen_conditioned_pytorch)\n",
    "\n",
    "\n",
    "# --- Data Preparation ---\n",
    "# Download the dataset and directly assign the path within this cell\n",
    "print(\"Downloading the dataset to ensure path is available...\")\n",
    "try:\n",
    "    downloaded_dataset_root = kagglehub.dataset_download(\"soumendraprasad/musical-instruments-sound-dataset\")\n",
    "    print(f\"Dataset path: {downloaded_dataset_root}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    downloaded_dataset_root = None # Set to None if download fails\n",
    "\n",
    "\n",
    "if downloaded_dataset_root is None or not os.path.exists(downloaded_dataset_root):\n",
    "    print(\"Failed to get the dataset path. Cannot proceed with data preparation.\")\n",
    "else:\n",
    "    # Define feature extraction parameters (should be consistent with model definition)\n",
    "    n_mfcc = 20\n",
    "    max_padding = 174\n",
    "    n_chroma = 12 # Although not used for musical features output, needed for audio conditioning feature extraction\n",
    "    total_audio_features_dim = n_mfcc + n_chroma\n",
    "    text_conditioning_dim = 128\n",
    "\n",
    "\n",
    "    # Collect music instrument files from the downloaded dataset\n",
    "    music_instrument_files = []\n",
    "\n",
    "    # Assuming the structure from the previous file listing\n",
    "    train_submission_path = os.path.join(downloaded_dataset_root, 'Train_submission', 'Train_submission')\n",
    "    test_submission_path = os.path.join(downloaded_dataset_root, 'Test_submission', 'Test_submission') # Assuming Test_submission is also relevant for data\n",
    "\n",
    "    # Collect files from Train_submission\n",
    "    if os.path.exists(train_submission_path):\n",
    "        print(f\"Searching for music instrument files in: {train_submission_path}\")\n",
    "        for root, dirs, files in os.walk(train_submission_path):\n",
    "            for name in files:\n",
    "                if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                     # Optional: further filter by keywords if needed\n",
    "                    if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                         music_instrument_files.append(os.path.join(root, name))\n",
    "        print(f\"Found {len(music_instrument_files)} music instrument files in training submission.\")\n",
    "    else:\n",
    "        print(f\"Warning: Training submission directory not found at {train_submission_path}\")\n",
    "\n",
    "\n",
    "    # Collect files from Test_submission (if it exists and contains relevant audio)\n",
    "    # You might decide to use test data only for evaluation later, but for now, let's include relevant audio\n",
    "    if os.path.exists(test_submission_path):\n",
    "         print(f\"Searching for music instrument files in: {test_submission_path}\")\n",
    "         for root, dirs, files in os.walk(test_submission_path):\n",
    "             for name in files:\n",
    "                 if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                      # Optional: further filter by keywords if needed\n",
    "                     if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                          music_instrument_files.append(os.path.join(root, name))\n",
    "         print(f\"Found {len(music_instrument_files)} music instrument files in test submission.\")\n",
    "    else:\n",
    "         print(f\"Warning: Test submission directory not found at {test_submission_path}\")\n",
    "\n",
    "\n",
    "    # Use the collected music instrument files for feature extraction\n",
    "    files_to_process_music = music_instrument_files\n",
    "\n",
    "    # Prepare lists to store extracted features for training\n",
    "    X_train_music_features = [] # For musical input (MFCCs only)\n",
    "    X_train_audio_conditioning_features = [] # For audio conditioning input (MFCC + Chroma)\n",
    "\n",
    "\n",
    "    print(\"\\nExtracting musical features (MFCCs) and audio conditioning features (MFCC+Chroma) from music instrument files...\")\n",
    "\n",
    "    # Process a subset of files for faster testing, or all files for full training\n",
    "    # process_limit = 100 # Set a limit for testing\n",
    "    # files_to_process_subset = files_to_process_music[:process_limit]\n",
    "    files_to_process_subset = files_to_process_music # Process all files\n",
    "\n",
    "\n",
    "    for i, file_path in enumerate(files_to_process_subset):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processing file {i + 1}/{len(files_to_process_subset)}\")\n",
    "\n",
    "        # Extract MFCC features for the musical input/output\n",
    "        mfccs_only = extract_mfcc_features_only(file_path, max_padding, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract combined features for the audio conditioning input\n",
    "        combined_features = extract_features(file_path, max_padding, n_mfcc=n_mfcc, n_chroma=n_chroma)\n",
    "\n",
    "\n",
    "        if mfccs_only is not None and combined_features is not None:\n",
    "            X_train_music_features.append(mfccs_only)\n",
    "            X_train_audio_conditioning_features.append(combined_features)\n",
    "\n",
    "\n",
    "    # Convert extracted features to numpy arrays (keeping as numpy for TensorFlow training setup initially)\n",
    "    X_train_music_features = np.array(X_train_music_features)\n",
    "    X_train_audio_conditioning_features = np.array(X_train_audio_conditioning_features)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    # Ensure that the split is consistent for both sets of features\n",
    "    if len(X_train_music_features) > 0:\n",
    "        X_train_music, X_val_music, X_train_audio_cond, X_val_audio_cond = train_test_split(\n",
    "            X_train_music_features, X_train_audio_conditioning_features, test_size=0.2, random_state=42\n",
    "        )\n",
    "    else:\n",
    "        print(\"No features were successfully extracted. Cannot split data.\")\n",
    "        # Initialize empty arrays to prevent errors in subsequent steps\n",
    "        X_train_music = np.array([])\n",
    "        X_val_music = np.array([])\n",
    "        X_train_audio_cond = np.array([])\n",
    "        X_val_audio_cond = np.array([])\n",
    "\n",
    "\n",
    "    print(\"\\nFeature extraction and data splitting complete.\")\n",
    "    print(f\"Shape of training musical features (MFCCs): {X_train_music.shape}\")\n",
    "    print(f\"Shape of validation musical features (MFCCs): {X_val_music.shape}\")\n",
    "    print(f\"Shape of training audio conditioning features (MFCC+Chroma): {X_train_audio_cond.shape}\")\n",
    "    print(f\"Shape of validation audio conditioning features (MFCC+Chroma): {X_val_audio_cond.shape}\")\n",
    "\n",
    "    # Create dummy text conditioning data for training\n",
    "    # This data is not based on actual text input yet, just for model training setup\n",
    "    # It should match the expected shape of the text_conditioning_input layer\n",
    "    dummy_text_conditioning_train = np.random.rand(X_train_music.shape[0], text_conditioning_dim)\n",
    "    dummy_text_conditioning_val = np.random.rand(X_val_music.shape[0], text_conditioning_dim)\n",
    "\n",
    "\n",
    "    print(f\"\\nShape of dummy training text conditioning data: {dummy_text_conditioning_train.shape}\")\n",
    "    print(f\"Shape of dummy validation text conditioning data: {dummy_text_conditioning_val.shape}\")\n",
    "\n",
    "\n",
    "    # --- Train the Model (PyTorch) ---\n",
    "    # Check if training data is available before training\n",
    "    if 'X_train_music' in globals() and X_train_music.shape[0] > 0:\n",
    "        # Convert NumPy data to PyTorch tensors and move to device\n",
    "        X_train_music_tensor = torch.FloatTensor(X_train_music).to(device)\n",
    "        X_val_music_tensor = torch.FloatTensor(X_val_music).to(device)\n",
    "        X_train_audio_cond_tensor = torch.FloatTensor(X_train_audio_cond).to(device)\n",
    "        X_val_audio_cond_tensor = torch.FloatTensor(X_val_audio_cond).to(device)\n",
    "        dummy_text_conditioning_train_tensor = torch.FloatTensor(dummy_text_conditioning_train).to(device)\n",
    "        dummy_text_conditioning_val_tensor = torch.FloatTensor(dummy_text_conditioning_val).to(device)\n",
    "\n",
    "        # Create PyTorch DataLoaders\n",
    "        from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_music_tensor, dummy_text_conditioning_train_tensor, X_train_audio_cond_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        val_dataset = TensorDataset(X_val_music_tensor, dummy_text_conditioning_val_tensor, X_val_audio_cond_tensor)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32) # No need to shuffle validation data\n",
    "\n",
    "        print(\"\\nData converted to PyTorch tensors and DataLoaders created for training.\")\n",
    "\n",
    "        # Define Loss Function and Optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer_pytorch = torch.optim.Adam(model_gen_conditioned_pytorch.parameters(), lr=0.001) # You can adjust the learning rate\n",
    "\n",
    "\n",
    "        num_epochs = 50 # You can adjust the number of epochs\n",
    "        print(f\"\\nStarting PyTorch model training for {num_epochs} epochs...\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model_gen_conditioned_pytorch.train() # Set the model to training mode\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for i, (musical_inputs, text_cond_inputs, audio_cond_inputs) in enumerate(train_loader):\n",
    "                # Zero the parameter gradients\n",
    "                optimizer_pytorch.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model_gen_conditioned_pytorch(musical_inputs, text_cond_inputs, audio_cond_inputs)\n",
    "\n",
    "                # Calculate loss\n",
    "                # The target for musical_inputs is the same as the input in this autoencoder-like setup\n",
    "                loss = criterion(outputs, musical_inputs)\n",
    "\n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer_pytorch.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Print statistics\n",
    "                if (i + 1) % 10 == 0: # Print every 10 batches\n",
    "                    print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}] finished. Average Training Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "            # Validation step\n",
    "            model_gen_conditioned_pytorch.eval() # Set the model to evaluation mode\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad(): # Disable gradient calculation for validation\n",
    "                for musical_inputs_val, text_cond_inputs_val, audio_cond_inputs_val in val_loader:\n",
    "                    outputs_val = model_gen_conditioned_pytorch(musical_inputs_val, text_cond_inputs_val, audio_cond_inputs_val)\n",
    "                    loss_val = criterion(outputs_val, musical_inputs_val)\n",
    "                    val_loss += loss_val.item()\n",
    "\n",
    "            print(f'Validation Loss: {val_loss / len(val_loader):.4f}')\n",
    "\n",
    "        print(\"\\nPyTorch model training complete.\")\n",
    "    else:\n",
    "        print(\"Training data is empty. Skipping PyTorch model training.\")\n",
    "\n",
    "\n",
    "# --- Function to generate musical features iteratively (PyTorch) ---\n",
    "def generate_musical_features_iterative_pytorch(model, start_sequence, text_conditioning_input, audio_conditioning_input, total_generation_steps):\n",
    "    \"\"\"Generates new MFCC sequences step-by-step using the trained PyTorch conditional model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained PyTorch conditional generation model.\n",
    "        start_sequence: A PyTorch tensor representing the initial musical sequence\n",
    "                        (shape: (1, sequence_length, musical_feature_dim)).\n",
    "        text_conditioning_input: The text conditioning input (shape: (1, text_conditioning_dim)).\n",
    "        audio_conditioning_input: The audio conditioning input (sequence of features,\n",
    "                                   shape: (1, sequence_length, total_audio_features_dim)).\n",
    "                                   This is the *full* audio conditioning sequence.\n",
    "        total_generation_steps: The total number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated musical MFCC sequence (shape: (total_generation_steps, musical_feature_dim)),\n",
    "        or None if generation fails.\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating musical feature sequence iteratively for {total_generation_steps} steps using PyTorch model...\")\n",
    "\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    generated_sequence = [] # List to store generated time steps\n",
    "    current_musical_sequence = start_sequence.clone().to(device) # Start with the initial sequence on the correct device\n",
    "    sequence_length = start_sequence.shape[1] # Expected input sequence length for the model\n",
    "    musical_feature_dim = start_sequence.shape[2] # Dimension of musical features\n",
    "\n",
    "    # Ensure conditioning inputs are on the correct device\n",
    "    text_conditioning_input = text_conditioning_input.to(device)\n",
    "    audio_conditioning_input = audio_conditioning_input.to(device)\n",
    "\n",
    "\n",
    "    # Ensure audio_conditioning_input has a batch dimension and correct sequence length\n",
    "    if audio_conditioning_input.ndim != 3 or audio_conditioning_input.shape[1] != sequence_length:\n",
    "         print(\"Error: audio_conditioning_input must have shape (1, sequence_length, total_audio_features_dim).\")\n",
    "         return None\n",
    "\n",
    "    # Ensure text_conditioning_input has a batch dimension and correct dimension\n",
    "    if text_conditioning_input.ndim != 2 or text_conditioning_input.shape[1] != text_conditioning_dim:\n",
    "        print(\"Error: text_conditioning_input must have shape (1, text_conditioning_dim).\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation during generation\n",
    "        for i in range(total_generation_steps):\n",
    "            # The model expects [musical_input, text_conditioning_input, audio_conditioning_input]\n",
    "            model_inputs = [current_musical_sequence, text_conditioning_input, audio_conditioning_input]\n",
    "\n",
    "            # Predict the next time step of musical features\n",
    "            try:\n",
    "                # The model predicts a sequence of length `musical_sequence_length`.\n",
    "                # We are interested in the prediction for the *next* time step.\n",
    "                predicted_sequence = model(*model_inputs) # Pass inputs unpacking the list\n",
    "                # predicted_sequence shape: (1, musical_sequence_length, musical_feature_dim)\n",
    "\n",
    "                # Take the last time step of the prediction\n",
    "                predicted_next_step = predicted_sequence[:, -1:, :] # Shape (1, 1, musical_feature_dim)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during model prediction at step {i}: {e}\")\n",
    "                return None\n",
    "\n",
    "\n",
    "            # Append the predicted time step to the generated sequence (as numpy array)\n",
    "            generated_sequence.append(predicted_next_step.squeeze().cpu().numpy()) # Move to CPU and convert to numpy\n",
    "\n",
    "            # Update the current musical sequence by removing the oldest time step and adding the new one\n",
    "            # This sliding window approach is standard for iterative sequence generation.\n",
    "            # Ensure the sequence length remains constant\n",
    "            current_musical_sequence = torch.cat((current_musical_sequence[:, 1:, :], predicted_next_step), dim=1)\n",
    "\n",
    "            # Print progress\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Generated {i + 1}/{total_generation_steps} steps.\")\n",
    "\n",
    "\n",
    "    # Convert the list of generated steps into a numpy array\n",
    "    generated_sequence_np = np.array(generated_sequence) # Shape (total_generation_steps, musical_feature_dim)\n",
    "\n",
    "    print(\"\\nIterative musical feature generation complete.\")\n",
    "    print(f\"Shape of the generated musical features: {generated_sequence_np.shape}\")\n",
    "\n",
    "    # Add inspection of generated features\n",
    "    print(\"\\nGenerated Musical Features Statistics:\")\n",
    "    print(f\"Shape: {generated_sequence_np.shape}\")\n",
    "    print(f\"Mean: {np.mean(generated_sequence_np)}\")\n",
    "    print(f\"Min: {np.min(generated_sequence_np)}\")\n",
    "    print(f\"Max: {np.max(generated_sequence_np)}\")\n",
    "    print(f\"Standard Deviation: {np.std(generated_sequence_np)}\")\n",
    "\n",
    "    # Save generated features to a file for inspection\n",
    "    features_output_filename = 'generated_musical_features_iterative_pytorch.npy'\n",
    "    np.save(features_output_filename, generated_sequence_np)\n",
    "    print(f\"Generated musical features saved to {features_output_filename}\")\n",
    "\n",
    "\n",
    "    return generated_sequence_np\n",
    "\n",
    "\n",
    "# --- Function to convert musical features to audio and save ---\n",
    "def convert_features_to_audio(musical_features, sr=22050, output_filename_wav='generated_music.wav', output_filename_mp3='generated_music.mp3'):\n",
    "    \"\"\"Converts musical features (MFCCs) to an audio file and saves it.\n",
    "\n",
    "    Args:\n",
    "        musical_features: A numpy array containing the generated musical MFCC sequence (shape: (time_steps, n_mfcc)).\n",
    "        sr: Sampling rate for audio output.\n",
    "        output_filename_wav: Filename for the output WAV file.\n",
    "        output_filename_mp3: Filename for the output MP3 file.\n",
    "\n",
    "    Returns:\n",
    "        The path to the generated audio file (MP3 or WAV), or None if conversion/saving fails.\n",
    "    \"\"\"\n",
    "    if musical_features is None or musical_features.shape[0] == 0:\n",
    "        print(\"No musical features provided for audio conversion.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\nConverting generated musical features to audio...\")\n",
    "    try:\n",
    "        # Assuming the features are MFCCs. Transpose back to (n_mfcc, time_steps) for inverse transform\n",
    "        audio_time_series = librosa.feature.inverse.mfcc_to_audio(musical_features.T, sr=sr)\n",
    "\n",
    "        # Add inspection of generated audio time series\n",
    "        print(\"\\nGenerated Audio Time Series Statistics:\")\n",
    "        print(f\"Shape: {audio_time_series.shape}\")\n",
    "        print(f\"Mean: {np.mean(audio_time_series)}\")\n",
    "        print(f\"Min: {np.min(audio_time_series)}\")\n",
    "        print(f\"Max: {np.max(audio_time_series)}\")\n",
    "        print(f\"Standard Deviation: {np.std(audio_time_series)}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MFCC to audio conversion: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Save the generated audio to a WAV file first\n",
    "    print(f\"Saving generated audio to {output_filename_wav}...\")\n",
    "    try:\n",
    "        sf.write(output_filename_wav, audio_time_series, sr)\n",
    "        print(f\"Generated audio saved successfully to {output_filename_wav}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving WAV file: {e}\")\n",
    "        return None\n",
    "\n",
    "    generated_audio_file = output_filename_wav # Default to WAV\n",
    "\n",
    "    # Attempt to convert the WAV file to MP3\n",
    "    print(f\"Attempting to convert {output_filename_wav} to {output_filename_mp3}...\")\n",
    "    try:\n",
    "        audio = AudioSegment.from_wav(output_filename_wav)\n",
    "        audio.export(output_filename_mp3, format=\"mp3\")\n",
    "        print(f\"Conversion to MP3 successful. Output saved to {output_filename_mp3}\")\n",
    "        generated_audio_file = output_filename_mp3 # Update to MP3 if successful\n",
    "        # Optionally, remove the intermediate WAV file\n",
    "        # os.remove(output_filename_wav)\n",
    "        # print(f\"Removed intermediate WAV file: {output_filename_wav}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MP3 conversion: {e}. Proceeding with WAV file.\")\n",
    "        # generated_audio_file remains the WAV file path\n",
    "\n",
    "    return generated_audio_file\n",
    "\n",
    "\n",
    "# --- Function to convert generated features to symbolic music (music21) (Step 4 in Plan) ---\n",
    "def convert_features_to_music21(musical_features_np, sr=22050, hop_length=512):\n",
    "    \"\"\"Converts generated musical features (MFCCs) to a music21 stream.\n",
    "\n",
    "    This is a placeholder function. Converting MFCCs directly to\n",
    "    meaningful symbolic music (notes, chords, rhythm) is a complex task\n",
    "    and requires more advanced techniques (e.g., a separate model trained\n",
    "    to map features to musical events).\n",
    "\n",
    "    Args:\n",
    "        musical_features_np: A numpy array containing the generated musical MFCC sequence (shape: (time_steps, n_mfcc)).\n",
    "        sr: Sampling rate.\n",
    "        hop_length: Hop length used during feature extraction.\n",
    "\n",
    "    Returns:\n",
    "        A music21 stream, or None if conversion is not implemented or fails.\n",
    "    \"\"\"\n",
    "    print(\"\\nAttempting to convert musical features to music21 stream...\")\n",
    "    print(\"Note: Direct conversion of MFCCs to symbolic music is complex and requires further development.\")\n",
    "    # Placeholder implementation: Create a simple stream for demonstration\n",
    "    from music21 import stream, note, duration, tempo, pitch\n",
    "\n",
    "    s = stream.Stream()\n",
    "\n",
    "    # Add a tempo indication (optional, but good practice)\n",
    "    # Estimate tempo from features or use a default\n",
    "    # For simplicity, using a default tempo\n",
    "    s.append(tempo.MetronomeMark(number=120))\n",
    "\n",
    "    # Simple approach: Map a feature value (e.g., the first MFCC coefficient) to pitch\n",
    "    # and use a fixed duration based on the hop length. This is a very basic mapping.\n",
    "    time_per_step = hop_length / sr # Duration of each feature vector in seconds\n",
    "\n",
    "    # Define a mapping range for MFCC values to MIDI pitches (adjust as needed)\n",
    "    # Find min/max of the first MFCC coefficient in the generated sequence\n",
    "    if musical_features_np.shape[0] > 0:\n",
    "        min_mfcc1 = np.min(musical_features_np[:, 0])\n",
    "        max_mfcc1 = np.max(musical_features_np[:, 0])\n",
    "        midi_pitch_range = 60 # Map to a range of 60 MIDI pitches (e.g., from 30 to 90)\n",
    "        min_midi_pitch = 30\n",
    "\n",
    "        for i, features in enumerate(musical_features_np):\n",
    "            # Use the first MFCC coefficient for pitch mapping\n",
    "            mfcc1 = features[0]\n",
    "\n",
    "            # Normalize the MFCC value to the range [0, 1]\n",
    "            if (max_mfcc1 - min_mfcc1) > 0:\n",
    "                normalized_mfcc1 = (mfcc1 - min_mfcc1) / (max_mfcc1 - min_mfcc1)\n",
    "            else:\n",
    "                normalized_mfcc1 = 0.5 # Default to middle if range is zero\n",
    "\n",
    "            # Map the normalized value to a MIDI pitch\n",
    "            midi_pitch_val = int(min_midi_pitch + normalized_mfcc1 * midi_pitch_range)\n",
    "            midi_pitch_val = max(0, min(127, midi_pitch_val)) # Clamp to valid MIDI range\n",
    "\n",
    "            try:\n",
    "                 # Create a note\n",
    "                 n = note.Note(midi_pitch_val)\n",
    "                 # Set the duration based on the time per feature step\n",
    "                 # Using a fixed duration for simplicity, could be refined\n",
    "                 n.duration = duration.Duration(time_per_step)\n",
    "\n",
    "                 s.append(n)\n",
    "            except Exception as note_error:\n",
    "                 print(f\"Error creating music21 note at step {i}: {note_error}\")\n",
    "                 # Optionally, append a rest or skip this step\n",
    "                 s.append(note.Rest(duration=duration.Duration(time_per_step)))\n",
    "\n",
    "\n",
    "            # Avoid creating an excessively large stream for demonstration\n",
    "            if i > 200: # Limit the number of elements for a quicker conversion/display\n",
    "                 break\n",
    "    else:\n",
    "         print(\"No generated features to convert to music21.\")\n",
    "         return None\n",
    "\n",
    "\n",
    "    print(f\"Created a music21 stream with {len(s.elements)} elements.\") # Use .elements to count top-level elements\n",
    "\n",
    "    return s\n",
    "\n",
    "# --- Function to save and display music21 stream (Step 5 in Plan) ---\n",
    "def save_and_display_music21_stream(music21_stream, output_filename_midi='generated_music.mid'):\n",
    "    \"\"\"Saves a music21 stream as a MIDI file and displays its text representation.\"\"\"\n",
    "    if music21_stream is None:\n",
    "        print(\"No music21 stream provided to save or display.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nSaving music21 stream to {output_filename_midi}...\")\n",
    "    try:\n",
    "        music21_stream.write('midi', fp=output_filename_midi)\n",
    "        print(f\"Music21 stream saved successfully to {output_filename_midi}\")\n",
    "\n",
    "        # Display the stream as text\n",
    "        print(\"\\nGenerated Music21 Stream (text representation):\")\n",
    "        print(music21_stream.show('text'))\n",
    "\n",
    "        return output_filename_midi # Return the path to the saved MIDI file\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving or displaying music21 stream: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Main music generation function (uses iterative generation with PyTorch model) ---\n",
    "def generate_music_from_input_v4(model, user_text_input=None, user_audio_input_path=None, input_type='text_prompt', target_audio_duration_sec=30, sr=22050, max_padding=174, n_mfcc=20, n_chroma=12, text_conditioning_dim=128):\n",
    "    \"\"\"Generates music based on user input (text and/or audio) using the PyTorch model and iterative generation.\n",
    "\n",
    "    Args:\n",
    "        model: The trained PyTorch conditional music generation model.\n",
    "        user_text_input: The user's text input (string for lyrics or prompt).\n",
    "        user_audio_input_path: Path to the user's audio file.\n",
    "        input_type: Type of text input ('text_lyrics', 'text_prompt'). 'audio_only' is also possible if no text is provided.\n",
    "        target_audio_duration_sec: The desired duration of the generated audio in seconds.\n",
    "        sr: Sampling rate for audio output.\n",
    "        max_padding: Target sequence length for padding/truncating features (used for initial sequence).\n",
    "        n_mfcc: Number of MFCCs extracted.\n",
    "        n_chroma: Number of chroma bins extracted.\n",
    "        text_conditioning_dim: Dimension of the text embedding.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - The path to the generated audio file (MP3 or WAV),\n",
    "        - The generated lyrics (string),\n",
    "        - The generated musical features (numpy array),\n",
    "        or (None, None, None) if an error occurred.\n",
    "    \"\"\"\n",
    "    text_conditioning_input_for_model = None\n",
    "    audio_conditioning_input_for_model = None\n",
    "    generated_lyrics = None\n",
    "\n",
    "    # Process text input if provided\n",
    "    if user_text_input:\n",
    "        is_prompt = (input_type == 'text_prompt')\n",
    "        conditioning_input_for_model, generated_lyrics = process_text_input(user_text_input, text_conditioning_dim, is_prompt=is_prompt)\n",
    "        if conditioning_input_for_model is None:\n",
    "            print(\"Error processing text input for conditioning.\")\n",
    "            return None, None, None\n",
    "        text_conditioning_input_for_model = conditioning_input_for_model\n",
    "\n",
    "    else:\n",
    "        print(\"No text input provided. Using dummy text conditioning.\")\n",
    "        # Create dummy text conditioning input as a PyTorch tensor and move to device\n",
    "        text_conditioning_input_for_model = torch.zeros((1, text_conditioning_dim)).to(device)\n",
    "\n",
    "\n",
    "    # Process audio input if provided\n",
    "    if user_audio_input_path and os.path.exists(user_audio_input_path):\n",
    "        print(\"Processing user audio input for conditioning.\")\n",
    "        # For iterative generation, we still need the full audio conditioning sequence\n",
    "        audio_conditioning_input_for_model = process_user_audio_for_conditioning(user_audio_input_path, max_padding, n_mfcc, n_chroma)\n",
    "        if audio_conditioning_input_for_model is None:\n",
    "            print(\"Error processing user audio input for conditioning.\")\n",
    "            return None, None, None\n",
    "    else:\n",
    "        print(\"No user audio file provided or found. Using dummy audio conditioning.\")\n",
    "        # Create dummy audio conditioning input as a PyTorch tensor and move to device\n",
    "        audio_conditioning_input_for_model = torch.zeros((1, max_padding, total_audio_features_dim)).to(device)\n",
    "\n",
    "\n",
    "    # Ensure both conditioning inputs are available\n",
    "    if text_conditioning_input_for_model is None or audio_conditioning_input_for_model is None:\n",
    "         print(\"Missing conditioning input(s). Cannot proceed.\")\n",
    "         return None, None, None\n",
    "\n",
    "\n",
    "    # Generate a starting musical sequence (e.g., take a random sample from the training data)\n",
    "    # This starting sequence will be the initial input for the iterative generation.\n",
    "    # Ensure X_train_music is available from data loading\n",
    "    if 'X_train_music' not in globals() or X_train_music.shape[0] == 0:\n",
    "         print(\"Training data (X_train_music) is not available or empty. Cannot generate a starting sequence.\")\n",
    "         return None, None, None\n",
    "\n",
    "    # Select a random starting sequence and convert to PyTorch tensor, add batch dim, move to device\n",
    "    start_index = np.random.randint(0, X_train_music.shape[0])\n",
    "    start_sequence_np = X_train_music[start_index:start_index+1, :, :]\n",
    "    start_sequence_tensor = torch.FloatTensor(start_sequence_np).to(device)\n",
    "\n",
    "\n",
    "    # Calculate the total number of time steps to generate\n",
    "    # Each time step in the feature sequence corresponds to hop_length samples (default 512 in librosa mfcc)\n",
    "    # Duration of one feature time step = hop_length / sr\n",
    "    # Total time steps needed = target_audio_duration_sec / (hop_length / sr) = target_audio_duration_sec * sr / hop_length\n",
    "    # Assuming default hop_length of 512\n",
    "    hop_length = 512\n",
    "    total_generation_steps = int(target_audio_duration_sec * sr / hop_length)\n",
    "    print(f\"Targeting {target_audio_duration_sec} seconds, which requires approximately {total_generation_steps} generation steps.\")\n",
    "\n",
    "\n",
    "    # Generate new musical features iteratively using the trained PyTorch conditional model\n",
    "    generated_musical_features_np = generate_musical_features_iterative_pytorch(\n",
    "        model,\n",
    "        start_sequence_tensor,\n",
    "        text_conditioning_input_for_model,\n",
    "        audio_conditioning_input_for_model,\n",
    "        total_generation_steps\n",
    "    )\n",
    "\n",
    "\n",
    "    if generated_musical_features_np is None:\n",
    "        print(\"Feature generation failed.\")\n",
    "        return None, generated_lyrics, None # Return None for audio path and features\n",
    "\n",
    "\n",
    "    # Convert the generated musical features to audio and save\n",
    "    generated_audio_file = convert_features_to_audio(generated_musical_features_np, sr=sr)\n",
    "\n",
    "\n",
    "    # Optionally, save the generated lyrics to a text file if lyrics were generated\n",
    "    lyrics_output_filename = 'generated_lyrics.txt'\n",
    "    if generated_lyrics:\n",
    "        try:\n",
    "            with open(lyrics_output_filename, 'w') as f:\n",
    "                f.write(generated_lyrics)\n",
    "            print(f\"Generated lyrics saved to {lyrics_output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving generated lyrics to file: {e}\")\n",
    "\n",
    "\n",
    "    return generated_audio_file, generated_lyrics, generated_musical_features_np # Return features\n",
    "\n",
    "\n",
    "# --- Example Usage (requires trained PyTorch model and data) ---\n",
    "# Assume model_gen_conditioned_pytorch is trained and X_train_music is defined.\n",
    "\n",
    "# Example: Generate music from a text prompt and user audio, aiming for 30 seconds\n",
    "user_text_prompt = \"Generate a piece of background music.\"\n",
    "# IMPORTANT: Replace '/content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg' with the actual path to your uploaded audio file\n",
    "# Ensure the audio file exists before running\n",
    "user_audio_input_path = '/content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg'\n",
    "if not os.path.exists(user_audio_input_path):\n",
    "    print(f\"Error: User audio file not found at {user_audio_input_path}. Please upload the file.\")\n",
    "    user_audio_input_path = None # Set to None if file doesn't exist\n",
    "\n",
    "target_duration_seconds = 30\n",
    "\n",
    "if user_audio_input_path is not None and 'model_gen_conditioned_pytorch' in globals() and 'X_train_music' in globals() and X_train_music.shape[0] > 0:\n",
    "    print(f\"\\nAttempting to generate music from text prompt: '{user_text_prompt}' using audio file at {user_audio_input_path}, targeting {target_duration_seconds} seconds.\")\n",
    "    generated_music_file, generated_lyrics_text, generated_features_np = generate_music_from_input_v4(\n",
    "        model_gen_conditioned_pytorch, # Use the PyTorch model\n",
    "        user_text_input=user_text_prompt,\n",
    "        user_audio_input_path=user_audio_input_path,\n",
    "        input_type='text_prompt',\n",
    "        target_audio_duration_sec=target_duration_seconds,\n",
    "        sr=sr,\n",
    "        max_padding=max_padding,\n",
    "        n_mfcc=n_mfcc,\n",
    "        n_chroma=n_chroma,\n",
    "        text_conditioning_dim=text_conditioning_dim\n",
    "    )\n",
    "\n",
    "\n",
    "    if generated_music_file:\n",
    "        print(f\"\\nGenerated music file: {generated_music_file}\")\n",
    "        if generated_lyrics_text:\n",
    "            print(f\"Generated lyrics:\\n{generated_lyrics_text}\")\n",
    "        # Display links to the generated files\n",
    "        print(\"\\nDownload the generated music file:\")\n",
    "        display(FileLink(generated_music_file))\n",
    "        if generated_lyrics_text: # Only display lyrics link if lyrics were generated\n",
    "            print(\"Download the generated lyrics file:\")\n",
    "            display(FileLink('generated_lyrics.txt'))\n",
    "\n",
    "        # Play the generated audio\n",
    "        print(f\"\\nPlaying generated audio file: {generated_music_file}\")\n",
    "        display(Audio(generated_music_file))\n",
    "\n",
    "        # Step 4: Convert generated features to symbolic music (music21)\n",
    "        if generated_features_np is not None:\n",
    "            music21_stream = convert_features_to_music21(generated_features_np, sr=sr, hop_length=512) # Pass hop_length\n",
    "\n",
    "            # Step 5: Save and display symbolic music (music21)\n",
    "            if music21_stream:\n",
    "                generated_midi_file = save_and_display_music21_stream(music21_stream)\n",
    "                if generated_midi_file:\n",
    "                    print(\"\\nDownload the generated MIDI file:\")\n",
    "                    display(FileLink(generated_midi_file))\n",
    "        else:\n",
    "            print(\"\\nNo generated features available for music21 conversion.\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"\\nMusic generation failed.\")\n",
    "else:\n",
    "    if user_audio_input_path is None:\n",
    "         print(\"\\nSkipping music generation as user audio file was not found.\")\n",
    "    else:\n",
    "         print(\"\\nSkipping music generation. PyTorch model or training data not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "76610152",
    "outputId": "a0947f79-4f49-473a-ff9d-3d08657f07e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generation pipeline (distilgpt2) initialized.\n",
      "Using device: cpu\n",
      "\n",
      "PyTorch conditional music generation model architecture defined.\n",
      "MusicGenerator(\n",
      "  (gru1): GRU(180, 128, batch_first=True, bidirectional=True)\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (gru2): GRU(256, 128, batch_first=True, bidirectional=True)\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  (linear): Linear(in_features=256, out_features=20, bias=True)\n",
      ")\n",
      "Downloading the dataset to ensure path is available...\n",
      "Using Colab cache for faster access to the 'musical-instruments-sound-dataset' dataset.\n",
      "Dataset path: /kaggle/input/musical-instruments-sound-dataset\n",
      "Searching for music instrument files in: /kaggle/input/musical-instruments-sound-dataset/Train_submission/Train_submission\n",
      "Found 1965 music instrument files in training submission.\n",
      "Searching for music instrument files in: /kaggle/input/musical-instruments-sound-dataset/Test_submission/Test_submission\n",
      "Found 2029 music instrument files in test submission.\n",
      "\n",
      "Extracting musical features (MFCCs) and audio conditioning features (MFCC+Chroma) from music instrument files...\n",
      "Processing file 100/2029\n",
      "Processing file 200/2029\n",
      "Processing file 300/2029\n",
      "Processing file 400/2029\n",
      "Processing file 500/2029\n",
      "Processing file 600/2029\n",
      "Processing file 700/2029\n",
      "Processing file 800/2029\n",
      "Processing file 900/2029\n",
      "Processing file 1000/2029\n",
      "Processing file 1100/2029\n",
      "Processing file 1200/2029\n",
      "Processing file 1300/2029\n",
      "Processing file 1400/2029\n",
      "Processing file 1500/2029\n",
      "Processing file 1600/2029\n",
      "Processing file 1700/2029\n",
      "Processing file 1800/2029\n",
      "Processing file 1900/2029\n",
      "Processing file 2000/2029\n",
      "\n",
      "Feature extraction and data splitting complete.\n",
      "Shape of training musical features (MFCCs): (1623, 174, 20)\n",
      "Shape of validation musical features (MFCCs): (406, 174, 20)\n",
      "Shape of training audio conditioning features (MFCC+Chroma): (1623, 174, 32)\n",
      "Shape of validation audio conditioning features (MFCC+Chroma): (406, 174, 32)\n",
      "\n",
      "Shape of dummy training text conditioning data: (1623, 128)\n",
      "Shape of dummy validation text conditioning data: (406, 128)\n",
      "\n",
      "Data converted to PyTorch tensors and DataLoaders created for training.\n",
      "\n",
      "Starting PyTorch model training for 50 epochs...\n",
      "Epoch [1/50], Step [10/51], Loss: 13829.5469\n",
      "Epoch [1/50], Step [20/51], Loss: 14148.8203\n",
      "Epoch [1/50], Step [30/51], Loss: 12346.6162\n",
      "Epoch [1/50], Step [40/51], Loss: 12142.7939\n",
      "Epoch [1/50], Step [50/51], Loss: 13112.3779\n",
      "Epoch [1/50] finished. Average Training Loss: 13841.5449\n",
      "Validation Loss: 13419.1780\n",
      "Epoch [2/50], Step [10/51], Loss: 12475.4033\n",
      "Epoch [2/50], Step [20/51], Loss: 14587.3955\n",
      "Epoch [2/50], Step [30/51], Loss: 14058.4131\n",
      "Epoch [2/50], Step [40/51], Loss: 13505.2734\n",
      "Epoch [2/50], Step [50/51], Loss: 10257.6113\n",
      "Epoch [2/50] finished. Average Training Loss: 13067.7435\n",
      "Validation Loss: 12717.4675\n",
      "Epoch [3/50], Step [10/51], Loss: 11855.4238\n",
      "Epoch [3/50], Step [20/51], Loss: 11846.6445\n",
      "Epoch [3/50], Step [30/51], Loss: 12871.7744\n",
      "Epoch [3/50], Step [40/51], Loss: 12002.0693\n",
      "Epoch [3/50], Step [50/51], Loss: 11677.4824\n",
      "Epoch [3/50] finished. Average Training Loss: 12383.4151\n",
      "Validation Loss: 12069.1750\n",
      "Epoch [4/50], Step [10/51], Loss: 12708.5527\n",
      "Epoch [4/50], Step [20/51], Loss: 13694.9375\n",
      "Epoch [4/50], Step [30/51], Loss: 10363.7070\n",
      "Epoch [4/50], Step [40/51], Loss: 12654.6113\n",
      "Epoch [4/50], Step [50/51], Loss: 11470.4043\n",
      "Epoch [4/50] finished. Average Training Loss: 11766.4149\n",
      "Validation Loss: 11469.1856\n",
      "Epoch [5/50], Step [10/51], Loss: 13483.8779\n",
      "Epoch [5/50], Step [20/51], Loss: 10613.2900\n",
      "Epoch [5/50], Step [30/51], Loss: 11754.4229\n",
      "Epoch [5/50], Step [40/51], Loss: 9762.3301\n",
      "Epoch [5/50], Step [50/51], Loss: 12512.8330\n",
      "Epoch [5/50] finished. Average Training Loss: 11195.5430\n",
      "Validation Loss: 10910.0398\n",
      "Epoch [6/50], Step [10/51], Loss: 11042.6934\n",
      "Epoch [6/50], Step [20/51], Loss: 9636.1641\n",
      "Epoch [6/50], Step [30/51], Loss: 10880.5234\n",
      "Epoch [6/50], Step [40/51], Loss: 10764.7988\n",
      "Epoch [6/50], Step [50/51], Loss: 11551.5439\n",
      "Epoch [6/50] finished. Average Training Loss: 10640.8238\n",
      "Validation Loss: 10381.5543\n",
      "Epoch [7/50], Step [10/51], Loss: 11673.1729\n",
      "Epoch [7/50], Step [20/51], Loss: 11721.3203\n",
      "Epoch [7/50], Step [30/51], Loss: 9698.1299\n",
      "Epoch [7/50], Step [40/51], Loss: 9090.1895\n",
      "Epoch [7/50], Step [50/51], Loss: 9424.9141\n",
      "Epoch [7/50] finished. Average Training Loss: 10122.3533\n",
      "Validation Loss: 9882.2613\n",
      "Epoch [8/50], Step [10/51], Loss: 8479.1572\n",
      "Epoch [8/50], Step [20/51], Loss: 9825.3232\n",
      "Epoch [8/50], Step [30/51], Loss: 11162.5430\n",
      "Epoch [8/50], Step [40/51], Loss: 9202.2793\n",
      "Epoch [8/50], Step [50/51], Loss: 10137.6299\n",
      "Epoch [8/50] finished. Average Training Loss: 9639.0278\n",
      "Validation Loss: 9412.4894\n",
      "Epoch [9/50], Step [10/51], Loss: 9953.2920\n",
      "Epoch [9/50], Step [20/51], Loss: 9798.5801\n",
      "Epoch [9/50], Step [30/51], Loss: 9887.9434\n",
      "Epoch [9/50], Step [40/51], Loss: 10413.1924\n",
      "Epoch [9/50], Step [50/51], Loss: 9331.4648\n",
      "Epoch [9/50] finished. Average Training Loss: 9193.1691\n",
      "Validation Loss: 8965.1282\n",
      "Epoch [10/50], Step [10/51], Loss: 9663.9414\n",
      "Epoch [10/50], Step [20/51], Loss: 8804.6309\n",
      "Epoch [10/50], Step [30/51], Loss: 10063.7275\n",
      "Epoch [10/50], Step [40/51], Loss: 10769.2910\n",
      "Epoch [10/50], Step [50/51], Loss: 8335.0742\n",
      "Epoch [10/50] finished. Average Training Loss: 8733.3609\n",
      "Validation Loss: 8537.8585\n",
      "Epoch [11/50], Step [10/51], Loss: 6724.9102\n",
      "Epoch [11/50], Step [20/51], Loss: 8551.3643\n",
      "Epoch [11/50], Step [30/51], Loss: 8619.7695\n",
      "Epoch [11/50], Step [40/51], Loss: 8535.8398\n",
      "Epoch [11/50], Step [50/51], Loss: 10592.8818\n",
      "Epoch [11/50] finished. Average Training Loss: 8335.0381\n",
      "Validation Loss: 8136.0752\n",
      "Epoch [12/50], Step [10/51], Loss: 7743.5391\n",
      "Epoch [12/50], Step [20/51], Loss: 6330.1289\n",
      "Epoch [12/50], Step [30/51], Loss: 7060.5190\n",
      "Epoch [12/50], Step [40/51], Loss: 8397.8848\n",
      "Epoch [12/50], Step [50/51], Loss: 8118.7393\n",
      "Epoch [12/50] finished. Average Training Loss: 7936.2570\n",
      "Validation Loss: 7748.4723\n",
      "Epoch [13/50], Step [10/51], Loss: 7646.4185\n",
      "Epoch [13/50], Step [20/51], Loss: 6550.4883\n",
      "Epoch [13/50], Step [30/51], Loss: 9013.2139\n",
      "Epoch [13/50], Step [40/51], Loss: 7434.7046\n",
      "Epoch [13/50], Step [50/51], Loss: 8157.4907\n",
      "Epoch [13/50] finished. Average Training Loss: 7561.8173\n",
      "Validation Loss: 7381.5886\n",
      "Epoch [14/50], Step [10/51], Loss: 5936.3779\n",
      "Epoch [14/50], Step [20/51], Loss: 6754.3335\n",
      "Epoch [14/50], Step [30/51], Loss: 6651.5786\n",
      "Epoch [14/50], Step [40/51], Loss: 7695.6318\n",
      "Epoch [14/50], Step [50/51], Loss: 5871.0552\n",
      "Epoch [14/50] finished. Average Training Loss: 7188.8247\n",
      "Validation Loss: 7029.6544\n",
      "Epoch [15/50], Step [10/51], Loss: 7443.5459\n",
      "Epoch [15/50], Step [20/51], Loss: 5409.6064\n",
      "Epoch [15/50], Step [30/51], Loss: 6155.3203\n",
      "Epoch [15/50], Step [40/51], Loss: 7014.4258\n",
      "Epoch [15/50], Step [50/51], Loss: 7685.1460\n",
      "Epoch [15/50] finished. Average Training Loss: 6846.9278\n",
      "Validation Loss: 6696.9297\n",
      "Epoch [16/50], Step [10/51], Loss: 6143.2861\n",
      "Epoch [16/50], Step [20/51], Loss: 6712.9067\n",
      "Epoch [16/50], Step [30/51], Loss: 6592.8062\n",
      "Epoch [16/50], Step [40/51], Loss: 5062.3228\n",
      "Epoch [16/50], Step [50/51], Loss: 5588.5254\n",
      "Epoch [16/50] finished. Average Training Loss: 6518.3260\n",
      "Validation Loss: 6378.6142\n",
      "Epoch [17/50], Step [10/51], Loss: 5970.6763\n",
      "Epoch [17/50], Step [20/51], Loss: 6020.4126\n",
      "Epoch [17/50], Step [30/51], Loss: 5414.0220\n",
      "Epoch [17/50], Step [40/51], Loss: 6952.9946\n",
      "Epoch [17/50], Step [50/51], Loss: 6144.6367\n",
      "Epoch [17/50] finished. Average Training Loss: 6199.9791\n",
      "Validation Loss: 6076.8482\n",
      "Epoch [18/50], Step [10/51], Loss: 5324.7666\n",
      "Epoch [18/50], Step [20/51], Loss: 6595.4546\n",
      "Epoch [18/50], Step [30/51], Loss: 5976.7144\n",
      "Epoch [18/50], Step [40/51], Loss: 3991.5015\n",
      "Epoch [18/50], Step [50/51], Loss: 5469.9487\n",
      "Epoch [18/50] finished. Average Training Loss: 5922.2266\n",
      "Validation Loss: 5790.2241\n",
      "Epoch [19/50], Step [10/51], Loss: 7573.6016\n",
      "Epoch [19/50], Step [20/51], Loss: 5851.2432\n",
      "Epoch [19/50], Step [30/51], Loss: 5963.1099\n",
      "Epoch [19/50], Step [40/51], Loss: 5349.8398\n",
      "Epoch [19/50], Step [50/51], Loss: 6149.3711\n",
      "Epoch [19/50] finished. Average Training Loss: 5617.1449\n",
      "Validation Loss: 5515.8499\n",
      "Epoch [20/50], Step [10/51], Loss: 5503.0000\n",
      "Epoch [20/50], Step [20/51], Loss: 6095.1016\n",
      "Epoch [20/50], Step [30/51], Loss: 6670.9912\n",
      "Epoch [20/50], Step [40/51], Loss: 6594.1353\n",
      "Epoch [20/50], Step [50/51], Loss: 6030.4624\n",
      "Epoch [20/50] finished. Average Training Loss: 5368.2632\n",
      "Validation Loss: 5258.7315\n",
      "Epoch [21/50], Step [10/51], Loss: 5485.3130\n",
      "Epoch [21/50], Step [20/51], Loss: 4542.7905\n",
      "Epoch [21/50], Step [30/51], Loss: 5027.5073\n",
      "Epoch [21/50], Step [40/51], Loss: 5429.8638\n",
      "Epoch [21/50], Step [50/51], Loss: 5556.4014\n",
      "Epoch [21/50] finished. Average Training Loss: 5108.7455\n",
      "Validation Loss: 5013.2638\n",
      "Epoch [22/50], Step [10/51], Loss: 3623.9282\n",
      "Epoch [22/50], Step [20/51], Loss: 4782.8809\n",
      "Epoch [22/50], Step [30/51], Loss: 3400.8621\n",
      "Epoch [22/50], Step [40/51], Loss: 4761.5283\n",
      "Epoch [22/50], Step [50/51], Loss: 5054.7754\n",
      "Epoch [22/50] finished. Average Training Loss: 4859.7667\n",
      "Validation Loss: 4779.4584\n",
      "Epoch [23/50], Step [10/51], Loss: 4499.8696\n",
      "Epoch [23/50], Step [20/51], Loss: 3338.4756\n",
      "Epoch [23/50], Step [30/51], Loss: 4224.1777\n",
      "Epoch [23/50], Step [40/51], Loss: 4749.5103\n",
      "Epoch [23/50], Step [50/51], Loss: 4074.4675\n",
      "Epoch [23/50] finished. Average Training Loss: 4635.7480\n",
      "Validation Loss: 4557.7843\n",
      "Epoch [24/50], Step [10/51], Loss: 4176.5522\n",
      "Epoch [24/50], Step [20/51], Loss: 3560.4495\n",
      "Epoch [24/50], Step [30/51], Loss: 4316.2754\n",
      "Epoch [24/50], Step [40/51], Loss: 2613.0540\n",
      "Epoch [24/50], Step [50/51], Loss: 3137.4739\n",
      "Epoch [24/50] finished. Average Training Loss: 4417.8737\n",
      "Validation Loss: 4348.0122\n",
      "Epoch [25/50], Step [10/51], Loss: 3018.0200\n",
      "Epoch [25/50], Step [20/51], Loss: 4779.7041\n",
      "Epoch [25/50], Step [30/51], Loss: 4186.6641\n",
      "Epoch [25/50], Step [40/51], Loss: 3915.5989\n",
      "Epoch [25/50], Step [50/51], Loss: 4495.4458\n",
      "Epoch [25/50] finished. Average Training Loss: 4216.5103\n",
      "Validation Loss: 4151.8297\n",
      "Epoch [26/50], Step [10/51], Loss: 3979.3923\n",
      "Epoch [26/50], Step [20/51], Loss: 4054.2803\n",
      "Epoch [26/50], Step [30/51], Loss: 4907.8950\n",
      "Epoch [26/50], Step [40/51], Loss: 4234.1836\n",
      "Epoch [26/50], Step [50/51], Loss: 3891.0615\n",
      "Epoch [26/50] finished. Average Training Loss: 4023.1669\n",
      "Validation Loss: 3962.9565\n",
      "Epoch [27/50], Step [10/51], Loss: 3373.8289\n",
      "Epoch [27/50], Step [20/51], Loss: 3786.9722\n",
      "Epoch [27/50], Step [30/51], Loss: 4005.8923\n",
      "Epoch [27/50], Step [40/51], Loss: 3176.1538\n",
      "Epoch [27/50], Step [50/51], Loss: 3179.2754\n",
      "Epoch [27/50] finished. Average Training Loss: 3838.6319\n",
      "Validation Loss: 3785.0172\n",
      "Epoch [28/50], Step [10/51], Loss: 3245.5959\n",
      "Epoch [28/50], Step [20/51], Loss: 4612.6650\n",
      "Epoch [28/50], Step [30/51], Loss: 4164.5591\n",
      "Epoch [28/50], Step [40/51], Loss: 3196.5574\n",
      "Epoch [28/50], Step [50/51], Loss: 3618.3474\n",
      "Epoch [28/50] finished. Average Training Loss: 3668.5472\n",
      "Validation Loss: 3617.6387\n",
      "Epoch [29/50], Step [10/51], Loss: 4132.8047\n",
      "Epoch [29/50], Step [20/51], Loss: 3924.8831\n",
      "Epoch [29/50], Step [30/51], Loss: 4445.2944\n",
      "Epoch [29/50], Step [40/51], Loss: 3865.1775\n",
      "Epoch [29/50], Step [50/51], Loss: 3711.5752\n",
      "Epoch [29/50] finished. Average Training Loss: 3498.9683\n",
      "Validation Loss: 3456.9980\n",
      "Epoch [30/50], Step [10/51], Loss: 3095.9380\n",
      "Epoch [30/50], Step [20/51], Loss: 3697.2385\n",
      "Epoch [30/50], Step [30/51], Loss: 2516.1733\n",
      "Epoch [30/50], Step [40/51], Loss: 3502.5352\n",
      "Epoch [30/50], Step [50/51], Loss: 2365.5400\n",
      "Epoch [30/50] finished. Average Training Loss: 3344.7943\n",
      "Validation Loss: 3307.0879\n",
      "Epoch [31/50], Step [10/51], Loss: 2916.8816\n",
      "Epoch [31/50], Step [20/51], Loss: 3192.4524\n",
      "Epoch [31/50], Step [30/51], Loss: 2931.3442\n",
      "Epoch [31/50], Step [40/51], Loss: 3858.4529\n",
      "Epoch [31/50], Step [50/51], Loss: 3124.6775\n",
      "Epoch [31/50] finished. Average Training Loss: 3196.9519\n",
      "Validation Loss: 3163.3175\n",
      "Epoch [32/50], Step [10/51], Loss: 2957.7031\n",
      "Epoch [32/50], Step [20/51], Loss: 4460.1831\n",
      "Epoch [32/50], Step [30/51], Loss: 3949.8762\n",
      "Epoch [32/50], Step [40/51], Loss: 2649.3787\n",
      "Epoch [32/50], Step [50/51], Loss: 3437.0442\n",
      "Epoch [32/50] finished. Average Training Loss: 3060.7845\n",
      "Validation Loss: 3027.2268\n",
      "Epoch [33/50], Step [10/51], Loss: 3098.9368\n",
      "Epoch [33/50], Step [20/51], Loss: 4518.3872\n",
      "Epoch [33/50], Step [30/51], Loss: 2653.5076\n",
      "Epoch [33/50], Step [40/51], Loss: 3087.7041\n",
      "Epoch [33/50], Step [50/51], Loss: 2708.3560\n",
      "Epoch [33/50] finished. Average Training Loss: 2926.0959\n",
      "Validation Loss: 2897.8936\n",
      "Epoch [34/50], Step [10/51], Loss: 3426.8186\n",
      "Epoch [34/50], Step [20/51], Loss: 3047.0859\n",
      "Epoch [34/50], Step [30/51], Loss: 2633.8965\n",
      "Epoch [34/50], Step [40/51], Loss: 3113.8879\n",
      "Epoch [34/50], Step [50/51], Loss: 2616.2122\n",
      "Epoch [34/50] finished. Average Training Loss: 2794.9956\n",
      "Validation Loss: 2772.1695\n",
      "Epoch [35/50], Step [10/51], Loss: 2951.2432\n",
      "Epoch [35/50], Step [20/51], Loss: 3150.6687\n",
      "Epoch [35/50], Step [30/51], Loss: 2758.9468\n",
      "Epoch [35/50], Step [40/51], Loss: 2893.9272\n",
      "Epoch [35/50], Step [50/51], Loss: 2767.1899\n",
      "Epoch [35/50] finished. Average Training Loss: 2668.0691\n",
      "Validation Loss: 2652.5395\n",
      "Epoch [36/50], Step [10/51], Loss: 2193.1411\n",
      "Epoch [36/50], Step [20/51], Loss: 2063.9553\n",
      "Epoch [36/50], Step [30/51], Loss: 4099.6777\n",
      "Epoch [36/50], Step [40/51], Loss: 2623.9109\n",
      "Epoch [36/50], Step [50/51], Loss: 3411.4094\n",
      "Epoch [36/50] finished. Average Training Loss: 2552.9416\n",
      "Validation Loss: 2538.3744\n",
      "Epoch [37/50], Step [10/51], Loss: 3315.8354\n",
      "Epoch [37/50], Step [20/51], Loss: 1655.9755\n",
      "Epoch [37/50], Step [30/51], Loss: 1786.0770\n",
      "Epoch [37/50], Step [40/51], Loss: 2925.6189\n",
      "Epoch [37/50], Step [50/51], Loss: 2327.2747\n",
      "Epoch [37/50] finished. Average Training Loss: 2437.3625\n",
      "Validation Loss: 2425.8869\n",
      "Epoch [38/50], Step [10/51], Loss: 2217.4700\n",
      "Epoch [38/50], Step [20/51], Loss: 2065.4241\n",
      "Epoch [38/50], Step [30/51], Loss: 2351.6626\n",
      "Epoch [38/50], Step [40/51], Loss: 2667.7285\n",
      "Epoch [38/50], Step [50/51], Loss: 2109.1279\n",
      "Epoch [38/50] finished. Average Training Loss: 2335.3124\n",
      "Validation Loss: 2320.9110\n",
      "Epoch [39/50], Step [10/51], Loss: 1607.3625\n",
      "Epoch [39/50], Step [20/51], Loss: 2576.5439\n",
      "Epoch [39/50], Step [30/51], Loss: 2455.9167\n",
      "Epoch [39/50], Step [40/51], Loss: 2418.8052\n",
      "Epoch [39/50], Step [50/51], Loss: 2090.6707\n",
      "Epoch [39/50] finished. Average Training Loss: 2229.1472\n",
      "Validation Loss: 2217.9908\n",
      "Epoch [40/50], Step [10/51], Loss: 2091.9243\n",
      "Epoch [40/50], Step [20/51], Loss: 2066.0798\n",
      "Epoch [40/50], Step [30/51], Loss: 2195.7600\n",
      "Epoch [40/50], Step [40/51], Loss: 1896.7969\n",
      "Epoch [40/50], Step [50/51], Loss: 1861.8889\n",
      "Epoch [40/50] finished. Average Training Loss: 2129.0175\n",
      "Validation Loss: 2117.9080\n",
      "Epoch [41/50], Step [10/51], Loss: 2562.4016\n",
      "Epoch [41/50], Step [20/51], Loss: 1603.4822\n",
      "Epoch [41/50], Step [30/51], Loss: 2225.8635\n",
      "Epoch [41/50], Step [40/51], Loss: 2266.9885\n",
      "Epoch [41/50], Step [50/51], Loss: 1487.3314\n",
      "Epoch [41/50] finished. Average Training Loss: 2027.7966\n",
      "Validation Loss: 2021.6302\n",
      "Epoch [42/50], Step [10/51], Loss: 1550.7410\n",
      "Epoch [42/50], Step [20/51], Loss: 1788.5001\n",
      "Epoch [42/50], Step [30/51], Loss: 1752.4707\n",
      "Epoch [42/50], Step [40/51], Loss: 1648.1687\n",
      "Epoch [42/50], Step [50/51], Loss: 2233.1860\n",
      "Epoch [42/50] finished. Average Training Loss: 1937.1624\n",
      "Validation Loss: 1929.4436\n",
      "Epoch [43/50], Step [10/51], Loss: 2488.8257\n",
      "Epoch [43/50], Step [20/51], Loss: 2068.0723\n",
      "Epoch [43/50], Step [30/51], Loss: 2216.4573\n",
      "Epoch [43/50], Step [40/51], Loss: 1714.2834\n",
      "Epoch [43/50], Step [50/51], Loss: 1494.7471\n",
      "Epoch [43/50] finished. Average Training Loss: 1850.6081\n",
      "Validation Loss: 1841.2768\n",
      "Epoch [44/50], Step [10/51], Loss: 1649.9609\n",
      "Epoch [44/50], Step [20/51], Loss: 2237.8347\n",
      "Epoch [44/50], Step [30/51], Loss: 1466.2533\n",
      "Epoch [44/50], Step [40/51], Loss: 1194.3673\n",
      "Epoch [44/50], Step [50/51], Loss: 1121.5779\n",
      "Epoch [44/50] finished. Average Training Loss: 1761.2357\n",
      "Validation Loss: 1756.5799\n",
      "Epoch [45/50], Step [10/51], Loss: 1481.3220\n",
      "Epoch [45/50], Step [20/51], Loss: 1464.2543\n",
      "Epoch [45/50], Step [30/51], Loss: 948.8373\n",
      "Epoch [45/50], Step [40/51], Loss: 1580.8534\n",
      "Epoch [45/50], Step [50/51], Loss: 1489.6237\n",
      "Epoch [45/50] finished. Average Training Loss: 1681.3784\n",
      "Validation Loss: 1675.3942\n",
      "Epoch [46/50], Step [10/51], Loss: 1962.0707\n",
      "Epoch [46/50], Step [20/51], Loss: 1465.6095\n",
      "Epoch [46/50], Step [30/51], Loss: 2077.9956\n",
      "Epoch [46/50], Step [40/51], Loss: 1382.5331\n",
      "Epoch [46/50], Step [50/51], Loss: 1437.7323\n",
      "Epoch [46/50] finished. Average Training Loss: 1605.2810\n",
      "Validation Loss: 1597.2893\n",
      "Epoch [47/50], Step [10/51], Loss: 1159.2025\n",
      "Epoch [47/50], Step [20/51], Loss: 1861.9001\n",
      "Epoch [47/50], Step [30/51], Loss: 883.1938\n",
      "Epoch [47/50], Step [40/51], Loss: 1607.7206\n",
      "Epoch [47/50], Step [50/51], Loss: 1416.7728\n",
      "Epoch [47/50] finished. Average Training Loss: 1523.2480\n",
      "Validation Loss: 1523.7973\n",
      "Epoch [48/50], Step [10/51], Loss: 2147.6724\n",
      "Epoch [48/50], Step [20/51], Loss: 1185.0302\n",
      "Epoch [48/50], Step [30/51], Loss: 1505.2827\n",
      "Epoch [48/50], Step [40/51], Loss: 1098.5417\n",
      "Epoch [48/50], Step [50/51], Loss: 1869.5137\n",
      "Epoch [48/50] finished. Average Training Loss: 1455.1117\n",
      "Validation Loss: 1454.0873\n",
      "Epoch [49/50], Step [10/51], Loss: 1110.0980\n",
      "Epoch [49/50], Step [20/51], Loss: 1338.3918\n",
      "Epoch [49/50], Step [30/51], Loss: 1786.4835\n",
      "Epoch [49/50], Step [40/51], Loss: 1561.4481\n",
      "Epoch [49/50], Step [50/51], Loss: 1103.9316\n",
      "Epoch [49/50] finished. Average Training Loss: 1386.1460\n",
      "Validation Loss: 1384.6993\n",
      "Epoch [50/50], Step [10/51], Loss: 1899.4928\n",
      "Epoch [50/50], Step [20/51], Loss: 1173.0646\n",
      "Epoch [50/50], Step [30/51], Loss: 714.9165\n",
      "Epoch [50/50], Step [40/51], Loss: 1382.6129\n",
      "Epoch [50/50], Step [50/51], Loss: 1000.9269\n",
      "Epoch [50/50] finished. Average Training Loss: 1322.1536\n",
      "Validation Loss: 1320.5689\n",
      "\n",
      "PyTorch model training complete.\n",
      "Error: User audio file not found at /content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg. Please upload the file.\n",
      "\n",
      "Skipping music generation as user audio file was not found.\n"
     ]
    }
   ],
   "source": [
    "# Regenerate the content of cell 'a1b11bff' to ensure functions and model are defined\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "from pydub import AudioSegment # Import AudioSegment for MP3 conversion\n",
    "from transformers import pipeline # Import pipeline for using pre-trained models\n",
    "from IPython.display import display, FileLink, Audio # Import display, FileLink, and Audio\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split for data splitting\n",
    "import kagglehub # Import kagglehub for dataset download\n",
    "import tensorflow as tf # Import tensorflow\n",
    "from tensorflow.keras.models import Model # Import Model\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Dropout, GRU, Input, TimeDistributed, Bidirectional # Import necessary layers\n",
    "import torch # Import PyTorch\n",
    "import torch.nn as nn # Import PyTorch neural network module\n",
    "from music21 import stream, note, duration, tempo # Import music21 elements\n",
    "\n",
    "\n",
    "# Define sampling rate (needed for audio processing and saving)\n",
    "sr = 22050 # Common sampling rate, adjust if your audio files use a different rate\n",
    "\n",
    "# Define feature extraction parameters (can be adjusted)\n",
    "n_mfcc = 20\n",
    "max_padding = 174 # This needs to be consistent with feature extraction\n",
    "n_chroma = 12 # Number of chroma bins\n",
    "total_audio_features_dim = n_mfcc + n_chroma # Total features for audio conditioning\n",
    "\n",
    "# Define the dimensions for the music generation model\n",
    "musical_feature_dim = n_mfcc # The model output is MFCCs\n",
    "musical_sequence_length = max_padding # The model input sequence length\n",
    "\n",
    "# Define the conditioning input dimensions\n",
    "text_conditioning_dim = 128 # Dimension of the text embedding\n",
    "\n",
    "# --- Helper functions ---\n",
    "\n",
    "# Initialize the text generation pipeline (if not already initialized)\n",
    "lyric_generator = None # Initialize to None\n",
    "try:\n",
    "    lyric_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "    print(\"Text generation pipeline (distilgpt2) initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing text generation pipeline: {e}\")\n",
    "    lyric_generator = None\n",
    "\n",
    "\n",
    "def generate_lyrics_with_language_model(prompt):\n",
    "    \"\"\"Generates lyrics based on a prompt using a language model.\"\"\"\n",
    "    if lyric_generator:\n",
    "        print(f\"Generating lyrics for prompt: '{prompt}' using DistilGPT-2.\")\n",
    "        try:\n",
    "            # Generate text with a reasonable length and without generating too many sequences\n",
    "            generated_text = lyric_generator(prompt, max_length=100, num_return_sequences=1, truncation=True)\n",
    "            lyrics = generated_text[0]['generated_text']\n",
    "            # Basic post-processing: remove the prompt from the generated text if it's repeated\n",
    "            if lyrics.startswith(prompt):\n",
    "                lyrics = lyrics[len(prompt):].strip()\n",
    "            print(\"Lyric generation successful.\")\n",
    "            return lyrics\n",
    "        except Exception as e:\n",
    "            print(f\"Error during lyric generation: {e}\")\n",
    "            return f\"Error generating lyrics for prompt: '{prompt}'\" # Return an error message\n",
    "    else:\n",
    "        print(\"Lyric generator not initialized. Cannot generate lyrics.\")\n",
    "        return f\"Cannot generate lyrics for prompt: '{prompt}' (generator not available)\"\n",
    "\n",
    "\n",
    "# Function to get text embeddings (placeholder)\n",
    "def get_text_embedding(text, embedding_dim):\n",
    "    \"\"\"Placeholder function to get a fixed-size embedding for text.\"\"\"\n",
    "    print(f\"Getting embedding for text: '{text[:50]}...' using a placeholder model.\")\n",
    "    # Return as numpy array for now, will convert to tensor later in process_text_input\n",
    "    return np.random.rand(1, embedding_dim) # Shape (1, embedding_dim)\n",
    "\n",
    "\n",
    "# Function to process text input\n",
    "def process_text_input(user_input, embedding_dim, is_prompt=False):\n",
    "    \"\"\"Processes user text input (lyrics or prompt) and returns a conditioning embedding.\"\"\"\n",
    "    if is_prompt:\n",
    "        lyrics = generate_lyrics_with_language_model(user_input)\n",
    "    else:\n",
    "        lyrics = user_input # User provided lyrics directly\n",
    "\n",
    "    conditioning_embedding_np = get_text_embedding(lyrics, embedding_dim)\n",
    "\n",
    "    # Convert to PyTorch tensor and move to device\n",
    "    conditioning_embedding_tensor = torch.FloatTensor(conditioning_embedding_np).to(device)\n",
    "\n",
    "\n",
    "    return conditioning_embedding_tensor, lyrics # Return as PyTorch tensor\n",
    "\n",
    "\n",
    "# Function to extract combined features (MFCC + Chroma) from an audio file.\n",
    "# This function is used for both training data preparation and user audio conditioning.\n",
    "def extract_features(file_path, target_sequence_length, n_mfcc=20, n_chroma=12):\n",
    "    \"\"\"Extract MFCC and Chroma features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract Chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_chroma=n_chroma)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = np.concatenate((mfccs, chroma), axis=0)\n",
    "\n",
    "        # Pad or truncate combined features to a fixed length\n",
    "        if combined_features.shape[1] < target_sequence_length:\n",
    "            combined_features = np.pad(combined_features, ((0, 0), (0, target_sequence_length - combined_features.shape[1])), mode='constant')\n",
    "        else:\n",
    "            combined_features = combined_features[:, :target_sequence_length]\n",
    "\n",
    "        return combined_features.T # Return as numpy array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process user audio for conditioning - calls extract_features\n",
    "def process_user_audio_for_conditioning(audio_file_path, target_sequence_length, n_mfcc, n_chroma):\n",
    "    \"\"\"Processes user audio input and returns features suitable for conditioning.\"\"\"\n",
    "    processed_features_np = extract_features(audio_file_path, target_sequence_length, n_mfcc, n_chroma)\n",
    "\n",
    "    if processed_features_np is not None:\n",
    "        # Convert to PyTorch tensor, add a batch dimension, and move to device\n",
    "        processed_features_tensor = torch.FloatTensor(processed_features_np).unsqueeze(0).to(device) # Shape (1, target_sequence_length, total_feature_dimension)\n",
    "        print(f\"Processed user audio features shape for conditioning: {processed_features_tensor.shape}\")\n",
    "        return processed_features_tensor\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to extract MFCC features only (used for model input/output targets)\n",
    "def extract_mfcc_features_only(file_path, target_sequence_length, n_mfcc=20):\n",
    "    \"\"\"Extract only MFCC features from an audio file.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None) # Load with original sampling rate\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Pad or truncate MFCCs to a fixed length\n",
    "        if mfccs.shape[1] < target_sequence_length:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, target_sequence_length - mfccs.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :target_sequence_length]\n",
    "\n",
    "        return mfccs.T # Return as numpy array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Model Definition (PyTorch) ---\n",
    "\n",
    "# Define the PyTorch model\n",
    "class MusicGenerator(nn.Module):\n",
    "    def __init__(self, musical_feature_dim, total_audio_features_dim, text_conditioning_dim, hidden_dim=128):\n",
    "        super(MusicGenerator, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.musical_feature_dim = musical_feature_dim\n",
    "        self.total_audio_features_dim = total_audio_features_dim\n",
    "        self.text_conditioning_dim = text_conditioning_dim\n",
    "\n",
    "        # GRU layers\n",
    "        # Input to GRU will be concatenation of musical features, text conditioning, and audio conditioning\n",
    "        # Dimension of combined input: musical_feature_dim + text_conditioning_dim + total_audio_features_dim\n",
    "        self.gru1 = nn.GRU(musical_feature_dim + text_conditioning_dim + total_audio_features_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.gru2 = nn.GRU(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True) # *2 for bidirectional output\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        # Output layer: predicts musical features for each time step\n",
    "        # Since the GRU returns sequences, we use a TimeDistributed-like approach with a linear layer\n",
    "        self.linear = nn.Linear(hidden_dim * 2, musical_feature_dim) # *2 for bidirectional output\n",
    "\n",
    "    def forward(self, musical_input, text_conditioning_input, audio_conditioning_input):\n",
    "        # musical_input shape: (batch_size, sequence_length, musical_feature_dim)\n",
    "        # text_conditioning_input shape: (batch_size, text_conditioning_dim)\n",
    "        # audio_conditioning_input shape: (batch_size, sequence_length, total_audio_features_dim)\n",
    "\n",
    "        # Repeat text conditioning across the sequence length\n",
    "        # text_conditioning_input shape: (batch_size, 1, text_conditioning_dim)\n",
    "        text_conditioning_repeated = text_conditioning_input.unsqueeze(1).repeat(1, musical_input.size(1), 1)\n",
    "        # text_conditioning_repeated shape: (batch_size, sequence_length, text_conditioning_dim)\n",
    "\n",
    "\n",
    "        # Concatenate inputs\n",
    "        combined_input = torch.cat((musical_input, text_conditioning_repeated, audio_conditioning_input), dim=-1)\n",
    "        # combined_input shape: (batch_size, sequence_length, musical_feature_dim + text_conditioning_dim + total_audio_features_dim)\n",
    "\n",
    "        # Pass through GRU layers\n",
    "        gru_out1, _ = self.gru1(combined_input)\n",
    "        dropout_out1 = self.dropout1(gru_out1)\n",
    "\n",
    "        gru_out2, _ = self.gru2(dropout_out1)\n",
    "        dropout_out2 = self.dropout2(gru_out2)\n",
    "\n",
    "        # Apply linear layer to each time step's output\n",
    "        output = self.linear(dropout_out2)\n",
    "        # output shape: (batch_size, sequence_length, musical_feature_dim)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Check if a GPU is available and use it\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Instantiate the PyTorch model\n",
    "model_gen_conditioned_pytorch = MusicGenerator(musical_feature_dim, total_audio_features_dim, text_conditioning_dim)\n",
    "\n",
    "# Move the model to the selected device\n",
    "model_gen_conditioned_pytorch.to(device)\n",
    "\n",
    "print(\"\\nPyTorch conditional music generation model architecture defined.\")\n",
    "print(model_gen_conditioned_pytorch)\n",
    "\n",
    "\n",
    "# --- Data Preparation ---\n",
    "# Download the dataset and directly assign the path within this cell\n",
    "print(\"Downloading the dataset to ensure path is available...\")\n",
    "try:\n",
    "    downloaded_dataset_root = kagglehub.dataset_download(\"soumendraprasad/musical-instruments-sound-dataset\")\n",
    "    print(f\"Dataset path: {downloaded_dataset_root}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    downloaded_dataset_root = None # Set to None if download fails\n",
    "\n",
    "\n",
    "if downloaded_dataset_root is None or not os.path.exists(downloaded_dataset_root):\n",
    "    print(\"Failed to get the dataset path. Cannot proceed with data preparation.\")\n",
    "else:\n",
    "    # Define feature extraction parameters (should be consistent with model definition)\n",
    "    n_mfcc = 20\n",
    "    max_padding = 174\n",
    "    n_chroma = 12 # Although not used for musical features output, needed for audio conditioning feature extraction\n",
    "    total_audio_features_dim = n_mfcc + n_chroma\n",
    "    text_conditioning_dim = 128\n",
    "\n",
    "\n",
    "    # Collect music instrument files from the downloaded dataset\n",
    "    music_instrument_files = []\n",
    "\n",
    "    # Assuming the structure from the previous file listing\n",
    "    train_submission_path = os.path.join(downloaded_dataset_root, 'Train_submission', 'Train_submission')\n",
    "    test_submission_path = os.path.join(downloaded_dataset_root, 'Test_submission', 'Test_submission') # Assuming Test_submission is also relevant for data\n",
    "\n",
    "    # Collect files from Train_submission\n",
    "    if os.path.exists(train_submission_path):\n",
    "        print(f\"Searching for music instrument files in: {train_submission_path}\")\n",
    "        for root, dirs, files in os.walk(train_submission_path):\n",
    "            for name in files:\n",
    "                if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                     # Optional: further filter by keywords if needed\n",
    "                    if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                         music_instrument_files.append(os.path.join(root, name))\n",
    "        print(f\"Found {len(music_instrument_files)} music instrument files in training submission.\")\n",
    "    else:\n",
    "        print(f\"Warning: Training submission directory not found at {train_submission_path}\")\n",
    "\n",
    "\n",
    "    # Collect files from Test_submission (if it exists and contains relevant audio)\n",
    "    # You might decide to use test data only for evaluation later, but for now, let's include relevant audio\n",
    "    if os.path.exists(test_submission_path):\n",
    "         print(f\"Searching for music instrument files in: {test_submission_path}\")\n",
    "         for root, dirs, files in os.walk(test_submission_path):\n",
    "             for name in files:\n",
    "                 if name.endswith('.wav') or name.endswith('.mp3'):\n",
    "                      # Optional: further filter by keywords if needed\n",
    "                     if any(keyword in name.lower() for keyword in ['violin', 'piano', 'guitar', 'drum', 'beat', 'chords', 'mus', 'fs', 'ar', 'lp', 'vn', 'va']):\n",
    "                          music_instrument_files.append(os.path.join(root, name))\n",
    "         print(f\"Found {len(music_instrument_files)} music instrument files in test submission.\")\n",
    "    else:\n",
    "         print(f\"Warning: Test submission directory not found at {test_submission_path}\")\n",
    "\n",
    "\n",
    "    # Use the collected music instrument files for feature extraction\n",
    "    files_to_process_music = music_instrument_files\n",
    "\n",
    "    # Prepare lists to store extracted features for training\n",
    "    X_train_music_features = [] # For musical input (MFCCs only)\n",
    "    X_train_audio_conditioning_features = [] # For audio conditioning input (MFCC + Chroma)\n",
    "\n",
    "\n",
    "    print(\"\\nExtracting musical features (MFCCs) and audio conditioning features (MFCC+Chroma) from music instrument files...\")\n",
    "\n",
    "    # Process a subset of files for faster testing, or all files for full training\n",
    "    # process_limit = 100 # Set a limit for testing\n",
    "    # files_to_process_subset = files_to_process_music[:process_limit]\n",
    "    files_to_process_subset = files_to_process_music # Process all files\n",
    "\n",
    "\n",
    "    for i, file_path in enumerate(files_to_process_subset):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Processing file {i + 1}/{len(files_to_process_subset)}\")\n",
    "\n",
    "        # Extract MFCC features for the musical input/output\n",
    "        mfccs_only = extract_mfcc_features_only(file_path, max_padding, n_mfcc=n_mfcc)\n",
    "\n",
    "        # Extract combined features for the audio conditioning input\n",
    "        combined_features = extract_features(file_path, max_padding, n_mfcc=n_mfcc, n_chroma=n_chroma)\n",
    "\n",
    "\n",
    "        if mfccs_only is not None and combined_features is not None:\n",
    "            X_train_music_features.append(mfccs_only)\n",
    "            X_train_audio_conditioning_features.append(combined_features)\n",
    "\n",
    "\n",
    "    # Convert extracted features to numpy arrays (keeping as numpy for TensorFlow training setup initially)\n",
    "    X_train_music_features = np.array(X_train_music_features)\n",
    "    X_train_audio_conditioning_features = np.array(X_train_audio_conditioning_features)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    # Ensure that the split is consistent for both sets of features\n",
    "    if len(X_train_music_features) > 0:\n",
    "        X_train_music, X_val_music, X_train_audio_cond, X_val_audio_cond = train_test_split(\n",
    "            X_train_music_features, X_train_audio_conditioning_features, test_size=0.2, random_state=42\n",
    "        )\n",
    "    else:\n",
    "        print(\"No features were successfully extracted. Cannot split data.\")\n",
    "        # Initialize empty arrays to prevent errors in subsequent steps\n",
    "        X_train_music = np.array([])\n",
    "        X_val_music = np.array([])\n",
    "        X_train_audio_cond = np.array([])\n",
    "        X_val_audio_cond = np.array([])\n",
    "\n",
    "\n",
    "    print(\"\\nFeature extraction and data splitting complete.\")\n",
    "    print(f\"Shape of training musical features (MFCCs): {X_train_music.shape}\")\n",
    "    print(f\"Shape of validation musical features (MFCCs): {X_val_music.shape}\")\n",
    "    print(f\"Shape of training audio conditioning features (MFCC+Chroma): {X_train_audio_cond.shape}\")\n",
    "    print(f\"Shape of validation audio conditioning features (MFCC+Chroma): {X_val_audio_cond.shape}\")\n",
    "\n",
    "    # Create dummy text conditioning data for training\n",
    "    # This data is not based on actual text input yet, just for model training setup\n",
    "    # It should match the expected shape of the text_conditioning_input layer\n",
    "    dummy_text_conditioning_train = np.random.rand(X_train_music.shape[0], text_conditioning_dim)\n",
    "    dummy_text_conditioning_val = np.random.rand(X_val_music.shape[0], text_conditioning_dim)\n",
    "\n",
    "\n",
    "    print(f\"\\nShape of dummy training text conditioning data: {dummy_text_conditioning_train.shape}\")\n",
    "    print(f\"Shape of dummy validation text conditioning data: {dummy_text_conditioning_val.shape}\")\n",
    "\n",
    "\n",
    "    # --- Train the Model (PyTorch) ---\n",
    "    # Check if training data is available before training\n",
    "    if 'X_train_music' in globals() and X_train_music.shape[0] > 0:\n",
    "        # Convert NumPy data to PyTorch tensors and move to device\n",
    "        X_train_music_tensor = torch.FloatTensor(X_train_music).to(device)\n",
    "        X_val_music_tensor = torch.FloatTensor(X_val_music).to(device)\n",
    "        X_train_audio_cond_tensor = torch.FloatTensor(X_train_audio_cond).to(device)\n",
    "        X_val_audio_cond_tensor = torch.FloatTensor(X_val_audio_cond).to(device)\n",
    "        dummy_text_conditioning_train_tensor = torch.FloatTensor(dummy_text_conditioning_train).to(device)\n",
    "        dummy_text_conditioning_val_tensor = torch.FloatTensor(dummy_text_conditioning_val).to(device)\n",
    "\n",
    "        # Create PyTorch DataLoaders\n",
    "        from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_music_tensor, dummy_text_conditioning_train_tensor, X_train_audio_cond_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        val_dataset = TensorDataset(X_val_music_tensor, dummy_text_conditioning_val_tensor, X_val_audio_cond_tensor)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32) # No need to shuffle validation data\n",
    "\n",
    "        print(\"\\nData converted to PyTorch tensors and DataLoaders created for training.\")\n",
    "\n",
    "        # Define Loss Function and Optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer_pytorch = torch.optim.Adam(model_gen_conditioned_pytorch.parameters(), lr=0.001) # You can adjust the learning rate\n",
    "\n",
    "\n",
    "        num_epochs = 50 # You can adjust the number of epochs\n",
    "        print(f\"\\nStarting PyTorch model training for {num_epochs} epochs...\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model_gen_conditioned_pytorch.train() # Set the model to training mode\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for i, (musical_inputs, text_cond_inputs, audio_cond_inputs) in enumerate(train_loader):\n",
    "                # Zero the parameter gradients\n",
    "                optimizer_pytorch.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model_gen_conditioned_pytorch(musical_inputs, text_cond_inputs, audio_cond_inputs)\n",
    "\n",
    "                # Calculate loss\n",
    "                # The target for musical_inputs is the same as the input in this autoencoder-like setup\n",
    "                loss = criterion(outputs, musical_inputs)\n",
    "\n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer_pytorch.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Print statistics\n",
    "                if (i + 1) % 10 == 0: # Print every 10 batches\n",
    "                    print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}] finished. Average Training Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "            # Validation step\n",
    "            model_gen_conditioned_pytorch.eval() # Set the model to evaluation mode\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad(): # Disable gradient calculation for validation\n",
    "                for musical_inputs_val, text_cond_inputs_val, audio_cond_inputs_val in val_loader:\n",
    "                    outputs_val = model_gen_conditioned_pytorch(musical_inputs_val, text_cond_inputs_val, audio_cond_inputs_val)\n",
    "                    loss_val = criterion(outputs_val, musical_inputs_val)\n",
    "                    val_loss += loss_val.item()\n",
    "\n",
    "            print(f'Validation Loss: {val_loss / len(val_loader):.4f}')\n",
    "\n",
    "        print(\"\\nPyTorch model training complete.\")\n",
    "    else:\n",
    "        print(\"Training data is empty. Skipping PyTorch model training.\")\n",
    "\n",
    "\n",
    "# --- Function to generate musical features iteratively (PyTorch) ---\n",
    "def generate_musical_features_iterative_pytorch(model, start_sequence, text_conditioning_input, audio_conditioning_input, total_generation_steps):\n",
    "    \"\"\"Generates new MFCC sequences step-by-step using the trained PyTorch conditional model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained PyTorch conditional generation model.\n",
    "        start_sequence: A PyTorch tensor representing the initial musical sequence\n",
    "                        (shape: (1, sequence_length, musical_feature_dim)).\n",
    "        text_conditioning_input: The text conditioning input (shape: (1, text_conditioning_dim)).\n",
    "        audio_conditioning_input: The audio conditioning input (sequence of features,\n",
    "                                   shape: (1, sequence_length, total_audio_features_dim)).\n",
    "                                   This is the *full* audio conditioning sequence.\n",
    "        total_generation_steps: The total number of time steps to generate.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the generated musical MFCC sequence (shape: (total_generation_steps, musical_feature_dim)),\n",
    "        or None if generation fails.\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating musical feature sequence iteratively for {total_generation_steps} steps using PyTorch model...\")\n",
    "\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    generated_sequence = [] # List to store generated time steps\n",
    "    current_musical_sequence = start_sequence.clone().to(device) # Start with the initial sequence on the correct device\n",
    "    sequence_length = start_sequence.shape[1] # Expected input sequence length for the model\n",
    "    musical_feature_dim = start_sequence.shape[2] # Dimension of musical features\n",
    "\n",
    "    # Ensure conditioning inputs are on the correct device\n",
    "    text_conditioning_input = text_conditioning_input.to(device)\n",
    "    audio_conditioning_input = audio_conditioning_input.to(device)\n",
    "\n",
    "\n",
    "    # Ensure audio_conditioning_input has a batch dimension and correct sequence length\n",
    "    if audio_conditioning_input.ndim != 3 or audio_conditioning_input.shape[1] != sequence_length:\n",
    "         print(\"Error: audio_conditioning_input must have shape (1, sequence_length, total_audio_features_dim).\")\n",
    "         return None\n",
    "\n",
    "    # Ensure text_conditioning_input has a batch dimension and correct dimension\n",
    "    if text_conditioning_input.ndim != 2 or text_conditioning_input.shape[1] != text_conditioning_dim:\n",
    "        print(\"Error: text_conditioning_input must have shape (1, text_conditioning_dim).\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation during generation\n",
    "        for i in range(total_generation_steps):\n",
    "            # The model expects [musical_input, text_conditioning_input, audio_conditioning_input]\n",
    "            model_inputs = [current_musical_sequence, text_conditioning_input, audio_conditioning_input]\n",
    "\n",
    "            # Predict the next time step of musical features\n",
    "            try:\n",
    "                # The model predicts a sequence of length `musical_sequence_length`.\n",
    "                # We are interested in the prediction for the *next* time step.\n",
    "                predicted_sequence = model(*model_inputs) # Pass inputs unpacking the list\n",
    "                # predicted_sequence shape: (1, musical_sequence_length, musical_feature_dim)\n",
    "\n",
    "                # Take the last time step of the prediction\n",
    "                predicted_next_step = predicted_sequence[:, -1:, :] # Shape (1, 1, musical_feature_dim)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during model prediction at step {i}: {e}\")\n",
    "                return None\n",
    "\n",
    "\n",
    "            # Append the predicted time step to the generated sequence (as numpy array)\n",
    "            generated_sequence.append(predicted_next_step.squeeze().cpu().numpy()) # Move to CPU and convert to numpy\n",
    "\n",
    "            # Update the current musical sequence by removing the oldest time step and adding the new one\n",
    "            # This sliding window approach is standard for iterative sequence generation.\n",
    "            # Ensure the sequence length remains constant\n",
    "            current_musical_sequence = torch.cat((current_musical_sequence[:, 1:, :], predicted_next_step), dim=1)\n",
    "\n",
    "            # Print progress\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Generated {i + 1}/{total_generation_steps} steps.\")\n",
    "\n",
    "\n",
    "    # Convert the list of generated steps into a numpy array\n",
    "    generated_sequence_np = np.array(generated_sequence) # Shape (total_generation_steps, musical_feature_dim)\n",
    "\n",
    "    print(\"\\nIterative musical feature generation complete.\")\n",
    "    print(f\"Shape of the generated musical features: {generated_sequence_np.shape}\")\n",
    "\n",
    "    # Add inspection of generated features\n",
    "    print(\"\\nGenerated Musical Features Statistics:\")\n",
    "    print(f\"Shape: {generated_sequence_np.shape}\")\n",
    "    print(f\"Mean: {np.mean(generated_sequence_np)}\")\n",
    "    print(f\"Min: {np.min(generated_sequence_np)}\")\n",
    "    print(f\"Max: {np.max(generated_sequence_np)}\")\n",
    "    print(f\"Standard Deviation: {np.std(generated_sequence_np)}\")\n",
    "\n",
    "    # Save generated features to a file for inspection\n",
    "    features_output_filename = 'generated_musical_features_iterative_pytorch.npy'\n",
    "    np.save(features_output_filename, generated_sequence_np)\n",
    "    print(f\"Generated musical features saved to {features_output_filename}\")\n",
    "\n",
    "\n",
    "    return generated_sequence_np\n",
    "\n",
    "\n",
    "# --- Function to convert musical features to audio and save ---\n",
    "def convert_features_to_audio(musical_features, sr=22050, output_filename_wav='generated_music.wav', output_filename_mp3='generated_music.mp3'):\n",
    "    \"\"\"Converts musical features (MFCCs) to an audio file and saves it.\n",
    "\n",
    "    Args:\n",
    "        musical_features: A numpy array containing the generated musical MFCC sequence (shape: (time_steps, n_mfcc)).\n",
    "        sr: Sampling rate for audio output.\n",
    "        output_filename_wav: Filename for the output WAV file.\n",
    "        output_filename_mp3: Filename for the output MP3 file.\n",
    "\n",
    "    Returns:\n",
    "        The path to the generated audio file (MP3 or WAV), or None if conversion/saving fails.\n",
    "    \"\"\"\n",
    "    if musical_features is None or musical_features.shape[0] == 0:\n",
    "        print(\"No musical features provided for audio conversion.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\nConverting generated musical features to audio...\")\n",
    "    try:\n",
    "        # Assuming the features are MFCCs. Transpose back to (n_mfcc, time_steps) for inverse transform\n",
    "        audio_time_series = librosa.feature.inverse.mfcc_to_audio(musical_features.T, sr=sr)\n",
    "\n",
    "        # Add inspection of generated audio time series\n",
    "        print(\"\\nGenerated Audio Time Series Statistics:\")\n",
    "        print(f\"Shape: {audio_time_series.shape}\")\n",
    "        print(f\"Mean: {np.mean(audio_time_series)}\")\n",
    "        print(f\"Min: {np.min(audio_time_series)}\")\n",
    "        print(f\"Max: {np.max(audio_time_series)}\")\n",
    "        print(f\"Standard Deviation: {np.std(audio_time_series)}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MFCC to audio conversion: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Save the generated audio to a WAV file first\n",
    "    print(f\"Saving generated audio to {output_filename_wav}...\")\n",
    "    try:\n",
    "        sf.write(output_filename_wav, audio_time_series, sr)\n",
    "        print(f\"Generated audio saved successfully to {output_filename_wav}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving WAV file: {e}\")\n",
    "        return None\n",
    "\n",
    "    generated_audio_file = output_filename_wav # Default to WAV\n",
    "\n",
    "    # Attempt to convert the WAV file to MP3\n",
    "    print(f\"Attempting to convert {output_filename_wav} to {output_filename_mp3}...\")\n",
    "    try:\n",
    "        audio = AudioSegment.from_wav(output_filename_wav)\n",
    "        audio.export(output_filename_mp3, format=\"mp3\")\n",
    "        print(f\"Conversion to MP3 successful. Output saved to {output_filename_mp3}\")\n",
    "        generated_audio_file = output_filename_mp3 # Update to MP3 if successful\n",
    "        # Optionally, remove the intermediate WAV file\n",
    "        # os.remove(output_filename_wav)\n",
    "        # print(f\"Removed intermediate WAV file: {output_filename_wav}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MP3 conversion: {e}. Proceeding with WAV file.\")\n",
    "        # generated_audio_file remains the WAV file path\n",
    "\n",
    "    return generated_audio_file\n",
    "\n",
    "\n",
    "# --- Function to convert generated features to symbolic music (music21) (Step 4 in Plan) ---\n",
    "def convert_features_to_music21(musical_features_np, sr=22050, hop_length=512):\n",
    "    \"\"\"Converts generated musical features (MFCCs) to a music21 stream.\n",
    "\n",
    "    This is a placeholder function. Converting MFCCs directly to\n",
    "    meaningful symbolic music (notes, chords, rhythm) is a complex task\n",
    "    and requires more advanced techniques (e.g., a separate model trained\n",
    "    to map features to musical events).\n",
    "\n",
    "    Args:\n",
    "        musical_features_np: A numpy array containing the generated musical MFCC sequence (shape: (time_steps, n_mfcc)).\n",
    "        sr: Sampling rate.\n",
    "        hop_length: Hop length used during feature extraction.\n",
    "\n",
    "    Returns:\n",
    "        A music21 stream, or None if conversion is not implemented or fails.\n",
    "    \"\"\"\n",
    "    print(\"\\nAttempting to convert musical features to music21 stream...\")\n",
    "    print(\"Note: Direct conversion of MFCCs to symbolic music is complex and requires further development.\")\n",
    "    # Placeholder implementation: Create a simple stream for demonstration\n",
    "    from music21 import stream, note, duration, tempo, pitch\n",
    "\n",
    "    s = stream.Stream()\n",
    "\n",
    "    # Add a tempo indication (optional, but good practice)\n",
    "    # Estimate tempo from features or use a default\n",
    "    # For simplicity, using a default tempo\n",
    "    s.append(tempo.MetronomeMark(number=120))\n",
    "\n",
    "    # Simple approach: Map a feature value (e.g., the first MFCC coefficient) to pitch\n",
    "    # and use a fixed duration based on the hop length. This is a very basic mapping.\n",
    "    time_per_step = hop_length / sr # Duration of each feature vector in seconds\n",
    "\n",
    "    # Define a mapping range for MFCC values to MIDI pitches (adjust as needed)\n",
    "    # Find min/max of the first MFCC coefficient in the generated sequence\n",
    "    if musical_features_np.shape[0] > 0:\n",
    "        min_mfcc1 = np.min(musical_features_np[:, 0])\n",
    "        max_mfcc1 = np.max(musical_features_np[:, 0])\n",
    "        midi_pitch_range = 60 # Map to a range of 60 MIDI pitches (e.g., from 30 to 90)\n",
    "        min_midi_pitch = 30\n",
    "\n",
    "        for i, features in enumerate(musical_features_np):\n",
    "            # Use the first MFCC coefficient for pitch mapping\n",
    "            mfcc1 = features[0]\n",
    "\n",
    "            # Normalize the MFCC value to the range [0, 1]\n",
    "            if (max_mfcc1 - min_mfcc1) > 0:\n",
    "                normalized_mfcc1 = (mfcc1 - min_mfcc1) / (max_mfcc1 - min_mfcc1)\n",
    "            else:\n",
    "                normalized_mfcc1 = 0.5 # Default to middle if range is zero\n",
    "\n",
    "            # Map the normalized value to a MIDI pitch\n",
    "            midi_pitch_val = int(min_midi_pitch + normalized_mfcc1 * midi_pitch_range)\n",
    "            midi_pitch_val = max(0, min(127, midi_pitch_val)) # Clamp to valid MIDI range\n",
    "\n",
    "            try:\n",
    "                 # Create a note\n",
    "                 n = note.Note(midi_pitch_val)\n",
    "                 # Set the duration based on the time per feature step\n",
    "                 # Using a fixed duration for simplicity, could be refined\n",
    "                 n.duration = duration.Duration(time_per_step)\n",
    "\n",
    "                 s.append(n)\n",
    "            except Exception as note_error:\n",
    "                 print(f\"Error creating music21 note at step {i}: {note_error}\")\n",
    "                 # Optionally, append a rest or skip this step\n",
    "                 s.append(note.Rest(duration=duration.Duration(time_per_step)))\n",
    "\n",
    "\n",
    "            # Avoid creating an excessively large stream for demonstration\n",
    "            if i > 200: # Limit the number of elements for a quicker conversion/display\n",
    "                 break\n",
    "    else:\n",
    "         print(\"No generated features to convert to music21.\")\n",
    "         return None\n",
    "\n",
    "\n",
    "    print(f\"Created a music21 stream with {len(s.elements)} elements.\") # Use .elements to count top-level elements\n",
    "\n",
    "    return s\n",
    "\n",
    "# --- Function to save and display music21 stream (Step 5 in Plan) ---\n",
    "def save_and_display_music21_stream(music21_stream, output_filename_midi='generated_music.mid'):\n",
    "    \"\"\"Saves a music21 stream as a MIDI file and displays its text representation.\"\"\"\n",
    "    if music21_stream is None:\n",
    "        print(\"No music21 stream provided to save or display.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nSaving music21 stream to {output_filename_midi}...\")\n",
    "    try:\n",
    "        music21_stream.write('midi', fp=output_filename_midi)\n",
    "        print(f\"Music21 stream saved successfully to {output_filename_midi}\")\n",
    "\n",
    "        # Display the stream as text\n",
    "        print(\"\\nGenerated Music21 Stream (text representation):\")\n",
    "        print(music21_stream.show('text'))\n",
    "\n",
    "        return output_filename_midi # Return the path to the saved MIDI file\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving or displaying music21 stream: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Main music generation function (uses iterative generation with PyTorch model) ---\n",
    "def generate_music_from_input_v4(model, user_text_input=None, user_audio_input_path=None, input_type='text_prompt', target_audio_duration_sec=30, sr=22050, max_padding=174, n_mfcc=20, n_chroma=12, text_conditioning_dim=128):\n",
    "    \"\"\"Generates music based on user input (text and/or audio) using the PyTorch model and iterative generation.\n",
    "\n",
    "    Args:\n",
    "        model: The trained PyTorch conditional music generation model.\n",
    "        user_text_input: The user's text input (string for lyrics or prompt).\n",
    "        user_audio_input_path: Path to the user's audio file.\n",
    "        input_type: Type of text input ('text_lyrics', 'text_prompt'). 'audio_only' is also possible if no text is provided.\n",
    "        target_audio_duration_sec: The desired duration of the generated audio in seconds.\n",
    "        sr: Sampling rate for audio output.\n",
    "        max_padding: Target sequence length for padding/truncating features (used for initial sequence).\n",
    "        n_mfcc: Number of MFCCs extracted.\n",
    "        n_chroma: Number of chroma bins extracted.\n",
    "        text_conditioning_dim: Dimension of the text embedding.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - The path to the generated audio file (MP3 or WAV),\n",
    "        - The generated lyrics (string),\n",
    "        - The generated musical features (numpy array),\n",
    "        or (None, None, None) if an error occurred.\n",
    "    \"\"\"\n",
    "    text_conditioning_input_for_model = None\n",
    "    audio_conditioning_input_for_model = None\n",
    "    generated_lyrics = None\n",
    "\n",
    "    # Process text input if provided\n",
    "    if user_text_input:\n",
    "        is_prompt = (input_type == 'text_prompt')\n",
    "        conditioning_input_for_model, generated_lyrics = process_text_input(user_text_input, text_conditioning_dim, is_prompt=is_prompt)\n",
    "        if conditioning_input_for_model is None:\n",
    "            print(\"Error processing text input for conditioning.\")\n",
    "            return None, None, None\n",
    "        text_conditioning_input_for_model = conditioning_input_for_model\n",
    "\n",
    "    else:\n",
    "        print(\"No text input provided. Using dummy text conditioning.\")\n",
    "        # Create dummy text conditioning input as a PyTorch tensor and move to device\n",
    "        text_conditioning_input_for_model = torch.zeros((1, text_conditioning_dim)).to(device)\n",
    "\n",
    "\n",
    "    # Process audio input if provided\n",
    "    if user_audio_input_path and os.path.exists(user_audio_input_path):\n",
    "        print(\"Processing user audio input for conditioning.\")\n",
    "        # For iterative generation, we still need the full audio conditioning sequence\n",
    "        audio_conditioning_input_for_model = process_user_audio_for_conditioning(user_audio_input_path, max_padding, n_mfcc, n_chroma)\n",
    "        if audio_conditioning_input_for_model is None:\n",
    "            print(\"Error processing user audio input for conditioning.\")\n",
    "            return None, None, None\n",
    "    else:\n",
    "        print(\"No user audio file provided or found. Using dummy audio conditioning.\")\n",
    "        # Create dummy audio conditioning input as a PyTorch tensor and move to device\n",
    "        audio_conditioning_input_for_model = torch.zeros((1, max_padding, total_audio_features_dim)).to(device)\n",
    "\n",
    "\n",
    "    # Ensure both conditioning inputs are available\n",
    "    if text_conditioning_input_for_model is None or audio_conditioning_input_for_model is None:\n",
    "         print(\"Missing conditioning input(s). Cannot proceed.\")\n",
    "         return None, None, None\n",
    "\n",
    "\n",
    "    # Generate a starting musical sequence (e.g., take a random sample from the training data)\n",
    "    # This starting sequence will be the initial input for the iterative generation.\n",
    "    # Ensure X_train_music is available from data loading\n",
    "    if 'X_train_music' not in globals() or X_train_music.shape[0] == 0:\n",
    "         print(\"Training data (X_train_music) is not available or empty. Cannot generate a starting sequence.\")\n",
    "         return None, None, None\n",
    "\n",
    "    # Select a random starting sequence and convert to PyTorch tensor, add batch dim, move to device\n",
    "    start_index = np.random.randint(0, X_train_music.shape[0])\n",
    "    start_sequence_np = X_train_music[start_index:start_index+1, :, :]\n",
    "    start_sequence_tensor = torch.FloatTensor(start_sequence_np).to(device)\n",
    "\n",
    "\n",
    "    # Calculate the total number of time steps to generate\n",
    "    # Each time step in the feature sequence corresponds to hop_length samples (default 512 in librosa mfcc)\n",
    "    # Duration of one feature time step = hop_length / sr\n",
    "    # Total time steps needed = target_audio_duration_sec / (hop_length / sr) = target_audio_duration_sec * sr / hop_length\n",
    "    # Assuming default hop_length of 512\n",
    "    hop_length = 512\n",
    "    total_generation_steps = int(target_audio_duration_sec * sr / hop_length)\n",
    "    print(f\"Targeting {target_audio_duration_sec} seconds, which requires approximately {total_generation_steps} generation steps.\")\n",
    "\n",
    "\n",
    "    # Generate new musical features iteratively using the trained PyTorch conditional model\n",
    "    generated_musical_features_np = generate_musical_features_iterative_pytorch(\n",
    "        model,\n",
    "        start_sequence_tensor,\n",
    "        text_conditioning_input_for_model,\n",
    "        audio_conditioning_input_for_model,\n",
    "        total_generation_steps\n",
    "    )\n",
    "\n",
    "\n",
    "    if generated_musical_features_np is None:\n",
    "        print(\"Feature generation failed.\")\n",
    "        return None, generated_lyrics, None # Return None for audio path and features\n",
    "\n",
    "\n",
    "    # Convert the generated musical features to audio and save\n",
    "    generated_audio_file = convert_features_to_audio(generated_musical_features_np, sr=sr)\n",
    "\n",
    "\n",
    "    # Optionally, save the generated lyrics to a text file if lyrics were generated\n",
    "    lyrics_output_filename = 'generated_lyrics.txt'\n",
    "    if generated_lyrics:\n",
    "        try:\n",
    "            with open(lyrics_output_filename, 'w') as f:\n",
    "                f.write(generated_lyrics)\n",
    "            print(f\"Generated lyrics saved to {lyrics_output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving generated lyrics to file: {e}\")\n",
    "\n",
    "\n",
    "    return generated_audio_file, generated_lyrics, generated_musical_features_np # Return features\n",
    "\n",
    "\n",
    "# --- Example Usage (requires trained PyTorch model and data) ---\n",
    "# Assume model_gen_conditioned_pytorch is trained and X_train_music is defined.\n",
    "\n",
    "# Example: Generate music from a text prompt and user audio, aiming for 30 seconds\n",
    "user_text_prompt = \"Generate a piece of background music.\"\n",
    "# IMPORTANT: Replace '/content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg' with the actual path to your uploaded audio file\n",
    "# Ensure the audio file exists before running\n",
    "user_audio_input_path = '/content/WhatsApp Audio 2025-09-08 at 13.18.31.mpeg'\n",
    "if not os.path.exists(user_audio_input_path):\n",
    "    print(f\"Error: User audio file not found at {user_audio_input_path}. Please upload the file.\")\n",
    "    user_audio_input_path = None # Set to None if file doesn't exist\n",
    "\n",
    "target_duration_seconds = 30\n",
    "\n",
    "if user_audio_input_path is not None and 'model_gen_conditioned_pytorch' in globals() and 'X_train_music' in globals() and X_train_music.shape[0] > 0:\n",
    "    print(f\"\\nAttempting to generate music from text prompt: '{user_text_prompt}' using audio file at {user_audio_input_path}, targeting {target_duration_seconds} seconds.\")\n",
    "    generated_music_file, generated_lyrics_text, generated_features_np = generate_music_from_input_v4(\n",
    "        model_gen_conditioned_pytorch, # Use the PyTorch model\n",
    "        user_text_input=user_text_prompt,\n",
    "        user_audio_input_path=user_audio_input_path,\n",
    "        input_type='text_prompt',\n",
    "        target_audio_duration_sec=target_duration_seconds,\n",
    "        sr=sr,\n",
    "        max_padding=max_padding,\n",
    "        n_mfcc=n_mfcc,\n",
    "        n_chroma=n_chroma,\n",
    "        text_conditioning_dim=text_conditioning_dim\n",
    "    )\n",
    "\n",
    "\n",
    "    if generated_music_file:\n",
    "        print(f\"\\nGenerated music file: {generated_music_file}\")\n",
    "        if generated_lyrics_text:\n",
    "            print(f\"Generated lyrics:\\n{generated_lyrics_text}\")\n",
    "        # Display links to the generated files\n",
    "        print(\"\\nDownload the generated music file:\")\n",
    "        display(FileLink(generated_music_file))\n",
    "        if generated_lyrics_text: # Only display lyrics link if lyrics were generated\n",
    "            print(\"Download the generated lyrics file:\")\n",
    "            display(FileLink('generated_lyrics.txt'))\n",
    "\n",
    "        # Play the generated audio\n",
    "        print(f\"\\nPlaying generated audio file: {generated_music_file}\")\n",
    "        display(Audio(generated_music_file))\n",
    "\n",
    "        # Step 4: Convert generated features to symbolic music (music21)\n",
    "        if generated_features_np is not None:\n",
    "            music21_stream = convert_features_to_music21(generated_features_np, sr=sr, hop_length=512) # Pass hop_length\n",
    "\n",
    "            # Step 5: Save and display symbolic music (music21)\n",
    "            if music21_stream:\n",
    "                generated_midi_file = save_and_display_music21_stream(music21_stream)\n",
    "                if generated_midi_file:\n",
    "                    print(\"\\nDownload the generated MIDI file:\")\n",
    "                    display(FileLink(generated_midi_file))\n",
    "        else:\n",
    "            print(\"\\nNo generated features available for music21 conversion.\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"\\nMusic generation failed.\")\n",
    "else:\n",
    "    if user_audio_input_path is None:\n",
    "         print(\"\\nSkipping music generation as user audio file was not found.\")\n",
    "    else:\n",
    "         print(\"\\nSkipping music generation. PyTorch model or training data not available.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
